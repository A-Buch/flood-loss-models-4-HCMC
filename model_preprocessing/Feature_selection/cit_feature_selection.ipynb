{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data preprocessing for HCMC survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"a.buch@stud.uni-heidelberg.de\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection done by Conditional Inference Trees \n",
    "\n",
    "CIT uses p-value as one-a-split criterion instead of using homogeneity. The algorithm will pick the feature with the least p-value and will start splitting from it. Then it will keep going until it no longer finds statistically significant p-value or some other criteria have met such as minimum node size or max split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score# , confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_feature_selection import r_ctree_statistics, save_selected_features\n",
    "\n",
    "\n",
    "seed = np.random.seed(11)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ruff check ./model_preprocessing/Feature_selection/utils_feature_selection.py --fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### install Rpackage for ctree in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpy2.robjects.packages.Package as a <module 'partykit'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load r library initally, enables the %%R magic\n",
    "# %load_ext rpy2.ipython\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr, data\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "\n",
    "# get basic R packages\n",
    "utils = importr('utils')\n",
    "base = importr('base')\n",
    "dplyr = importr('dplyr')\n",
    "stats = importr(\"stats\")\n",
    "\n",
    "# pandas.DataFrames to R dataframes \n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "pandas2ri.activate()\n",
    "\n",
    "# print r df in html\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "\n",
    "\n",
    "# get partykit library containing ctree , ctree_controls etc\n",
    "partykit = importr('partykit')\n",
    "partykit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_contentloss_euro</th>\n",
       "      <th>Target_businessreduction</th>\n",
       "      <th>inundation_duration_h</th>\n",
       "      <th>water_depth_cm</th>\n",
       "      <th>contaminations.0</th>\n",
       "      <th>contaminations.1</th>\n",
       "      <th>contaminations.2</th>\n",
       "      <th>contaminations.3</th>\n",
       "      <th>contaminations.4</th>\n",
       "      <th>flowvelocity</th>\n",
       "      <th>...</th>\n",
       "      <th>resilience_govern_careing</th>\n",
       "      <th>resilience_govern_careing_increases</th>\n",
       "      <th>resilience_left_alone</th>\n",
       "      <th>resilience_neighbor_management</th>\n",
       "      <th>perception_who_responsible4protection.Rank1</th>\n",
       "      <th>perception_govern_support_future</th>\n",
       "      <th>perception_private_economy_future</th>\n",
       "      <th>shp_content_value_euro</th>\n",
       "      <th>shp_registered_capital_euro</th>\n",
       "      <th>elevation_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11047.7</td>\n",
       "      <td>1.83886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>736.5</td>\n",
       "      <td>1.87277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target_contentloss_euro  Target_businessreduction  inundation_duration_h   \n",
       "395                      0.0                       NaN                    4.0  \\\n",
       "396                      0.0                       0.0                    3.0   \n",
       "\n",
       "     water_depth_cm  contaminations.0  contaminations.1  contaminations.2   \n",
       "395            70.0                 0                 1                 0  \\\n",
       "396           100.0                 0                 1                 0   \n",
       "\n",
       "     contaminations.3  contaminations.4  flowvelocity  ...   \n",
       "395                 0                 1             1  ...  \\\n",
       "396                 0                 1             1  ...   \n",
       "\n",
       "     resilience_govern_careing  resilience_govern_careing_increases   \n",
       "395                        1.0                                  1.0  \\\n",
       "396                        NaN                                  NaN   \n",
       "\n",
       "     resilience_left_alone  resilience_neighbor_management   \n",
       "395                      5                             1.0  \\\n",
       "396                      5                             NaN   \n",
       "\n",
       "     perception_who_responsible4protection.Rank1   \n",
       "395                                          2.0  \\\n",
       "396                                          3.0   \n",
       "\n",
       "     perception_govern_support_future  perception_private_economy_future   \n",
       "395                               1.0                                3.0  \\\n",
       "396                               NaN                                3.0   \n",
       "\n",
       "     shp_content_value_euro  shp_registered_capital_euro  elevation_m  \n",
       "395                     NaN                      11047.7      1.83886  \n",
       "396                     NaN                        736.5      1.87277  \n",
       "\n",
       "[2 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates = pd.read_excel(\"../../input_survey_data/input_data_business_2.xlsx\")\n",
    "print(df_candidates.shape)\n",
    "df_candidates.tail(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386, 60)\n"
     ]
    }
   ],
   "source": [
    "targets = [\"Target_contentloss_euro\", \"Target_businessreduction\"]\n",
    "target = targets[0]\n",
    "\n",
    "## TODO make entire wokflow as loop over both target variables\n",
    "# ## iterate over both targets and store results \n",
    "# for target in [\"Target_contentloss_euro\", \"Target_businessreduction\"]:\n",
    "\n",
    "#     print( f\"Apply Elastic Net on {target}:\\n\")\n",
    "#     y = df_candidates[target]\n",
    "\n",
    "\n",
    "## remove cases where target information is missing\n",
    "df_candidates = df_candidates[ ~df_candidates[f\"{target}\"].isna()]\n",
    "print(df_candidates.shape)\n",
    "\n",
    "\n",
    "X = df_candidates.drop(targets, axis=1)\n",
    "y = df_candidates[target]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, \n",
    "    random_state=seed, shuffle=False\n",
    ")\n",
    "\n",
    "train = pd.concat([y_train, X_train], axis=1)\n",
    "test = pd.concat([y_test, X_test], axis=1)\n",
    "#print(train.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CV with gridSearch TODO   ###################\n",
    "# ## specify model\n",
    "# cit_model = partykit.ctree(Formula('Target_contentloss_euro ~ .'),  \n",
    "#                                 data=train\n",
    "#                           )\n",
    "# ## hyperparameter tunning \n",
    "# param_dist = [{'mincriterion': 0.95}]\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# cit_model_cv = GridSearchCV(estimator = cit_model, \n",
    "#                          param_grid = param_dist\n",
    "#                          )#, scoring= 'r2', cv = folds, verbose = 1,return_train_score=True)      \n",
    "\n",
    "# # # fit the model\n",
    "# cit_model_cv.fit(X_train, y_train)\n",
    "  \n",
    "\n",
    "#################### without Hyperparameter tunning  #####################\n",
    "# fit ctree\n",
    "## Minciterion = confidence level (smaller values => larger trees; e.g mincriterion=0.8, p-value must be smaller than 0.2 in order for a node to split)\n",
    "cit_model = partykit.ctree(Formula(f'{target} ~ .'),  \n",
    "                                data=train,\n",
    "                                control = partykit.ctree_control(mincriterion = 0.8)\n",
    "                          )\n",
    "\n",
    "\n",
    "## store trained model for evaluation\n",
    "filename = f'./models_trained/cit_{target}'\n",
    "pickle.dump(cit_model, open(filename, 'wb'))\n",
    "\n",
    "# keras = install.packages(\"keras\")\n",
    "# # Save the model\n",
    "# keras.save_model_hdf5(model, \"model.h5\")\n",
    "# # Recreate the exact same model purely from the file\n",
    "# new_model <- load_model_hdf5(\"model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## actual p-values (without log)\n",
    "# strucchange = importr(\"strucchange\")\n",
    "# strucchange.sctest(cit_model, node = 1)[1]  # p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           inundation_duration_h  water_depth_cm  contaminations.0   \n",
      "statistic               2.080154       11.993719          0.143349  \\\n",
      "p_value                 0.999915        0.030494          1.000000   \n",
      "criterion              -9.373248       -0.030969        -70.800235   \n",
      "\n",
      "           contaminations.1  contaminations.2  contaminations.3   \n",
      "statistic          0.149846          0.087360          0.030447  \\\n",
      "p_value            1.000000          1.000000          1.000000   \n",
      "criterion        -69.576261        -84.629148       -114.650836   \n",
      "\n",
      "           contaminations.4  flowvelocity  warning_time_h   \n",
      "statistic          1.199385      3.896394        1.645725  \\\n",
      "p_value            1.000000      0.943685        0.999998   \n",
      "criterion        -18.527556     -2.876792      -12.909114   \n",
      "\n",
      "           emergency_measures.1  ...  resilience_govern_careing   \n",
      "statistic              0.462363  ...                   1.487844  \\\n",
      "p_value                1.000000  ...                   1.000000   \n",
      "criterion            -39.800464  ...                 -14.600782   \n",
      "\n",
      "           resilience_govern_careing_increases  resilience_left_alone   \n",
      "statistic                             6.079653               0.299933  \\\n",
      "p_value                               0.550046               1.000000   \n",
      "criterion                            -0.798609             -50.859540   \n",
      "\n",
      "           resilience_neighbor_management   \n",
      "statistic                        0.166464  \\\n",
      "p_value                          1.000000   \n",
      "criterion                      -66.683591   \n",
      "\n",
      "           perception_who_responsible4protection.Rank1   \n",
      "statistic                                    22.907534  \\\n",
      "p_value                                       0.000099   \n",
      "criterion                                    -0.000099   \n",
      "\n",
      "           perception_govern_support_future   \n",
      "statistic                          0.388196  \\\n",
      "p_value                            1.000000   \n",
      "criterion                        -44.193657   \n",
      "\n",
      "           perception_private_economy_future  shp_content_value_euro   \n",
      "statistic                           0.274005            8.671403e+01  \\\n",
      "p_value                             1.000000            7.273446e-19   \n",
      "criterion                         -53.240319           -7.273446e-19   \n",
      "\n",
      "           shp_registered_capital_euro  elevation_m  \n",
      "statistic                 2.097972e+02     2.931013  \n",
      "p_value                   8.821498e-46     0.994868  \n",
      "criterion                -8.821498e-46    -5.272326  \n",
      "\n",
      "[3 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "## get statisitcs to obtain important feautres\n",
    "\n",
    "cit_stats = r_ctree_statistics(cit_model)\n",
    "cit_stats.columns = X.columns\n",
    "print(cit_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log rather than the p-value is used because it is numerically much more stable when used for comparisons, computing the minimal value, etc. Note that the p-values can become extremely small when significant. \n",
    "\n",
    "\n",
    "statistic DEF: \n",
    "\n",
    "citrerion DEF: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 58\n",
      "selected features: 10\n",
      "dropped features: 48\n",
      "selected features: \n",
      "['water_depth_cm', 'b_area', 'hh_monthly_income_cat', 'shp_sector', 'shp_employees', 'shp_suppliers_location.3', 'shp_suppliers_location.4', 'perception_who_responsible4protection.Rank1', 'shp_content_value_euro', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../input_survey_data/fs_cit_Target_contentloss_euro.xlsx\n"
     ]
    }
   ],
   "source": [
    "## get signifcant features\n",
    "selected_feat = cit_stats.loc[:, cit_stats.loc[\"p_value\",:]<= 0.05]\n",
    "\n",
    "## write selected predictors to disk\n",
    "save_selected_features(X_train, y_train, selected_feat.columns, filename=f\"../../input_survey_data/fs_cit_{target}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 58)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict\n",
    "cit_pred = stats.predict(cit_model, test, type=\"response\") #  type = \"prob\" # conditional class probabilities\n",
    "cit_pred  = base.round(cit_pred)\n",
    "\n",
    "## get back to python dtypes\n",
    "cit_pred = np.array(cit_pred)\n",
    "y_test = np.array(y_test)\n",
    "y_test = np.array(y_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#np.mean((y_test - cit_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ro',\n",
       " 'rx',\n",
       " 'rx2',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '_vector',\n",
       " '_html_template',\n",
       " '__init__',\n",
       " '_iter_repr',\n",
       " '__repr__',\n",
       " '_repr_html_',\n",
       " 'from_length',\n",
       " '__parameters__',\n",
       " '__abstractmethods__',\n",
       " '_abc_impl',\n",
       " '_add_rops',\n",
       " '__add__',\n",
       " '__getitem__',\n",
       " '__setitem__',\n",
       " 'names',\n",
       " 'items',\n",
       " 'sample',\n",
       " 'repr_format_elt',\n",
       " '_iter_formatted',\n",
       " '__repr_content__',\n",
       " '__annotations__',\n",
       " '__rname__',\n",
       " '_RObjectMixin__tempfile',\n",
       " '_RObjectMixin__file',\n",
       " '_RObjectMixin__fifo',\n",
       " '_RObjectMixin__sink',\n",
       " '_RObjectMixin__close',\n",
       " '_RObjectMixin__readlines',\n",
       " '_RObjectMixin__unlink',\n",
       " '_RObjectMixin__show',\n",
       " '_RObjectMixin__print',\n",
       " '_RObjectMixin__slots',\n",
       " 'slots',\n",
       " '__str__',\n",
       " '__getstate__',\n",
       " '__setstate__',\n",
       " 'r_repr',\n",
       " 'rclass',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__slots__',\n",
       " '__hash__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__',\n",
       " '_R_TYPE',\n",
       " '_R_GET_PTR',\n",
       " '_R_SIZEOF_ELT',\n",
       " '_R_VECTOR_ELT',\n",
       " '_R_SET_VECTOR_ELT',\n",
       " '_CAST_IN',\n",
       " '__sexp__',\n",
       " '__sexp_refcount__',\n",
       " 'rid',\n",
       " 'typeof',\n",
       " 'named',\n",
       " 'list_attrs',\n",
       " 'do_slot',\n",
       " 'do_slot_assign',\n",
       " 'get_attrib',\n",
       " 'rsame',\n",
       " 'names_from_c_attribute',\n",
       " '_sexpobject',\n",
       " 'from_iterable',\n",
       " '_raise_incompatible_C_size',\n",
       " '_check_C_compatible',\n",
       " 'from_memoryview',\n",
       " 'from_object',\n",
       " '__len__',\n",
       " '__iter__',\n",
       " 'index',\n",
       " '__orig_bases__',\n",
       " '_is_protocol',\n",
       " '__class_getitem__']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit_model_p = pickle.load(open(f\"./models_trained/cit_{target}\", 'rb'))\n",
    "type(cit_model_p)\n",
    "#cit_model_p.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anna\\Documents\\UNI\\MA_topic\\flood-loss-models-4-HCMC\\model_preprocessing\\Feature_selection\\cit_feature_selection.ipynb Cell 20\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cit_model_p\u001b[39m.\u001b[39mnames\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cit_model_p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(cit_model_p\u001b[39m.\u001b[39;49mrx(\u001b[39m3\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cit_model_p\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "cit_model_p.names\n",
    "cit_model_p = np.array(cit_model_p.rx(3))\n",
    "cit_model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of check_scoring must be an object implementing 'fit'. Got <rpy2.robjects.vectors.ListVector object at 0x00000282FCAEBDC0> [RTYPES.VECSXP]\r\nR classes: ('constparty', 'party')\r\n[Lis..., Lis..., Lis..., Lan..., NUL..., Lis..., Sex..., Sex...]\r\n  1: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FD0C0> [RTYPES.VECSXP]\r\n  2: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FA0C0> [RTYPES.VECSXP]\r\n  3: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FD0C0> [RTYPES.VECSXP]\r\n  4: <class 'rpy2.rinterface.LangSexpVector'>\r\n  <rpy2.rinterface.LangSexpVector object at 0x00000282DF5FA0C0> [RTYPES.LANGSXP]\r\n  5: <class 'rpy2.rinterface_lib.sexp.NULLType'>\r\n  <rpy2.rinterface_lib.sexp.NULLType object at 0x00000282DEFB6780> [RTYPES.NILSXP]\r\n  6: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FA0C0> [RTYPES.VECSXP]\r\n  7: <class 'rpy2.rinterface.SexpClosure'>\r\n  <rpy2.rinterface.SexpClosure object at 0x00000282DF5FD0C0> [RTYPES.CLOSXP]\r\n  8: <class 'rpy2.rinterface.SexpClosure'>\r\n  <rpy2.rinterface.SexpClosure object at 0x00000282F202A5C0> [RTYPES.CLOSXP] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anna\\Documents\\UNI\\MA_topic\\flood-loss-models-4-HCMC\\model_preprocessing\\Feature_selection\\cit_feature_selection.ipynb Cell 20\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cv \u001b[39m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_repeats\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# evaluate the model and collect the scores\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m n_scores \u001b[39m=\u001b[39m cross_val_score(cit_model, X, y, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49mcv, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/cit_feature_selection.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMean Accuracy: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (np\u001b[39m.\u001b[39mmean(n_scores), np\u001b[39m.\u001b[39mstd(n_scores)))\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:560\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39m\"\"\"Evaluate a score by cross-validation.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[39mRead more in the :ref:`User Guide <cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m[0.3315057  0.08022103 0.03531816]\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39;49mscoring)\n\u001b[0;32m    562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:201\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m to_ignore \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    199\u001b[0m params \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 201\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    202\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39;49mfunc\u001b[39m.\u001b[39;49m\u001b[39m__qualname__\u001b[39;49m\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m constraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of check_scoring must be an object implementing 'fit'. Got <rpy2.robjects.vectors.ListVector object at 0x00000282FCAEBDC0> [RTYPES.VECSXP]\r\nR classes: ('constparty', 'party')\r\n[Lis..., Lis..., Lis..., Lan..., NUL..., Lis..., Sex..., Sex...]\r\n  1: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FD0C0> [RTYPES.VECSXP]\r\n  2: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FA0C0> [RTYPES.VECSXP]\r\n  3: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FD0C0> [RTYPES.VECSXP]\r\n  4: <class 'rpy2.rinterface.LangSexpVector'>\r\n  <rpy2.rinterface.LangSexpVector object at 0x00000282DF5FA0C0> [RTYPES.LANGSXP]\r\n  5: <class 'rpy2.rinterface_lib.sexp.NULLType'>\r\n  <rpy2.rinterface_lib.sexp.NULLType object at 0x00000282DEFB6780> [RTYPES.NILSXP]\r\n  6: <class 'rpy2.rinterface.ListSexpVector'>\r\n  <rpy2.rinterface.ListSexpVector object at 0x00000282DF5FA0C0> [RTYPES.VECSXP]\r\n  7: <class 'rpy2.rinterface.SexpClosure'>\r\n  <rpy2.rinterface.SexpClosure object at 0x00000282DF5FD0C0> [RTYPES.CLOSXP]\r\n  8: <class 'rpy2.rinterface.SexpClosure'>\r\n  <rpy2.rinterface.SexpClosure object at 0x00000282F202A5C0> [RTYPES.CLOSXP] instead."
     ]
    }
   ],
   "source": [
    "# define the model evaluation by k-fold CV\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(cit_model_p, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare r2 for train and test sets (for all polynomial fits)\n",
    "print(\"R-squared values: \\n\")\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    train_r2 = round(sklearn.metrics.r2_score(y_train, y_train_pred[:, i]), 2)\n",
    "    test_r2 = round(sklearn.metrics.r2_score(y_test, y_test_pred[:, i]), 2)\n",
    "    print(\"Polynomial degree {0}: train score={1}, test score={2}\".format(degree, \n",
    "                                                                         train_r2, \n",
    "                                                                         test_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# param_dist = {'n_estimators': [10, 100, 200, 500],\n",
    "#               'max_depth': [1, 3, 5, 10,20],\n",
    "#               'colsample_bynode': [0.1, 0.3] # nbr of feautres for each split point\n",
    "#               #'subsample': 0.8  # define subsample of train st, xgb has not bootstrapping\n",
    "#               }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply Elastic net on Target_contentloss_euro:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nElasticNet does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\Anna\\Documents\\UNI\\MA_topic\\flood-loss-models-4-HCMC\\model_preprocessing\\Feature_selection\\elastic_net_feature_selection.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# classes have to start from zero on for lasso regression, make continous variables as categorical \u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#le = LabelEncoder()\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#y_train = le.fit_transform(y_train)\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m## TODO adapt ratio (l1_ratio) between ridge and lasso reg: \u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# r = 0, equivalent to Ridge Regression,  r = 1 equivalent to Lasso Regression\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m elastic_net \u001b[39m=\u001b[39m ElasticNet(alpha\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, l1_ratio\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39mseed)\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m elastic_net\u001b[39m.\u001b[39;49mfit(X_train, y_train) \n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#elastic_net.predict([[1.5]])\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/model_preprocessing/Feature_selection/elastic_net_feature_selection.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLasso regression:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:908\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n",
      "\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n",
      "\u001b[0;32m    907\u001b[0m     X_copied \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_X \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept\n",
      "\u001b[1;32m--> 908\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n",
      "\u001b[0;32m    909\u001b[0m         X,\n",
      "\u001b[0;32m    910\u001b[0m         y,\n",
      "\u001b[0;32m    911\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    912\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    913\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n",
      "\u001b[0;32m    914\u001b[0m         copy\u001b[39m=\u001b[39;49mX_copied,\n",
      "\u001b[0;32m    915\u001b[0m         multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n",
      "\u001b[0;32m    916\u001b[0m         y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n",
      "\u001b[0;32m    917\u001b[0m     )\n",
      "\u001b[0;32m    918\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n",
      "\u001b[0;32m    919\u001b[0m         y, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m    920\u001b[0m     )\n",
      "\u001b[0;32m    922\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n",
      "\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n",
      "\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n",
      "\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n",
      "\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n",
      "\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n",
      "\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   1104\u001b[0m     )\n",
      "\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n",
      "\u001b[0;32m   1107\u001b[0m     X,\n",
      "\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n",
      "\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n",
      "\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n",
      "\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n",
      "\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n",
      "\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n",
      "\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n",
      "\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n",
      "\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n",
      "\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n",
      "\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n",
      "\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m   1120\u001b[0m )\n",
      "\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n",
      "\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n",
      "\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n",
      "\u001b[0;32m    918\u001b[0m         )\n",
      "\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n",
      "\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n",
      "\u001b[0;32m    922\u001b[0m             array,\n",
      "\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n",
      "\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n",
      "\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    926\u001b[0m         )\n",
      "\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n",
      "\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n",
      "\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n",
      "\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n",
      "\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    160\u001b[0m     )\n",
      "\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\n",
      "ElasticNet does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "## iterate over both targets and store results \n",
    "\n",
    "for target in [\"Target_contentloss_euro\", \"Target_businessreduction\"]:\n",
    "\n",
    "    print( f\"Apply Elastic Net on {target}:\\n\")\n",
    "    y = df_candidates[target]\n",
    "\n",
    "    ## normalize data \n",
    "    scaler = MinMaxScaler() \n",
    "    X = scaler.fit_transform(pd.DataFrame(X_unscaled))\n",
    "    y = scaler.fit_transform(pd.DataFrame(y))\n",
    "    \n",
    "    ## test train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, \n",
    "        random_state=seed, shuffle=True\n",
    "    )\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "    # classes have to start from zero on for lasso regression, make continous variables as categorical \n",
    "    #le = LabelEncoder()\n",
    "    #y_train = le.fit_transform(y_train)\n",
    "\n",
    "    ## set up model\n",
    "    ## TODO adapt ratio (l1_ratio) between ridge and lasso reg: \n",
    "    # r = 0, equivalent to Ridge Regression,  r = 1 equivalent to Lasso Regression\n",
    "    elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=seed)\n",
    "    elastic_net = SelectFromModel(elastic_net) \n",
    "    elastic_net.fit(X_train, y_train) \n",
    "    #elastic_net.predict([[1.5]])\n",
    "\n",
    "\n",
    "    print(\"Elastic Net:\")\n",
    "    selected_feat = X_train.columns[(elastic_net.get_support())]\n",
    "    not_selected_feat = X_train.columns[~(elastic_net.get_support())]\n",
    "\n",
    "    print(\"total features: {}\".format((X_train.shape[1])))\n",
    "    print(\"selected features: {}\".format(len(selected_feat)))\n",
    "    print(\"dropped features: \\n{}\\n\".format(X_unscaled.columns[not_selected_feat].to_list()))\n",
    "    ## print(\"features with coefficients shrank to zero: {}\".format(np.sum(elastic_net.estimator_.coef_ == 0)))\n",
    "    #print(f\"Selected features: \\n{X_unscaled.columns[selected_feat]}\")\n",
    "    #X_train[(ridge_.estimator_.coef_ == 0)#]\n",
    "\n",
    "    ## store trained model for evaluation\n",
    "    filename = f'./models_trained/elastic_net{target}.sav'\n",
    "    pickle.dump(elastic_net, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "    \n",
    "    ## write selected features from training set to disk\n",
    "    train = pd.concat([y_train, X_train], axis=1)\n",
    "    df_elastic_net = train[[target] + X_unscaled.columns[selected_feat].to_list()]\n",
    "    #df_elastic_net.info()\n",
    "    df_elastic_net.to_excel(f\"../../input_survey_data/fs_elasticnet_{target}.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # ## predict unseen X_test set\n",
    "    # y_lasso_pred = elastic_net.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
