{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data preprocessing for HCMC survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"a.buch@stud.uni-heidelberg.de\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection done by eXtreme Gradient Boosting (XGBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import copy as cp\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedKFold, RepeatedStratifiedKFold, RepeatedKFold, cross_val_score, cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../../../\")\n",
    "import utils.utils_feature_selection as fs\n",
    "import utils.utils_evaluation as e\n",
    "import utils.utils_figures as f\n",
    "import utils.settings as s\n",
    "import utils.pipelines_continous as p\n",
    "\n",
    "s.init()\n",
    "seed = s.seed\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_businessreduction</th>\n",
       "      <th>inundation_duration_h</th>\n",
       "      <th>water_depth_cm</th>\n",
       "      <th>contaminations.0</th>\n",
       "      <th>flowvelocity</th>\n",
       "      <th>warning_time_h</th>\n",
       "      <th>emergency_measures.1</th>\n",
       "      <th>emergency_measures.2</th>\n",
       "      <th>emergency_measures.3</th>\n",
       "      <th>emergency_measures.4</th>\n",
       "      <th>emergency_measures.6</th>\n",
       "      <th>emergency_measures.7</th>\n",
       "      <th>emergency_measures.8</th>\n",
       "      <th>emergency_measures.9</th>\n",
       "      <th>overall_problem_house</th>\n",
       "      <th>protect_valuables_impl</th>\n",
       "      <th>water_barriers_impl</th>\n",
       "      <th>pumping_equipment_impl</th>\n",
       "      <th>elevation_building_impl</th>\n",
       "      <th>resistant_material_building_impl</th>\n",
       "      <th>electricity_higher_impl</th>\n",
       "      <th>flood_protections_impl</th>\n",
       "      <th>flood_experience</th>\n",
       "      <th>elevation_building_height_cm</th>\n",
       "      <th>elevation_rel2surrounding_cat</th>\n",
       "      <th>bage</th>\n",
       "      <th>b_area</th>\n",
       "      <th>hh_monthly_income_cat</th>\n",
       "      <th>shp_owner</th>\n",
       "      <th>shp_sector</th>\n",
       "      <th>shp_employees</th>\n",
       "      <th>shp_avgmonthly_sale_cat</th>\n",
       "      <th>shp_finance_investments</th>\n",
       "      <th>shp_profits_last5years</th>\n",
       "      <th>shp_risk_tolerance</th>\n",
       "      <th>shp_monetary_resources4prevention</th>\n",
       "      <th>resilience_city_protection</th>\n",
       "      <th>resilience_more_future_affected</th>\n",
       "      <th>resilience_govern_warnings_helpful</th>\n",
       "      <th>resilience_govern_careing</th>\n",
       "      <th>resilience_left_alone</th>\n",
       "      <th>resilience_neighbor_management</th>\n",
       "      <th>perception_who_responsible4protection.Rank1</th>\n",
       "      <th>perception_private_economy_future</th>\n",
       "      <th>contaminations_light</th>\n",
       "      <th>contaminations_heavy</th>\n",
       "      <th>shp_suppliers_HCMC</th>\n",
       "      <th>shp_content_value_euro</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>shp_registered_capital_euro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83886</td>\n",
       "      <td>11047.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87277</td>\n",
       "      <td>736.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target_businessreduction  inundation_duration_h  water_depth_cm   \n",
       "395                       NaN                    4.0            70.0  \\\n",
       "396                       0.0                    3.0           100.0   \n",
       "\n",
       "     contaminations.0  flowvelocity  warning_time_h  emergency_measures.1   \n",
       "395                 0             1             NaN                     1  \\\n",
       "396                 0             1             NaN                     1   \n",
       "\n",
       "     emergency_measures.2  emergency_measures.3  emergency_measures.4   \n",
       "395                     0                     1                     0  \\\n",
       "396                     0                     1                     0   \n",
       "\n",
       "     emergency_measures.6  emergency_measures.7  emergency_measures.8   \n",
       "395                     1                     0                     0  \\\n",
       "396                     0                     0                     0   \n",
       "\n",
       "     emergency_measures.9  overall_problem_house  protect_valuables_impl   \n",
       "395                     0                      1                       1  \\\n",
       "396                     0                      0                       1   \n",
       "\n",
       "     water_barriers_impl  pumping_equipment_impl  elevation_building_impl   \n",
       "395                    5                       1                        1  \\\n",
       "396                    5                       5                        5   \n",
       "\n",
       "     resistant_material_building_impl  electricity_higher_impl   \n",
       "395                                 5                        5  \\\n",
       "396                                 5                        5   \n",
       "\n",
       "     flood_protections_impl  flood_experience  elevation_building_height_cm   \n",
       "395                       5                 5                          70.0  \\\n",
       "396                       5                 4                           NaN   \n",
       "\n",
       "     elevation_rel2surrounding_cat  bage  b_area  hh_monthly_income_cat   \n",
       "395                              1   NaN   130.0                    NaN  \\\n",
       "396                              0   5.0    33.0                    1.0   \n",
       "\n",
       "     shp_owner  shp_sector  shp_employees  shp_avgmonthly_sale_cat   \n",
       "395          1          17              2                        3  \\\n",
       "396          1          11              2                        3   \n",
       "\n",
       "     shp_finance_investments  shp_profits_last5years  shp_risk_tolerance   \n",
       "395                        1                     4.0                 3.0  \\\n",
       "396                        1                     4.0                 3.0   \n",
       "\n",
       "     shp_monetary_resources4prevention  resilience_city_protection   \n",
       "395                                3.0                         1.0  \\\n",
       "396                                4.0                         NaN   \n",
       "\n",
       "     resilience_more_future_affected  resilience_govern_warnings_helpful   \n",
       "395                              5.0                                 1.0  \\\n",
       "396                              NaN                                 NaN   \n",
       "\n",
       "     resilience_govern_careing  resilience_left_alone   \n",
       "395                        1.0                      5  \\\n",
       "396                        NaN                      5   \n",
       "\n",
       "     resilience_neighbor_management   \n",
       "395                             1.0  \\\n",
       "396                             NaN   \n",
       "\n",
       "     perception_who_responsible4protection.Rank1   \n",
       "395                                          2.0  \\\n",
       "396                                          3.0   \n",
       "\n",
       "     perception_private_economy_future  contaminations_light   \n",
       "395                                3.0                     1  \\\n",
       "396                                3.0                     1   \n",
       "\n",
       "     contaminations_heavy  shp_suppliers_HCMC  shp_content_value_euro   \n",
       "395                     0                   1                     NaN  \\\n",
       "396                     0                   1                     NaN   \n",
       "\n",
       "     elevation_m  shp_registered_capital_euro  \n",
       "395      1.83886                      11047.7  \n",
       "396      1.87277                        736.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates = pd.read_excel(\"../../../input_survey_data/input_data_businessreduction.xlsx\")\n",
    "df_candidates.tail(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing valeus per feature\n",
      " warning_time_h                                 0.775819\n",
      "shp_content_value_euro                         0.158690\n",
      "elevation_building_height_cm                   0.158690\n",
      "shp_registered_capital_euro                    0.118388\n",
      "Target_businessreduction                       0.090680\n",
      "shp_risk_tolerance                             0.070529\n",
      "perception_who_responsible4protection.Rank1    0.070529\n",
      "bage                                           0.068010\n",
      "perception_private_economy_future              0.065491\n",
      "hh_monthly_income_cat                          0.060453\n",
      "resilience_govern_careing                      0.057935\n",
      "shp_monetary_resources4prevention              0.045340\n",
      "resilience_govern_warnings_helpful             0.045340\n",
      "resilience_more_future_affected                0.037783\n",
      "shp_profits_last5years                         0.037783\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## delete features with more than 10% missing values\n",
    "\n",
    "print(\"Percentage of missing valeus per feature\\n\", df_candidates.isna().mean().sort_values(ascending=False)[:15] ) \n",
    "## --> kepp threshold by 15% less would delete important features e.g. content values, registerd capitaletc.\n",
    "\n",
    "# drop warning time due to 77% nan\n",
    "df_candidates = df_candidates.drop(\"warning_time_h\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select only damage cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = [\"Target_contentloss_euro\", \"Target_businessreduction\"]\n",
    "# target = targets[1]\n",
    "\n",
    "# print(f\"Removing {df_candidates.loc[df_candidates[target]==0,:].shape[0]} zero loss records\")\n",
    "# df_candidates = df_candidates.loc[df_candidates[target]!=0,:]\n",
    "\n",
    "# print(f\"Keeping {df_candidates.shape} damage cases for model training and evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target varibale distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "\n",
    "# print(df_candidates[target].describe())\n",
    "\n",
    "# closses = df_candidates[target] #df_candidates[target][df_candidates[target] != 0.0 ]\n",
    "# closses.hist(bins=2000, figsize=(8, 8))\n",
    "# plt.ylim(1, 20)\n",
    "# plt.xlim(1, 20000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [ 0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8] # store outside, for plotting\n",
    "#n_estimators = [ 50, 100, 200, 300, 500, 800]\n",
    "\n",
    "param_grid = {#'model__n_estimators': n_estimators,\n",
    "              'model__n_estimators': [ 10, 30, 50, 70, 100, 200, 300, 400], # get only low train scores with this\n",
    "              'model__max_depth': [1, 2, 3, 5, 7, 8, 10, 15],\n",
    "              #'model__max_leaves': [0, 3, 5],\n",
    "            #   'model__colsample_bytree': [0.1, 0.3, 0.5, 0.7, 1.0 ], # Percentage of columns to be randomly samples for each tree\n",
    "            #   'model__colsample_bynode': [0.1, 0.3, 0.5, 0.7, 1.0], # nbr of feautres for each split point\n",
    "             'model__eta': learning_rate,  # == eta\n",
    "            #   'model__gamma': [0.1, 0.2, 0.3, 0.5 ] , # min_split_loss -  larger gamma is, the more conservative the algorithm is\n",
    "            #'model__subsample': [0.0, 0.2, 0.5, 0.6, 0.8, 0.9],  # define subsample of train st prior to growing trees, prevent overfitting\n",
    "            #  'model__reg_alpha': [0.0, 0.5, 1.0, 2.0, 4.0, 5.0, 6.0 ,7.0],   # Lasso Regularization term on weights , higher values = more consrvative \n",
    "            #  'model__reg_lambda': [0.0,  0.05, 0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0],  # Ridge Regularization term on weights ,  higher values = more consrvative\n",
    "            #   'model__min_child_weight': [0, 1, 2, 3, 4,],\n",
    "            #   \"model__max_delta_step\":  [0, 3, 5, 6, 7],           # for LogisticReg good to solve imbalance \n",
    "          #   'model__objective': [None, 'reg:absoluteerror'],#'multi:softprob,'reg:squarederror','reg:models_trained'],\n",
    "          #  # 'model__tree_method': [\"hist\", \"gpu_hist\"],\n",
    "           #'model__booster': [None, \"gblinear\", \"gbtree\"],\n",
    "            \"model__validate_parameters\":[True],\n",
    "              }\n",
    "\n",
    "# 'model__scale_pos_weight': [0.0, 0.3, 0.5, 0.7, 0.9, 1.0],  # only  for clasifcation: handle imbalance, ratio between negative and positive examples\n",
    "\n",
    "# Objective candidate: multi:softmax\n",
    "# Objective candidate: multi:softprob\n",
    "# Objective candidate: reg:squarederror\n",
    "# Objective candidate: reg:squaredlogerror\n",
    "# Objective candidate: reg:logistic\n",
    "## Objective candidate: reg:linear\n",
    "# Objective candidate: reg:pseudohubererror\n",
    "# Objective candidate: reg:gamma\n",
    "# Objective candidate: reg:absoluteerror\n",
    "\n",
    "## DOC: https://xgboost.readthedocs.io/en/stable/parameter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply XGBoost on Target_businessreduction, with pipeline pipe_xgb:\n",
      "Uses  397  records, from those have  {149}  records zero businessreduction\n",
      "Dropping 36 records from entire dataset due that these values are nan in target variable\n",
      "Training set size 288\n",
      "Test set size 73\n",
      "Best hyperparams: {'model__validate_parameters': True, 'model__n_estimators': 100, 'model__max_depth': 7, 'model__eta': 0.2}\n",
      "Create new XGBoost model based on best hyperparameters\n",
      "\n",
      "Select features based on permutation feature importance\n",
      "Most important features: ['water_depth_cm', 'shp_monetary_resources4prevention', 'bage', 'perception_who_responsible4protection.Rank1', 'emergency_measures.7']\n",
      "total features: 48\n",
      "selected features: 35\n",
      "dropped features: 13\n",
      "selected features: \n",
      "['water_depth_cm', 'shp_monetary_resources4prevention', 'bage', 'perception_who_responsible4protection.Rank1', 'emergency_measures.7', 'flood_experience', 'inundation_duration_h', 'shp_registered_capital_euro', 'resilience_govern_warnings_helpful', 'elevation_building_height_cm', 'elevation_m', 'resilience_left_alone', 'resilience_govern_careing', 'elevation_building_impl', 'shp_employees', 'shp_profits_last5years', 'shp_content_value_euro', 'hh_monthly_income_cat', 'overall_problem_house', 'resistant_material_building_impl', 'flowvelocity', 'protect_valuables_impl', 'water_barriers_impl', 'shp_finance_investments', 'emergency_measures.1', 'resilience_neighbor_management', 'emergency_measures.4', 'elevation_rel2surrounding_cat', 'emergency_measures.2', 'emergency_measures.6', 'perception_private_economy_future', 'resilience_city_protection', 'shp_suppliers_HCMC', 'emergency_measures.8', 'flood_protections_impl']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_xgboost_businessreduction_pipe_xgb.xlsx\n",
      "\n",
      "Training set\n",
      "\n",
      "    Model Performance:\n",
      "        Mean Squared Error: 1.5\n",
      "        Root Mean Square Error: 1.2\n",
      "        Mean Absolute Error: 0.8\n",
      "        Mean Absolute Percentage Error: inf\n",
      "        R²-Score: 1.0\n",
      "        Adjusted R²-Score: 1.0\n",
      "    \n",
      "\n",
      "Testing set\n",
      "\n",
      "    Model Performance:\n",
      "        Mean Squared Error: 453.7\n",
      "        Root Mean Square Error: 21.3\n",
      "        Mean Absolute Error: 13.6\n",
      "        Mean Absolute Percentage Error: inf\n",
      "        R²-Score: 0.38\n",
      "        Adjusted R²-Score: -0.87\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "## iterate over both targets and store results \n",
    "\n",
    "fi_threshold = 0.000\n",
    "eval_set_list = []\n",
    "\n",
    "#targets = [\"Target_contentloss_euro\"]# ,\n",
    "targets = [\"Target_businessreduction\"]\n",
    "#importances_threshold = {\"target_contentloss_euro\": 0.000, \"Target_businessreduction\": 0.000 }\n",
    "\n",
    "plt.ioff()  # Prevent plt showing stuff\n",
    "\n",
    "\n",
    "\n",
    "for target in targets:\n",
    "\n",
    "    ## iterate over piplines. Each piplines contains precrosseing methods and several  classifier\n",
    "    pipelines = [\"pipe_xgb\"]#, \"pipe_ximput_xgb\"]\n",
    "        \n",
    "\n",
    "    for pipe_name in pipelines:\n",
    "\n",
    "        print( f\"\\nApply XGBoost on {target}, with pipeline {pipe_name}:\")\n",
    "\n",
    "        ## load sinlge pipeline\n",
    "        pipe = joblib.load(f'./pipelines/{pipe_name}.pkl')\n",
    "        \n",
    "        \n",
    "        df_candidates_t = df_candidates\n",
    "\n",
    "        ## TEST run xgb with and without nan in X\n",
    "        ## clean df from remaining records containg nan\n",
    "        #df_candidates_t = df_candidates_t.dropna()\n",
    "        #df_candidates_t = df_candidates_t[df_candidates_t[target]!=0.0]\n",
    "\n",
    "        #print(\"Amount of missing target values should be zero: \", df_candidates_t[target].isna().sum())\n",
    "        print(\"Uses \", df_candidates_t.shape[0], \" records, from those have \", \n",
    "            { (df_candidates_t[target][df_candidates_t[target]==0.0]).count() }, f\" records zero {target.split('_')[1]}\")\n",
    "\n",
    "\n",
    "        ## drop samples where target is nan\n",
    "        print(f\"Dropping {df_candidates_t[f'{target}'].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "        df_candidates_t = df_candidates_t[ ~df_candidates_t[f\"{target}\"].isna()]\n",
    "\n",
    "\n",
    "        if pipe_name != \"pipe_ximput_xgb\":\n",
    "            pass\n",
    "            # ## drop instances where target is nan\n",
    "            # print(\"Before dropping records with nan\", df_candidates_t.shape)\n",
    "            # df_candidates_t = df_candidates_t.dropna()\n",
    "            # print(\"After dropping records with nan\", df_candidates_t.shape)\n",
    "        else:\n",
    "            ##impute nans in X\n",
    "            for c in df_candidates_t.drop(targets, axis=1).columns: \n",
    "                #df_candidates_t[f\"{c}\"].fillna(value=np.nanmedian(df_candidates_t[f\"{c}\"]), inplace=True)\n",
    "                df_candidates_t[c].fillna(df_candidates_t[c].median(), inplace=True)\n",
    "       \n",
    "        # split into predictors and target variable\n",
    "        X_unscaled = df_candidates_t.drop([target] + targets, axis=1)  # remove targets from X\n",
    "        y = df_candidates_t[target]\n",
    "        \n",
    "        ## test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_unscaled, y, test_size=0.20, \n",
    "            random_state=seed, shuffle=True\n",
    "        )\n",
    "        ## save evaluation set for later usage in feature importance\n",
    "        eval_set =  pd.concat([y_test, X_test], axis=1) #[(X_test, y_test)]\n",
    "        eval_set_list.append({pipe_name : eval_set})\n",
    "        \n",
    "        print(\"Training set size\", X_train.shape[0])\n",
    "        print(\"Test set size\", X_test.shape[0])\n",
    "\n",
    "        ## normalize data \n",
    "        X_train, X_test = fs.normalize_X(X_train, X_test)\n",
    "\n",
    "        ## Hyperparmaters and CV\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed) \n",
    "        #cv = RepeatedKFold(n_splits=20, n_repeats=2, random_state=seed)\n",
    "        model_cv = RandomizedSearchCV(   #         #GridSearchCV(\n",
    "            estimator=pipe, \n",
    "            #param_grid=param_grid,\n",
    "            param_distributions=param_grid, \n",
    "            #random_state=seed,\n",
    "            cv=cv,\n",
    "            scoring= \"neg_mean_absolute_error\",\n",
    "            #best_ntree_score(XGBRegressor, X_train, y_train) , #\"neg_mean_absolute_error\",   #\n",
    "            refit=True,   ## Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "                            ## If refit=False, clf.fit() will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after fit().\n",
    "                            ## ! When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit()\n",
    "            verbose=False,\n",
    "        )\n",
    "                # Nested CV with parameter optimization\n",
    "        # nested_score = cross_val_score(model_cv, X=X_train, y=y_train, cv=outer_cv)\n",
    "        # nested_scores = nested_score.mean()\n",
    "        # print(\"nested_scores\", nested_scores)\n",
    "\n",
    "        ## Fit best model on training set\n",
    "        model_cv.fit(\n",
    "            X_train, y_train,\n",
    "            model__early_stopping_rounds=3,\n",
    "            model__eval_metric=\"mae\",\n",
    "            model__eval_set=[(X_test, y_test)],\n",
    "            model__verbose=False\n",
    "            )\n",
    " \n",
    "        print(f\"Best hyperparams: {model_cv.best_params_}\")\n",
    "        # print(\"Train R^2 Score : %.1f\" %model_cv.best_estimator_.score(X_train, y_train))\n",
    "        #print(\"MAE of best model: %.1f\" %model_cv.best_score_,\" on iteration \", model_cv.best_estimator_.best_iteration)  \n",
    "\n",
    "        # fit model again with best hyperparams\n",
    "        print(\"Create new XGBoost model based on best hyperparameters\")\n",
    "        model = model_cv.best_estimator_\n",
    "         \n",
    "        ## store best trained model for evaluation\n",
    "        filename = f'./models_trained/xgboost_{target}_{pipe_name}.sav'\n",
    "        #pickle.dump(model_cv.best_estimator_, open(filename, 'wb'))\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        ## Evaluate model\n",
    "        # print(f\"Training set score (R^2): {round(model.score(X_train, y_train), 2)}\")  # how well did the model on the training set\n",
    "        # print(f\"Test set score (R^2): {model_cv.score(X_test, y_test)}\")   # .. compared to the unseen test set for overfitting - acutal not needed\n",
    "        # r2 = variance explained by model / total variance --> higher r2= better fitted model\n",
    "\n",
    "        ## get signifcant features based on absolute coeff values\n",
    "        print(\"\\nSelect features based on permutation feature importance\")\n",
    "\n",
    "        # ## select significant features byPermuation feature importance\n",
    "        importances = e.permutation_feature_importance(model, X_test, y_test, repeats=5, seed=seed)\n",
    "\n",
    "        df_importance = pd.DataFrame(\n",
    "            {\"importances\" : importances[0]},\n",
    "            index=X_train.columns.to_list(),\n",
    "            ) \n",
    "        df_importance = df_importance.sort_values(\"importances\", ascending=False)  # get most important features to the top\n",
    "        print(\"Most important features:\", df_importance.iloc[:5].index.to_list())\n",
    "        df_importance = df_importance.loc[df_importance.importances >= fi_threshold, : ]\n",
    "        #df_importance.head(5)\n",
    "        # ## write selected predictors and response to disk\n",
    "        fs.save_selected_features(\n",
    "            X_train, \n",
    "            pd.DataFrame(y_train, columns=[target]), \n",
    "            df_importance.T.columns, \n",
    "            filename=f\"../../../input_survey_data/fs_xgboost_{target.split('_')[1]}_{pipe_name}.xlsx\"\n",
    "        )\n",
    "\n",
    "        ## Evaluate\n",
    "        ## print evaluation report + check for overfitting \n",
    "        print(\"\\nTraining set\")\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        #y_pred_train = model_cv.best_estimator_.predict(X_train)\n",
    "        e.evaluation_report(y_train, y_pred_train,\n",
    "                            X_unscaled.shape[1], \n",
    "                            filepath=f\"./models_evaluation/xgboost/eval_train_{target.split('_')[1]}_{pipe_name}.csv\")\n",
    "\n",
    "        print(\"\\nTesting set\")\n",
    "        #y_pred = model_cv.best_estimator_.predict(X_test)\n",
    "        y_pred = model.predict(X_test)\n",
    "        e.evaluation_report(y_test, y_pred, \n",
    "                            X_unscaled.shape[1], \n",
    "                            filepath=f\"./models_evaluation/xgboost/eval_test_{target.split('_')[1]}_{pipe_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.best_params_\n",
    "\n",
    "##  MAE: + in target unit +  less likely to be affected by extreme values.\n",
    "## MAPE: r (MAPE) quantifies the average absolute difference between \n",
    "## the anticipated and observed values of the target variable as a percentage of the observed value. \n",
    "## The method works well for assessing models where the target variable spans a broad range of scales\n",
    "\n",
    "# 75 % in train mit 300 trress, subsam0.8, maxdep=3, aber bad teest R2\n",
    "\n",
    "\n",
    "## R²-Score: 0.46\n",
    "# {'model__validate_parameters': True,\n",
    "#  'model__n_estimators': 800,\n",
    "#  'model__max_depth': 10,\n",
    "#  'model__eta': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save best model, its hyperparamters, and evlation scores and selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Performance:\n",
      "        Mean Squared Error: 1.5\n",
      "        Root Mean Square Error: 1.2\n",
      "        Mean Absolute Error: 0.8\n",
      "        Mean Absolute Percentage Error: inf\n",
      "        R²-Score: 1.0\n",
      "        Adjusted R²-Score: 1.0\n",
      "    \n",
      "\n",
      "    Model Performance:\n",
      "        Mean Squared Error: 453.7\n",
      "        Root Mean Square Error: 21.3\n",
      "        Mean Absolute Error: 13.6\n",
      "        Mean Absolute Percentage Error: inf\n",
      "        R²-Score: 0.38\n",
      "        Adjusted R²-Score: -0.87\n",
      "    \n",
      "total features: 48\n",
      "selected features: 35\n",
      "dropped features: 13\n",
      "selected features: \n",
      "['water_depth_cm', 'shp_monetary_resources4prevention', 'bage', 'perception_who_responsible4protection.Rank1', 'emergency_measures.7', 'flood_experience', 'inundation_duration_h', 'shp_registered_capital_euro', 'resilience_govern_warnings_helpful', 'elevation_building_height_cm', 'elevation_m', 'resilience_left_alone', 'resilience_govern_careing', 'elevation_building_impl', 'shp_employees', 'shp_profits_last5years', 'shp_content_value_euro', 'hh_monthly_income_cat', 'overall_problem_house', 'resistant_material_building_impl', 'flowvelocity', 'protect_valuables_impl', 'water_barriers_impl', 'shp_finance_investments', 'emergency_measures.1', 'resilience_neighbor_management', 'emergency_measures.4', 'elevation_rel2surrounding_cat', 'emergency_measures.2', 'emergency_measures.6', 'perception_private_economy_future', 'resilience_city_protection', 'shp_suppliers_HCMC', 'emergency_measures.8', 'flood_protections_impl']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/best_fs_xgb_businessreduction_pipe_xgb.xlsx\n"
     ]
    }
   ],
   "source": [
    "## Stored best xgb model for business reduction: best_xgboost_Target_businessreduction_pipe_xgb.sav\n",
    "## Its hyperaprams xgb_best_param_model_cv.best_params_  # early_stop=3\n",
    "\n",
    "## best model and hyperparams (here early stoping =3)\n",
    "filename = f'./models_trained/best_xgb_{target}_{pipe_name}.sav'\n",
    "if not glob(filename):\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "best_params = model_cv.best_params_\n",
    "best_params = pd.DataFrame({\"hyperparameter\":best_params.keys(), \"value\":best_params.values()})\n",
    "filepath = f'./models_evaluation/best_xgb_hyperparams_{target}_{pipe_name}.sav'\n",
    "if not glob(filepath):\n",
    "    best_params.to_csv(filename, index = False)\n",
    "\n",
    "## eval report\n",
    "y_pred_train = model.predict(X_train)\n",
    "e.evaluation_report(y_train, y_pred_train,\n",
    "                    X_unscaled.shape[1], \n",
    "                    filepath=f\"./models_evaluation/xgboost/best_eval_train_{target.split('_')[1]}_{pipe_name}.csv\")\n",
    "y_pred = model.predict(X_test)\n",
    "e.evaluation_report(y_test, y_pred, \n",
    "                    X_unscaled.shape[1], \n",
    "                    filepath=f\"./models_evaluation/xgboost/best_eval_test_{target.split('_')[1]}_{pipe_name}.csv\")\n",
    "\n",
    "## selected features\n",
    "fs.save_selected_features(\n",
    "        X_train, \n",
    "        pd.DataFrame(y_train, columns=[target]), \n",
    "        df_importance.T.columns, \n",
    "        filename=f\"../../../input_survey_data/best_fs_xgb_{target.split('_')[1]}_{pipe_name}.xlsx\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "## Plot learning rate see if    \n",
    "means = model_cv.cv_results_['mean_test_score']\n",
    "stds = model_cv.cv_results_['std_test_score']\n",
    "params = model_cv.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# plot results\n",
    "# scores = np.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "# for i, value in enumerate(learning_rate):\n",
    "#     plt.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "# \tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "#plt.use('Agg')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "plt.errorbar(learning_rate, means, yerr=stds)\n",
    "plt.legend()\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('log_loss_vs_learning_rate.png')\n",
    "plt.show()\n",
    "# scores = np.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "# for i, value in enumerate(learning_rate):\n",
    "#     plt.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\n",
    "# plt.legend()\n",
    "# plt.xlabel('learning_rate')\n",
    "# plt.ylabel('Log Loss')\n",
    "# plt.savefig('n_estimators_vs_learning_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9655423861987216\n",
      "0.37493201206736726\n"
     ]
    }
   ],
   "source": [
    "#plt.savefig(f\"./models_trained/FI_{target}.png\", bbox_inches='tight')\n",
    "#sns_plot.figure.savefig(\"output.png\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target_contentloss_euro\n"
     ]
    }
   ],
   "source": [
    "## reload models\n",
    "\n",
    "target = targets[0]\n",
    "print(target)\n",
    "\n",
    "model_eval = pickle.load(open(f\"./models_trained/xgboost_{target}_{pipe_name}.sav\", 'rb'))\n",
    "#model_eval.get_params()\n",
    "#dir(model_eval)#.feature_importances_[model_eval.feature_importances_>0.015].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "Have the same feature importance method across all applied ML models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>resilience_more_future_affected</td>\n",
       "      <td>-0.037958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b_area</td>\n",
       "      <td>-0.018347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emergency_measures.3</td>\n",
       "      <td>-0.017422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pumping_equipment_impl</td>\n",
       "      <td>-0.006612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>electricity_higher_impl</td>\n",
       "      <td>-0.005806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  importances\n",
       "35  resilience_more_future_affected    -0.037958\n",
       "24                           b_area    -0.018347\n",
       "6              emergency_measures.3    -0.017422\n",
       "15           pumping_equipment_impl    -0.006612\n",
       "18          electricity_higher_impl    -0.005806"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Permuation feature importance\n",
    "result = e.permutation_feature_importance(model, X_test, y_test, repeats=5, seed=seed)\n",
    "\n",
    "df_importance = pd.DataFrame({\n",
    "    \"name\" : X_train.columns.to_list(),\n",
    "    \"importances\" : result[0],\n",
    "     }) \n",
    "df_importance = df_importance.sort_values(\"importances\", ascending=True)  # get most important features to the top\n",
    "df_importance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "\n",
    "# f.plot_feature_importance(df_importance.importances, n=10, figure_size=(20, 15), target=target)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "# drop features which dont reduce the loss\n",
    "df_importance = df_importance.loc[df_importance.importances >= 0.001, : ] \n",
    "#sorted_idx = model_eval.feature_importances_.argsort()\n",
    "#plt.barh(df_importance.name[-5:], df_importance.importances[-5:])\n",
    "plt.barh(df_importance.name, df_importance.importances)\n",
    "#plt.bar(X_train.columns[sorted_idx[:15]], model_eval.feature_importances_[sorted_idx[:15]])\n",
    "plt.xticks(\n",
    "   # ticks = range(len(X_train.columns[sorted_idx[:15]])),\n",
    "   # labels =X_train.columns[sorted_idx[:15],],\n",
    "    rotation = 90\n",
    "    )\n",
    "plt.title(f\"Feature Importances for {target}\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'../../../figures/best_xgb_feature_importance_{target}_{pipe_name}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical clustering on Spearman rank correlation\n",
    "\n",
    "Select only feautres with low collienarity to solve disadvantage of perumation feature importance.\n",
    "Randomizing one feature would lead to only small importance score - the model performance wouldnt be move influenced - due that the information is included in other correlated features. Removing one feature keeps the similar inforamtion in the other feautres unchanged and the model learns from the correlated feature. Therefore apply hierachical clustering to select less correlated features\n",
    "\n",
    "See also:\n",
    "- Brill 2020 (dissertation)\n",
    "- https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html # code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.cluster.hierarchy as shc\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.title(\"Customers Dendrogram\")\n",
    "\n",
    "# # Selecting Annual Income and Spending Scores by index\n",
    "# selected_data = X_train.dropna()\n",
    "# selected_data = selected_data.T # only possible with out nan\n",
    "# clusters = shc.linkage(selected_data, \n",
    "#             method='ward', optimal_ordering=False,\n",
    "#             metric=\"euclidean\")\n",
    "# shc.dendrogram(Z=clusters, \n",
    "#                #p=20, # p -> value for truncation mode\n",
    "#                orientation=\"right\",\n",
    "#                labels=X_train.columns\n",
    "#                ) \n",
    "# plt.show()\n",
    "\n",
    "# ## TODO adapt with spearman rank order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.stats import spearmanr\n",
    "# from scipy.spatial.distance import squareform\n",
    "# from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# corr = spearmanr(X_unscaled_no_nan).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = ward(distance_matrix, checks=False )\n",
    "# dendro = dendrogram(\n",
    "#     dist_linkage, labels=X_unscaled_no_nan.columns.tolist(), ax=ax1, leaf_rotation=90\n",
    "# )\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closs hyperapram , no model__early_stopping_rounds, repeatedcv wit h10 folds\n",
    "## best train R2: ntree=30, max_depth =1, no furhter params\n",
    "\n",
    "# learning_rate = [ 0.00001, 0.0001, 0.001, 0.1, 0.2]#, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9] # store outside, for plotting\n",
    "# n_estimators = [ 50, 100, 200, 300, 500, 800]\n",
    "\n",
    "# param_grid = {'model__n_estimators': n_estimators,\n",
    "#     #'model__n_estimators': [ 3, 5, 10, 20], # get only low train scores with this\n",
    "#               'model__max_depth': [1, 2, 3, 5, 7, 8, 10, 15],\n",
    "#               #'model__max_leaves': [0, 3, 5],\n",
    "#              # 'model__colsample_bytree': [ 0.3, 0.5, 0.7, 1.0 ], # Percentage of columns to be randomly samples for each tree\n",
    "#              # 'model__colsample_bynode': [ 0.3, 0.5, 0.7, 1.0], # nbr of feautres for each split point\n",
    "#              # 'model__eta': learning_rate,  # == eta\n",
    "#             #   'model__gamma': [0.2, 0.3, 0.5, 0.8, 1, 3] , # min_split_loss -  larger gamma is, the more conservative the algorithm is\n",
    "#               'model__subsample': [0.0, 0.2, 0.5, 0.6, 0.8, 0.9],  # define subsample of train st prior to growing trees, prevent overfitting\n",
    "#             #  'model__reg_alpha': [0.5, 1.0, 2.0, 4.0, 5.0, 6.0 ,7.0],   # Lasso Regularization term on weights , higher values = more consrvative \n",
    "#             #  'model__reg_lambda': [0.0, 0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0],  # Ridge Regularization term on weights ,  higher values = more consrvative\n",
    "#             #   'model__min_child_weight': [0, 1, 2, 3, 4,],\n",
    "#             #   \"model__max_delta_step\":  [0, 3, 5, 6, 7],           # for LogisticReg good to solve imbalance \n",
    "#           #   'model__objective': [None, 'reg:absoluteerror'],#'multi:softprob,'reg:squarederror','reg:models_trained'],\n",
    "#           #  # 'model__tree_method': [\"hist\", \"gpu_hist\"],\n",
    "#           #   'model__booster': [None, \"gblinear\", \"gbtree\"],\n",
    "#             \"model__validate_parameters\":[True],\n",
    "#               }\n",
    "\n",
    "# # 'model__scale_pos_weight': [0.0, 0.3, 0.5, 0.7, 0.9, 1.0],  # only  for clasifcation: handle imbalance, ratio between negative and positive examples\n",
    "\n",
    "# # Objective candidate: multi:softmax\n",
    "# # Objective candidate: multi:softprob\n",
    "# # Objective candidate: reg:squarederror\n",
    "# # Objective candidate: reg:squaredlogerror\n",
    "# # Objective candidate: reg:logistic\n",
    "# ## Objective candidate: reg:linear\n",
    "# # Objective candidate: reg:pseudohubererror\n",
    "# # Objective candidate: reg:gamma\n",
    "# # Objective candidate: reg:absoluteerror\n",
    "\n",
    "# ## DOC: https://xgboost.readthedocs.io/en/stable/parameter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = shc.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "X_train_sel = X_train[:, selected_features]\n",
    "X_test_sel = X_test[:, selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## permutation based FI (build in func from skilearn)\n",
    "\n",
    "# perm_importance = permutation_importance(xgb, X_test, y_test)\n",
    "# The visualization of the importance:\n",
    "\n",
    "# sorted_idx = perm_importance.importances_mean.argsort()\n",
    "# plt.barh(boston.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
