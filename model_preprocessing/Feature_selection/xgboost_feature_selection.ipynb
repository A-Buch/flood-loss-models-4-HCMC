{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection done by eXtreme Gradient Boosting (XGBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\rpy2\\robjects\\packages.py:367: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedKFold, RepeatedStratifiedKFold, RepeatedKFold, cross_val_score, cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline  # make sure not to mix it with sklearn Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../../../\")\n",
    "import utils.utils_feature_selection as fs\n",
    "import utils.utils_evaluation as e\n",
    "import utils.utils_figures as f\n",
    "import utils.settings as s\n",
    "import utils.pipelines as p\n",
    "\n",
    "s.init()\n",
    "seed = s.seed\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_contentloss_euro</th>\n",
       "      <th>Target_businessreduction</th>\n",
       "      <th>inundation_duration_h</th>\n",
       "      <th>water_depth_cm</th>\n",
       "      <th>contaminations.0</th>\n",
       "      <th>flowvelocity</th>\n",
       "      <th>warning_time_h</th>\n",
       "      <th>emergency_measures.1</th>\n",
       "      <th>emergency_measures.2</th>\n",
       "      <th>emergency_measures.3</th>\n",
       "      <th>emergency_measures.4</th>\n",
       "      <th>emergency_measures.6</th>\n",
       "      <th>emergency_measures.7</th>\n",
       "      <th>emergency_measures.8</th>\n",
       "      <th>emergency_measures.9</th>\n",
       "      <th>overall_problem_house</th>\n",
       "      <th>protect_valuables_impl</th>\n",
       "      <th>water_barriers_impl</th>\n",
       "      <th>pumping_equipment_impl</th>\n",
       "      <th>elevation_building_impl</th>\n",
       "      <th>resistant_material_building_impl</th>\n",
       "      <th>electricity_higher_impl</th>\n",
       "      <th>flood_protections_impl</th>\n",
       "      <th>flood_experience</th>\n",
       "      <th>elevation_building_height_cm</th>\n",
       "      <th>elevation_rel2surrounding_cat</th>\n",
       "      <th>bage</th>\n",
       "      <th>b_area</th>\n",
       "      <th>hh_monthly_income_cat</th>\n",
       "      <th>shp_owner</th>\n",
       "      <th>shp_sector</th>\n",
       "      <th>shp_employees</th>\n",
       "      <th>shp_avgmonthly_sale_cat</th>\n",
       "      <th>shp_finance_investments</th>\n",
       "      <th>shp_profits_last5years</th>\n",
       "      <th>shp_risk_tolerance</th>\n",
       "      <th>shp_monetary_resources4prevention</th>\n",
       "      <th>resilience_city_protection</th>\n",
       "      <th>resilience_more_future_affected</th>\n",
       "      <th>resilience_govern_warnings_helpful</th>\n",
       "      <th>resilience_govern_careing</th>\n",
       "      <th>resilience_left_alone</th>\n",
       "      <th>resilience_neighbor_management</th>\n",
       "      <th>perception_who_responsible4protection.Rank1</th>\n",
       "      <th>perception_private_economy_future</th>\n",
       "      <th>contaminations_light</th>\n",
       "      <th>contaminations_heavy</th>\n",
       "      <th>shp_suppliers_HCMC</th>\n",
       "      <th>shp_content_value_euro</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>shp_registered_capital_euro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83886</td>\n",
       "      <td>11047.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87277</td>\n",
       "      <td>736.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target_contentloss_euro  Target_businessreduction  inundation_duration_h   \n",
       "395                      0.0                       NaN                    4.0  \\\n",
       "396                      0.0                       0.0                    3.0   \n",
       "\n",
       "     water_depth_cm  contaminations.0  flowvelocity  warning_time_h   \n",
       "395            70.0                 0             1             NaN  \\\n",
       "396           100.0                 0             1             NaN   \n",
       "\n",
       "     emergency_measures.1  emergency_measures.2  emergency_measures.3   \n",
       "395                     1                     0                     1  \\\n",
       "396                     1                     0                     1   \n",
       "\n",
       "     emergency_measures.4  emergency_measures.6  emergency_measures.7   \n",
       "395                     0                     1                     0  \\\n",
       "396                     0                     0                     0   \n",
       "\n",
       "     emergency_measures.8  emergency_measures.9  overall_problem_house   \n",
       "395                     0                     0                      1  \\\n",
       "396                     0                     0                      0   \n",
       "\n",
       "     protect_valuables_impl  water_barriers_impl  pumping_equipment_impl   \n",
       "395                       1                    5                       1  \\\n",
       "396                       1                    5                       5   \n",
       "\n",
       "     elevation_building_impl  resistant_material_building_impl   \n",
       "395                        1                                 5  \\\n",
       "396                        5                                 5   \n",
       "\n",
       "     electricity_higher_impl  flood_protections_impl  flood_experience   \n",
       "395                        5                       5                 5  \\\n",
       "396                        5                       5                 4   \n",
       "\n",
       "     elevation_building_height_cm  elevation_rel2surrounding_cat  bage   \n",
       "395                          70.0                              1   NaN  \\\n",
       "396                           NaN                              0   5.0   \n",
       "\n",
       "     b_area  hh_monthly_income_cat  shp_owner  shp_sector  shp_employees   \n",
       "395   130.0                    NaN          1          17              2  \\\n",
       "396    33.0                    1.0          1          11              2   \n",
       "\n",
       "     shp_avgmonthly_sale_cat  shp_finance_investments  shp_profits_last5years   \n",
       "395                        3                        1                     4.0  \\\n",
       "396                        3                        1                     4.0   \n",
       "\n",
       "     shp_risk_tolerance  shp_monetary_resources4prevention   \n",
       "395                 3.0                                3.0  \\\n",
       "396                 3.0                                4.0   \n",
       "\n",
       "     resilience_city_protection  resilience_more_future_affected   \n",
       "395                         1.0                              5.0  \\\n",
       "396                         NaN                              NaN   \n",
       "\n",
       "     resilience_govern_warnings_helpful  resilience_govern_careing   \n",
       "395                                 1.0                        1.0  \\\n",
       "396                                 NaN                        NaN   \n",
       "\n",
       "     resilience_left_alone  resilience_neighbor_management   \n",
       "395                      5                             1.0  \\\n",
       "396                      5                             NaN   \n",
       "\n",
       "     perception_who_responsible4protection.Rank1   \n",
       "395                                          2.0  \\\n",
       "396                                          3.0   \n",
       "\n",
       "     perception_private_economy_future  contaminations_light   \n",
       "395                                3.0                     1  \\\n",
       "396                                3.0                     1   \n",
       "\n",
       "     contaminations_heavy  shp_suppliers_HCMC  shp_content_value_euro   \n",
       "395                     0                   1                     NaN  \\\n",
       "396                     0                   1                     NaN   \n",
       "\n",
       "     elevation_m  shp_registered_capital_euro  \n",
       "395      1.83886                      11047.7  \n",
       "396      1.87277                        736.5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates = pd.read_excel(\"../../../input_survey_data/input_data_business.xlsx\")\n",
    "df_candidates.tail(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted b their number of nan values\n",
      " warning_time_h                  77.581864\n",
      "elevation_building_height_cm    15.869018\n",
      "shp_content_value_euro          15.869018\n",
      "shp_registered_capital_euro     11.838791\n",
      "Target_businessreduction         9.068010\n",
      "dtype: float64\n",
      "(397, 51)\n"
     ]
    }
   ],
   "source": [
    "## delete features with more than 10% missing values\n",
    "\n",
    "# print(df_candidates.isna().sum(axis=0).sort_values(ascending=False))\n",
    "print(\"Features sorted b their number of nan values\\n\", (df_candidates.isna().mean()*100).sort_values(ascending=False).head(5))\n",
    "print(df_candidates.shape)\n",
    "## --> keep these feature due that they could be important for model performance (based on findings from other studies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.DataFrame()\n",
    "# X = df_candidates.drop([\"Target_contentloss_euro\", \"Target_businessreduction\"], axis=1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# d = scaler.fit_transform(X)\n",
    "# X_scaled = pd.DataFrame(d, columns=X.columns)\n",
    "\n",
    "# for f in [\"warning_time_h\", \"shp_registered_capital_euro\", \"elevation_building_height_cm\",  \"shp_content_value_euro\"]:\n",
    "#     X_scaled[f\"{f}\"] = X_scaled[f\"{f}\"].replace(np.nan, np.nanmedian(X_scaled[f\"{f}\"]))\n",
    "\n",
    "# X_scaled_drop_nan = X_scaled.dropna()\n",
    "\n",
    "# df_vif = fs.vif_score(X_scaled_drop_nan).reset_index(drop=True)\n",
    "# print(df_vif.sort_values(\"vif_scores\", ascending=False).head(15)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target varibale distirbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 20000.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closses = df_candidates.Target_contentloss_euro[df_candidates.Target_contentloss_euro != 0.0 ]\n",
    "closses.describe()\n",
    "\n",
    "\n",
    "closses.hist(bins=2000, figsize=(8, 8))\n",
    "plt.ylim(1, 20)\n",
    "plt.xlim(1, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'xgbclassifier__n_estimators': [10, 20 ,30, 40, 50, 60, 100],\n",
    "              'xgbclassifier__max_depth': [1, 2, 3, 4, 5, 6],\n",
    "              'xgbclassifier__colsample_bytree': [0.005, 0.1, 0.3, 0.5], # Percentage of columns to be randomly samples for each tree\n",
    "              'xgbclassifier__colsample_bynode': [0.05, 0.1, 0.2], # nbr of feautres for each split point\n",
    "              'xgbclassifier__learning_rate': [0.001, 00.1, 0.1, 0.3, 0.5, 0.7],  # == eta\n",
    "              'xgbclassifier__gamma': [0.5, 1, 2, 3] , # min_split_loss -  larger gamma is, the more conservative the algorithm is\n",
    "              #'subsample': 0.8  # define subsample of train st, xgb has not bootstrapping\n",
    "              'xgbclassifier__reg_alpha': [0, 0.5, 1.0, 2.0],   # Lasso Regularization term on weights , higher values = more consrvative \n",
    "              'xgbclassifier__reg_lambda': [0.0, 0.5, 1.0, 2.0],  # Ridge Regularization term on weights ,  higher values = more consrvative\n",
    "              'xgbclassifier__min_child_weight': [0, 1, 2, 3, 4,],\n",
    "              'xgbclassifier__objective': [None, 'reg:linear'],\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply XGBoost on Target_contentloss_euro, with pipeline pipe_us_xgb:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Discretization of target:\n",
      "156 records are euqally split into categories, so that same number of records is in each class (equal frequency binning) \n",
      "Group labels and bins : [1, 2, 3, 4] Target_contentloss_euro\n",
      "(7.399, 39.7]        41\n",
      "(137.95, 382.7]      40\n",
      "(382.7, 224190.4]    38\n",
      "(39.7, 137.95]       37\n",
      "Name: count, dtype: int64\n",
      "check if correct discretized: Target_contentloss_euro_c\n",
      "0    226\n",
      "1     41\n",
      "3     40\n",
      "4     38\n",
      "2     37\n",
      "Name: count, dtype: int64\n",
      "Balancing will result in following class frequencies\n",
      "Counter({0: 203, 3: 38, 1: 36, 2: 35, 4: 31})\n",
      "Counter({0: 50, 3: 38, 1: 36, 2: 35, 4: 31})\n",
      "Best hyperparams: {'xgbclassifier__reg_lambda': 1.0, 'xgbclassifier__reg_alpha': 0.5, 'xgbclassifier__objective': None, 'xgbclassifier__n_estimators': 40, 'xgbclassifier__min_child_weight': 4, 'xgbclassifier__max_depth': 1, 'xgbclassifier__learning_rate': 0.001, 'xgbclassifier__gamma': 3, 'xgbclassifier__colsample_bytree': 0.005, 'xgbclassifier__colsample_bynode': 0.05}\n",
      "Create new XGBoost model based on best hyperparameters\n",
      "\n",
      "Select features based on permutation feature importance\n",
      "Most important features: ['Target_businessreduction', 'resilience_more_future_affected', 'hh_monthly_income_cat', 'shp_owner', 'shp_sector']\n",
      "total features: 50\n",
      "selected features: 50\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['Target_businessreduction', 'resilience_more_future_affected', 'hh_monthly_income_cat', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_govern_warnings_helpful', 'inundation_duration_h', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'b_area', 'bage', 'elevation_rel2surrounding_cat', 'elevation_building_height_cm', 'water_depth_cm', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_xgboost_contentloss_pipe_us_xgb_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.59\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.59\n",
      "\n",
      "Apply XGBoost on Target_contentloss_euro, with pipeline pipe_ximput_us_xgb:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Discretization of target:\n",
      "156 records are euqally split into categories, so that same number of records is in each class (equal frequency binning) \n",
      "Group labels and bins : [1, 2, 3, 4] Target_contentloss_euro\n",
      "(7.399, 39.7]        41\n",
      "(137.95, 382.7]      40\n",
      "(382.7, 224190.4]    38\n",
      "(39.7, 137.95]       37\n",
      "Name: count, dtype: int64\n",
      "check if correct discretized: Target_contentloss_euro_c\n",
      "0    226\n",
      "1     41\n",
      "3     40\n",
      "4     38\n",
      "2     37\n",
      "Name: count, dtype: int64\n",
      "Balancing will result in following class frequencies\n",
      "Counter({0: 203, 3: 38, 1: 36, 2: 35, 4: 31})\n",
      "Counter({0: 50, 3: 38, 1: 36, 2: 35, 4: 31})\n",
      "Best hyperparams: {'xgbclassifier__reg_lambda': 1.0, 'xgbclassifier__reg_alpha': 0.5, 'xgbclassifier__objective': None, 'xgbclassifier__n_estimators': 40, 'xgbclassifier__min_child_weight': 4, 'xgbclassifier__max_depth': 1, 'xgbclassifier__learning_rate': 0.001, 'xgbclassifier__gamma': 3, 'xgbclassifier__colsample_bytree': 0.005, 'xgbclassifier__colsample_bynode': 0.05}\n",
      "Create new XGBoost model based on best hyperparameters\n",
      "\n",
      "Select features based on permutation feature importance\n",
      "Most important features: ['Target_businessreduction', 'resilience_more_future_affected', 'hh_monthly_income_cat', 'shp_owner', 'shp_sector']\n",
      "total features: 50\n",
      "selected features: 50\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['Target_businessreduction', 'resilience_more_future_affected', 'hh_monthly_income_cat', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_govern_warnings_helpful', 'inundation_duration_h', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'b_area', 'bage', 'elevation_rel2surrounding_cat', 'elevation_building_height_cm', 'water_depth_cm', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_xgboost_contentloss_pipe_ximput_us_xgb_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.59\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.59\n"
     ]
    }
   ],
   "source": [
    "## iterate over both targets and store results \n",
    "#sampling_strategy = \"majority\"\n",
    "#\"not minority\" # =resample all classes bot not smallest class \n",
    "#\"majority\"= only largest class is et to size of smallest class, intermediate classe are untouched\n",
    "\n",
    "zero_loss_ratio = 0.25\n",
    "fi_threshold = 0.000\n",
    "\n",
    "def ratio_multiplier(y, sampling_stategy=zero_loss_ratio):\n",
    "    \"\"\"\"\n",
    "    https://imbalanced-learn.org/stable/auto_examples/api/plot_sampling_strategy_usage.html#sphx-glr-auto-examples-api-plot-sampling-strategy-usage-py\n",
    "    y : y train\n",
    "    sampling_stategy = str defineing imblearn stategy or float between 0-1.0 defining ratio to which y is reduced .e.g. (0.75 >- y will be 75% of its former size)\n",
    "    \"\"\"\n",
    "    # if sampling_stategy == \"majority\": \n",
    "    #     return sampling_stategy\n",
    "    # else:\n",
    "    multiplier = {0: zero_loss_ratio} # set only zero loss class to the half or 3/4 of its size\n",
    "    target_stats = Counter(y)\n",
    "    for key, value in target_stats.items():\n",
    "        if key in multiplier:\n",
    "            target_stats[key] = int(value * multiplier[key])\n",
    "    return target_stats\n",
    "\n",
    "\n",
    "\n",
    "targets = [\"Target_contentloss_euro\"]#, \"Target_businessreduction\"]\n",
    "importances_threshold = {\"Target_contentloss_euro\": 0.000, \"Target_businessreduction\": 0.000 }\n",
    "\n",
    "# fi_plots =  {\"Target_contentloss_euro\": None, \"Target_businessreduction\": None }\n",
    "\n",
    "plt.ioff()  # Prevent plt showing stuff\n",
    "\n",
    "\n",
    "for target in targets:\n",
    "\n",
    "\n",
    "    ## iterate over piplines. Each piplines contains precrosseing methods and several  classifier\n",
    "    pipelines = [\"pipe_us_xgb\", \"pipe_ximput_us_xgb\"]\n",
    "        \n",
    "\n",
    "    for pipe_name in pipelines:\n",
    "\n",
    "        print( f\"\\nApply XGBoost on {target}, with pipeline {pipe_name}:\")\n",
    "\n",
    "        ## load sinlge pipeline\n",
    "        pipe = joblib.load(f'./pipelines/{pipe_name}.pkl')\n",
    "        \n",
    "        \n",
    "        df_candidates_t = df_candidates\n",
    "\n",
    "        ## TEST run xgb with and without nan in X\n",
    "        ## clean df from remaining records containg nan\n",
    "        #df_candidates_t = df_candidates_t.dropna()\n",
    "        #df_candidates_t = df_candidates_t[df_candidates_t[target]!=0.0]\n",
    "\n",
    "        #print(\"Amount of missing target values should be zero: \", df_candidates_t[target].isna().sum())\n",
    "        print(\"Uses \", df_candidates_t.shape[0], \" records, from those have \", \n",
    "            { (df_candidates_t[target][df_candidates_t[target]==0.0]).count() }, f\" records zero {target.split('_')[1]}\")\n",
    "\n",
    "\n",
    "        ## drop samples where target is nan\n",
    "        print(f\"Dropping {df_candidates_t[f'{target}'].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "        df_candidates_t = df_candidates_t[ ~df_candidates_t[f\"{target}\"].isna()]\n",
    "\n",
    "            \n",
    "            \n",
    "        # Discretize target variable \n",
    "        print(\"Discretization of target:\")\n",
    "\n",
    "        target_c = target + \"_c\"\n",
    "\n",
    "        no_contentloss = df_candidates_t[df_candidates_t[target]==0.0]\n",
    "        no_contentloss[target_c] = 0 #\"no loss\"\n",
    "        contentloss = df_candidates_t.drop(no_contentloss.index,  axis=0)  # reduce df to not yet discretized records\n",
    "    \n",
    "        ## exclude also very high values for content loss\n",
    "        if target == \"Target_contentloss_euro\":\n",
    "            #extreme_high_contenloss = df_candidates_t[df_candidates_t[target] >= extreme_high_lower_bin_boundary]\n",
    "            #extreme_high_contenloss[target_c] = 4 # \"very high\"\n",
    "            #contentloss = contentloss.drop(extreme_high_contenloss.index,  axis=0) #  reduce df to not yet discretized records\n",
    "            \n",
    "            ## equal frequency binning for major group of content losses\n",
    "            contentloss = fs.equal_freq_binning(\n",
    "                df=contentloss, variable_name=target, \n",
    "                cuts=4, group_labels=[1, 2, 3, 4 ], #[\"low\", \"medium\", \"high\"], \n",
    "                drop_old_variable=False)\n",
    "            df_candidates_t = pd.concat([no_contentloss, contentloss], axis=0).reset_index(drop=True)\n",
    "    #        df_candidates_t = pd.concat([no_contentloss, contentloss, extreme_high_contenloss], axis=0).reset_index(drop=True)\n",
    "\n",
    "        # make for 4 eqal bins for business reduction, due that no extreme high values exists\n",
    "        if target == \"Target_businessreduction\":\n",
    "            contentloss = fs.equal_freq_binning(\n",
    "                df=contentloss, variable_name=target, \n",
    "                cuts=4, group_labels=[1, 2, 3, 4 ],\n",
    "                drop_old_variable=False)\n",
    "            df_candidates_t = pd.concat([no_contentloss, contentloss], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        df_candidates_t.insert(1, target_c, df_candidates_t.pop(target_c))\n",
    "        df_candidates_t = df_candidates_t.sort_values(target)  # sort values to assigne later \n",
    "                                    #print(\"check if correct discretized:\", df_candidates_t[[target_c, target]].tail(5))  # visual check if correct discretized\n",
    "        print(\"check if correct discretized:\", df_candidates_t[target_c].value_counts())  # visual check if correct discretized\n",
    "\n",
    "        ## save distribution of discretized variable for visual check to disk\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1)  # create figure & 1 axis\n",
    "        df_candidates_t[target_c].hist( bins=len( df_candidates_t[target_c].unique())*2-1, grid=False ) # get space between single bars\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'../../../figures/histo_{target_c}_{pipe_name}_dscrt.png')\n",
    "\n",
    "\n",
    "\n",
    "        if pipe_name == \"pipe_ximput_us_xgb\":\n",
    "            ##impute nans in X\n",
    "            for c in df_candidates_t.drop(targets, axis=1): \n",
    "                df_candidates_t[f\"{c}\"].fillna(value=np.nanmedian(df_candidates_t[f\"{c}\"]), inplace=True)\n",
    " \n",
    "\n",
    "        # split into predictors and target variable\n",
    "        X_unscaled = df_candidates_t.drop([target_c] + targets, axis=1)  # remove targets from X\n",
    "        y = df_candidates_t[target_c]\n",
    "        \n",
    "        ## test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_unscaled, y, test_size=0.10, \n",
    "            random_state=seed, shuffle=True\n",
    "        )\n",
    "        eval_set = [(X_test, y_test)]\n",
    "\n",
    "        ## normalize data \n",
    "        X_train, X_test = fs.normalize_X(X_train, X_test)\n",
    "        \n",
    "\n",
    "\n",
    "        ## Pre-check class distribution with user-defined balancing stategy\n",
    "        print(\"Balancing will result in following class frequencies\")\n",
    "        print(Counter(y_train))\n",
    "        print(ratio_multiplier(y_train, zero_loss_ratio))\n",
    "\n",
    "        ## Hyperparmaters and CV\n",
    "\n",
    "        ## TODO adapt repeats + find better method maybe RepeatedStratifiedKFold\n",
    "        #cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed) \n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)#, random_state=seed)        #  StratifiedKFold = fold contains same percantega of class as in orignal training set, addresees imbalancing\n",
    "        model_cv = RandomizedSearchCV(\n",
    "            estimator=pipe, \n",
    "            param_distributions=param_grid, \n",
    "            cv=cv, \n",
    "            #scoring=\"f1_micro\",   #TODO test also e.g \"f1\" or recall, \"neg_mean_absolute_error\",\n",
    "            refit=True,   ## Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "                            ## If refit=False, clf.fit() will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after fit().\n",
    "                            ## ! When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit()\n",
    "            #random_state=seed,\n",
    "        )\n",
    "        ## Fit model \n",
    "        model_cv.fit(X_train, y_train)\n",
    " \n",
    "\n",
    "\n",
    "        print(f\"Best hyperparams: {model_cv.best_params_}\")\n",
    "        # print(\"Train R^2 Score : %.1f\" %model_cv.best_estimator_.score(X_train, y_train))\n",
    "        #print(\"MAE of best model: %.1f\" %model_cv.best_score_,\" on iteration \", model_cv.best_estimator_.best_iteration)  \n",
    "\n",
    "        # fit model again with best hyperparams\n",
    "        print(\"Create new XGBoost model based on best hyperparameters\")\n",
    "        model = XGBClassifier(\n",
    "            n_estimators = model_cv.best_params_['xgbclassifier__n_estimators'], \n",
    "            max_depth = model_cv.best_params_['xgbclassifier__max_depth'],\n",
    "            colsample_bynode = model_cv.best_params_['xgbclassifier__colsample_bynode'],\n",
    "            colsample_bytree = model_cv.best_params_['xgbclassifier__colsample_bytree'],\n",
    "            learning_rate = model_cv.best_params_['xgbclassifier__learning_rate'],\n",
    "            gamma = model_cv.best_params_['xgbclassifier__gamma'],\n",
    "            reg_alpha = model_cv.best_params_['xgbclassifier__reg_alpha'],  # Lasso Regularization term on weights \n",
    "            reg_lambda = model_cv.best_params_['xgbclassifier__reg_lambda'],\n",
    "            min_child_weight = model_cv.best_params_['xgbclassifier__min_child_weight'],\n",
    "            #random_state=seed,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        ## store best trained model for evaluation\n",
    "        filename = f'./models_trained/xgboost_{target_c}_{pipe_name}_dscrt.sav'\n",
    "        #pickle.dump(model_cv.best_estimator_, open(filename, 'wb'))\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "        ## Evaluate model\n",
    "        # print(f\"Training set score (R^2): {round(model.score(X_train, y_train), 2)}\")  # how well did the model on the training set\n",
    "        # print(f\"Test set score (R^2): {model_cv.score(X_test, y_test)}\")   # .. compared to the unseen test set for overfitting - acutal not needed\n",
    "        # r2 = variance explained by model / total variance --> higher r2= better fitted model\n",
    "\n",
    "        ## get signifcant features based on absolute coeff values\n",
    "        print(\"\\nSelect features based on permutation feature importance\")\n",
    "\n",
    "        # ## select significant features byPermuation feature importance\n",
    "        importances = e.permutation_feature_importance(model, X_test, y_test, repeats=5, seed=seed)\n",
    "\n",
    "        df_importance = pd.DataFrame(\n",
    "            {\"importances\" : importances[0]},\n",
    "            index=X_train.columns.to_list(),\n",
    "            ) \n",
    "        df_importance = df_importance.sort_values(\"importances\", ascending=False)  # get most important features to the top\n",
    "        print(\"Most important features:\", df_importance.iloc[:5].index.to_list())\n",
    "        df_importance = df_importance.loc[df_importance.importances >= fi_threshold, : ]\n",
    "        #df_importance.head(5)\n",
    "        # ## write selected predictors and response to disk\n",
    "        fs.save_selected_features(\n",
    "            X_train, \n",
    "            pd.DataFrame(y_train, columns=[target_c]), \n",
    "            df_importance.T.columns, \n",
    "            filename=f\"../../../input_survey_data/fs_xgboost_{target_c.split('_')[1]}_{pipe_name}_dscrt.xlsx\"\n",
    "        )\n",
    "\n",
    "        ## evaluate\n",
    "        ## print evaluation report + check for overfitting \n",
    "        print(\"\\nTraining set\")\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        #y_pred_train = model_cv.best_estimator_.predict(X_train)\n",
    "        print(f\"Training set score (F1): {round(f1_score(y_train, y_pred_train, average='micro'), 2)}\") \n",
    "        # average=\"micro\"\n",
    "        #e.evaluation_report(y_train, y_pred_train)\n",
    "\n",
    "        print(\"\\nTesting set\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        #e.evaluation_report(y_test, y_pred)\n",
    "        print(f\"Test set score (F1): {round(f1_score(y_test, y_pred, average='micro'), 2)}\") \n",
    "\n",
    "        ## confusion matrix\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,8))  # create figure & 1 axis\n",
    "        sns.heatmap(confusion_matrix(y_test, y_pred), ax=ax, annot=True)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'../../../figures/confusionmatrix_{target_c}_discretized.png')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig(f\"./models_trained/FI_{target}.png\", bbox_inches='tight')\n",
    "#sns_plot.figure.savefig(\"output.png\")\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload models\n",
    "\n",
    "target = targets[0] + \"_c\"\n",
    "print(target)\n",
    "\n",
    "model_eval = pickle.load(open(f\"./models_trained/xgboost_{target}.sav\", 'rb'))\n",
    "#model_eval.get_params()\n",
    "#dir(model_eval)#.feature_importances_[model_eval.feature_importances_>0.015].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "Have the same feature importance method across all applied ML models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Permuation feature importance\n",
    "# result = e.permutation_feature_importance(model_eval, X_test, y_test, repeats=5, seed=seed)\n",
    "\n",
    "# df_importance = pd.DataFrame({\n",
    "#     \"name\" : X_train.columns.to_list(),\n",
    "#     \"importances\" : result[0],\n",
    "#      }) \n",
    "# df_importance = df_importance.sort_values(\"importances\", ascending=False)  # get most important features to the top\n",
    "# df_importance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "\n",
    "# f.plot_feature_importance(df_importance.importances, n=10, figure_size=(20, 15), target=target)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "# drop features which dont reduce the loss\n",
    "df_importance = df_importance.loc[df_importance.importances >= 0.001, : ] \n",
    "#sorted_idx = model_eval.feature_importances_.argsort()\n",
    "#plt.barh(df_importance.name[-5:], df_importance.importances[-5:])\n",
    "plt.bar(df_importance.name, df_importance.importances)\n",
    "#plt.bar(X_train.columns[sorted_idx[:15]], model_eval.feature_importances_[sorted_idx[:15]])\n",
    "plt.xticks(\n",
    "   # ticks = range(len(X_train.columns[sorted_idx[:15]])),\n",
    "   # labels =X_train.columns[sorted_idx[:15],],\n",
    "    rotation = 90\n",
    "    )\n",
    "plt.title(f\"Feature Importances for {target}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical clustering on Spearman rank correlation\n",
    "\n",
    "Select only feautres with low collienarity to solve disadvantage of perumation feature importance.\n",
    "Randomizing one feature would lead to only small importance score - the model performance wouldnt be move influenced - due that the information is included in other correlated features. Removing one feature keeps the similar inforamtion in the other feautres unchanged and the model learns from the correlated feature. Therefore apply hierachical clustering to select less correlated features\n",
    "\n",
    "See also:\n",
    "- Brill 2020 (dissertation)\n",
    "- https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html # code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.cluster.hierarchy as shc\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.title(\"Customers Dendrogram\")\n",
    "\n",
    "# # Selecting Annual Income and Spending Scores by index\n",
    "# selected_data = X_train.dropna()\n",
    "# selected_data = selected_data.T # only possible with out nan\n",
    "# clusters = shc.linkage(selected_data, \n",
    "#             method='ward', optimal_ordering=False,\n",
    "#             metric=\"euclidean\")\n",
    "# shc.dendrogram(Z=clusters, \n",
    "#                #p=20, # p -> value for truncation mode\n",
    "#                orientation=\"right\",\n",
    "#                labels=X_train.columns\n",
    "#                ) \n",
    "# plt.show()\n",
    "\n",
    "# ## TODO adapt with spearman rank order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.stats import spearmanr\n",
    "# from scipy.spatial.distance import squareform\n",
    "# from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# corr = spearmanr(X_unscaled_no_nan).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = ward(distance_matrix, checks=False )\n",
    "# dendro = dendrogram(\n",
    "#     dist_linkage, labels=X_unscaled_no_nan.columns.tolist(), ax=ax1, leaf_rotation=90\n",
    "# )\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = shc.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "X_train_sel = X_train[:, selected_features]\n",
    "X_test_sel = X_test[:, selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## permutation based FI (build in func from skilearn)\n",
    "\n",
    "# perm_importance = permutation_importance(xgb, X_test, y_test)\n",
    "# The visualization of the importance:\n",
    "\n",
    "# sorted_idx = perm_importance.importances_mean.argsort()\n",
    "# plt.barh(boston.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
