{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data preprocessing for HCMC survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"a.buch@stud.uni-heidelberg.de\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection for content losses done by Logistic Regression\n",
    "\n",
    "Due to many zero losses especially in content losses, a binary regression was tested to distinguish between occured losses and no losses. The before applied elastic net result showed that the elastic net algorithm might be a bit too complex for the moderate size of training set and the imbalnced distribution with in the response (many zero losses compared to only a very a left skewed distribution of occured content losses)  \n",
    "\n",
    "*Sources*\n",
    "Geron 2019: https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#idm45022190228392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import f1_score, confusion_matrix, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline  # make sure not to mix it with sklearn Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, \"../../../\")\n",
    "import utils.utils_feature_selection as fs\n",
    "import utils.utils_evaluation as e\n",
    "import utils.utils_figures as f\n",
    "import utils.settings as s\n",
    "import utils.pipelines_discretize as p\n",
    "\n",
    "\n",
    "s.init()\n",
    "seed = s.seed\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_contentloss_euro</th>\n",
       "      <th>Target_businessreduction</th>\n",
       "      <th>inundation_duration_h</th>\n",
       "      <th>water_depth_cm</th>\n",
       "      <th>contaminations.0</th>\n",
       "      <th>flowvelocity</th>\n",
       "      <th>warning_time_h</th>\n",
       "      <th>emergency_measures.1</th>\n",
       "      <th>emergency_measures.2</th>\n",
       "      <th>emergency_measures.3</th>\n",
       "      <th>emergency_measures.4</th>\n",
       "      <th>emergency_measures.6</th>\n",
       "      <th>emergency_measures.7</th>\n",
       "      <th>emergency_measures.8</th>\n",
       "      <th>emergency_measures.9</th>\n",
       "      <th>overall_problem_house</th>\n",
       "      <th>protect_valuables_impl</th>\n",
       "      <th>water_barriers_impl</th>\n",
       "      <th>pumping_equipment_impl</th>\n",
       "      <th>elevation_building_impl</th>\n",
       "      <th>resistant_material_building_impl</th>\n",
       "      <th>electricity_higher_impl</th>\n",
       "      <th>flood_protections_impl</th>\n",
       "      <th>flood_experience</th>\n",
       "      <th>elevation_building_height_cm</th>\n",
       "      <th>elevation_rel2surrounding_cat</th>\n",
       "      <th>bage</th>\n",
       "      <th>b_area</th>\n",
       "      <th>hh_monthly_income_cat</th>\n",
       "      <th>shp_owner</th>\n",
       "      <th>shp_sector</th>\n",
       "      <th>shp_employees</th>\n",
       "      <th>shp_avgmonthly_sale_cat</th>\n",
       "      <th>shp_finance_investments</th>\n",
       "      <th>shp_profits_last5years</th>\n",
       "      <th>shp_risk_tolerance</th>\n",
       "      <th>shp_monetary_resources4prevention</th>\n",
       "      <th>resilience_city_protection</th>\n",
       "      <th>resilience_more_future_affected</th>\n",
       "      <th>resilience_govern_warnings_helpful</th>\n",
       "      <th>resilience_govern_careing</th>\n",
       "      <th>resilience_left_alone</th>\n",
       "      <th>resilience_neighbor_management</th>\n",
       "      <th>perception_who_responsible4protection.Rank1</th>\n",
       "      <th>perception_private_economy_future</th>\n",
       "      <th>contaminations_light</th>\n",
       "      <th>contaminations_heavy</th>\n",
       "      <th>shp_suppliers_HCMC</th>\n",
       "      <th>shp_content_value_euro</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>shp_registered_capital_euro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83886</td>\n",
       "      <td>11047.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87277</td>\n",
       "      <td>736.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target_contentloss_euro  Target_businessreduction  inundation_duration_h   \n",
       "395                      0.0                       NaN                    4.0  \\\n",
       "396                      0.0                       0.0                    3.0   \n",
       "\n",
       "     water_depth_cm  contaminations.0  flowvelocity  warning_time_h   \n",
       "395            70.0                 0             1             NaN  \\\n",
       "396           100.0                 0             1             NaN   \n",
       "\n",
       "     emergency_measures.1  emergency_measures.2  emergency_measures.3   \n",
       "395                     1                     0                     1  \\\n",
       "396                     1                     0                     1   \n",
       "\n",
       "     emergency_measures.4  emergency_measures.6  emergency_measures.7   \n",
       "395                     0                     1                     0  \\\n",
       "396                     0                     0                     0   \n",
       "\n",
       "     emergency_measures.8  emergency_measures.9  overall_problem_house   \n",
       "395                     0                     0                      1  \\\n",
       "396                     0                     0                      0   \n",
       "\n",
       "     protect_valuables_impl  water_barriers_impl  pumping_equipment_impl   \n",
       "395                       1                    5                       1  \\\n",
       "396                       1                    5                       5   \n",
       "\n",
       "     elevation_building_impl  resistant_material_building_impl   \n",
       "395                        1                                 5  \\\n",
       "396                        5                                 5   \n",
       "\n",
       "     electricity_higher_impl  flood_protections_impl  flood_experience   \n",
       "395                        5                       5                 5  \\\n",
       "396                        5                       5                 4   \n",
       "\n",
       "     elevation_building_height_cm  elevation_rel2surrounding_cat  bage   \n",
       "395                          70.0                              1   NaN  \\\n",
       "396                           NaN                              0   5.0   \n",
       "\n",
       "     b_area  hh_monthly_income_cat  shp_owner  shp_sector  shp_employees   \n",
       "395   130.0                    NaN          1          17              2  \\\n",
       "396    33.0                    1.0          1          11              2   \n",
       "\n",
       "     shp_avgmonthly_sale_cat  shp_finance_investments  shp_profits_last5years   \n",
       "395                        3                        1                     4.0  \\\n",
       "396                        3                        1                     4.0   \n",
       "\n",
       "     shp_risk_tolerance  shp_monetary_resources4prevention   \n",
       "395                 3.0                                3.0  \\\n",
       "396                 3.0                                4.0   \n",
       "\n",
       "     resilience_city_protection  resilience_more_future_affected   \n",
       "395                         1.0                              5.0  \\\n",
       "396                         NaN                              NaN   \n",
       "\n",
       "     resilience_govern_warnings_helpful  resilience_govern_careing   \n",
       "395                                 1.0                        1.0  \\\n",
       "396                                 NaN                        NaN   \n",
       "\n",
       "     resilience_left_alone  resilience_neighbor_management   \n",
       "395                      5                             1.0  \\\n",
       "396                      5                             NaN   \n",
       "\n",
       "     perception_who_responsible4protection.Rank1   \n",
       "395                                          2.0  \\\n",
       "396                                          3.0   \n",
       "\n",
       "     perception_private_economy_future  contaminations_light   \n",
       "395                                3.0                     1  \\\n",
       "396                                3.0                     1   \n",
       "\n",
       "     contaminations_heavy  shp_suppliers_HCMC  shp_content_value_euro   \n",
       "395                     0                   1                     NaN  \\\n",
       "396                     0                   1                     NaN   \n",
       "\n",
       "     elevation_m  shp_registered_capital_euro  \n",
       "395      1.83886                      11047.7  \n",
       "396      1.87277                        736.5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates = pd.read_excel(\"../../../input_survey_data/input_data_business.xlsx\")\n",
    "print(df_candidates.shape)\n",
    "df_candidates.tail(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing valeus per feature\n",
      " warning_time_h                                 0.775819\n",
      "elevation_building_height_cm                   0.158690\n",
      "shp_content_value_euro                         0.158690\n",
      "shp_registered_capital_euro                    0.118388\n",
      "Target_businessreduction                       0.090680\n",
      "perception_who_responsible4protection.Rank1    0.070529\n",
      "shp_risk_tolerance                             0.070529\n",
      "bage                                           0.068010\n",
      "perception_private_economy_future              0.065491\n",
      "hh_monthly_income_cat                          0.060453\n",
      "resilience_govern_careing                      0.057935\n",
      "resilience_govern_warnings_helpful             0.045340\n",
      "shp_monetary_resources4prevention              0.045340\n",
      "resilience_more_future_affected                0.037783\n",
      "resilience_city_protection                     0.037783\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## delete features with more than 10% missing values\n",
    "\n",
    "#df_candidates = df_candidates[df_candidates.columns[df_candidates.isna().mean() < 0.15]]  # drop feautres with more than 10% missing values\n",
    "#print(df_candidates.isna().sum(axis=0).sort_values(ascending=False))\n",
    "print(\"Percentage of missing valeus per feature\\n\", df_candidates.isna().mean().sort_values(ascending=False)[:15] ) \n",
    "## --> ekpp threshold by 15% less would delete important features e.g. content values, registerd capitaletc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning_time_h                                 308\n",
      "elevation_building_height_cm                    63\n",
      "shp_content_value_euro                          63\n",
      "shp_registered_capital_euro                     47\n",
      "Target_businessreduction                        36\n",
      "perception_who_responsible4protection.Rank1     28\n",
      "shp_risk_tolerance                              28\n",
      "bage                                            27\n",
      "perception_private_economy_future               26\n",
      "hh_monthly_income_cat                           24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_candidates.isna().sum().sort_values(ascending=False)[:10])# remaining abs number of nan per feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter ranges to test\n",
    "weights = np.linspace(0.0, 0.99, 200) # class weight range\n",
    "param_grid = {\n",
    "    \"model__penalty\": [\"none\", \"l2\"],      # alpha: constant mulitplies penality term, alpha = 0 is equivalent to an OLS solved by the LinearRegression\n",
    "    \"model__tol\": [0.0001, 0.001, 0.01, 0.1, 0.5,],\n",
    "    \"model__C\": [1,2,3,4],  # inverse reularization strength\n",
    "    \"model__max_iter\": [1,2,3,4],\n",
    "    \"model__l1_ratio\": np.arange(0.0, 1.01, 0.25),     # r = 0, equivalent to Ridge Regression (=L2),  r = 1 equivalent to Lasso Regression (=L1) \n",
    "    \"model__solver\": [\"liblinear\", \"newton-cg\"],\n",
    "    \"model__class_weight\": [{0:x, 1:1.0-x} for x in weights],\n",
    "    \"model__random_state\": [seed],\n",
    "}\n",
    "## TODO fix  this current workaround with beginning of names for pipes with BaggingRegressor\n",
    "bagging_names = ['bagging__estimator__penalty', 'bagging__estimator__tol', \n",
    "                 'bagging__estimator__C', 'bagging__estimator__max_iter',\n",
    "                 'bagging__estimator__l1_ratio', 'bagging__estimator__solver', \n",
    "                 'bagging__estimator__class_weight','bagging__estimator__random_state']\n",
    "param_bag_grid = dict(zip(bagging_names, list(param_grid.values())))\n",
    "\n",
    "\n",
    "param_bagging = {\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [seed]\n",
    "    # 'bootstrap_features': [True, False],\n",
    "    # 'n_estimators': [20,50,100],\n",
    "    # 'max_samples': [0.5,1.0, X_train.shape[0]//2,],\n",
    "    # 'max_features': [0.5,1.0, X_train.shape[1]//2,],\n",
    "    # oob_score\n",
    "}\n",
    "## TODO add hyperparams for Bagging: \n",
    "## current defaults: n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model and select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_us_bag_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.01, 'bagging__estimator__solver': 'newton-cg', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'none', 'bagging__estimator__max_iter': 2, 'bagging__estimator__l1_ratio': 0.25, 'bagging__estimator__class_weight': {0: 0.009949748743718593, 1: 0.9900502512562814}, 'bagging__estimator__C': 3}\n",
      "Train R^2 Score : -1.450\n",
      "CV score:  0.5797619047619047 -1.4499999999999997 -1.4374999999999996\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.01, 'bagging__estimator__solver': 'newton-cg', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'none', 'bagging__estimator__max_iter': 2, 'bagging__estimator__l1_ratio': 0.25, 'bagging__estimator__class_weight': {0: 0.009949748743718593, 1: 0.9900502512562814}, 'bagging__estimator__C': 3}\n",
      "Train R^2 Score : 0.408\n",
      "Test R^2 Score : 0.410\n",
      "RMSE:  0.59 euros or in %\n",
      "Most important features: ['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees']\n",
      "total features: 49\n",
      "selected features: 49\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_more_future_affected', 'resilience_govern_warnings_helpful', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_rel2surrounding_cat', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'elevation_building_height_cm', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_us_bag_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.41\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.41\n",
      "Mean MAE: 0.088 (0.059)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_bag_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.5, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 2, 'bagging__estimator__l1_ratio': 0.0, 'bagging__estimator__class_weight': {0: 0.22386934673366835, 1: 0.7761306532663317}, 'bagging__estimator__C': 2}\n",
      "Train R^2 Score : -1.450\n",
      "CV score:  0.5797619047619047 -1.4499999999999997 -1.4374999999999996\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.5, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 2, 'bagging__estimator__l1_ratio': 0.0, 'bagging__estimator__class_weight': {0: 0.22386934673366835, 1: 0.7761306532663317}, 'bagging__estimator__C': 2}\n",
      "Train R^2 Score : 0.408\n",
      "Test R^2 Score : 0.410\n",
      "RMSE:  0.59 euros or in %\n",
      "Most important features: ['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees']\n",
      "total features: 49\n",
      "selected features: 49\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_more_future_affected', 'resilience_govern_warnings_helpful', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_rel2surrounding_cat', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'elevation_building_height_cm', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_bag_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.41\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.41\n",
      "Mean MAE: 0.026 (0.017)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_xdrop_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  27\n",
      "Before balancing Counter({1: 16, 0: 11})\n",
      "After balancing Counter({1: 16, 0: 8})\n",
      "Best hyperparams: {'model__tol': 0.1, 'model__solver': 'newton-cg', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 4, 'model__l1_ratio': 0.75, 'model__class_weight': {0: 0.2785929648241206, 1: 0.7214070351758795}, 'model__C': 4}\n",
      "Train R^2 Score : 0.815\n",
      "CV score:  0.7444444444444444 0.8148148148148148 0.75\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'model__tol': 0.1, 'model__solver': 'newton-cg', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 4, 'model__l1_ratio': 0.75, 'model__class_weight': {0: 0.2785929648241206, 1: 0.7214070351758795}, 'model__C': 4}\n",
      "Train R^2 Score : 0.815\n",
      "Test R^2 Score : 0.750\n",
      "RMSE:  0.25 euros or in %\n",
      "Most important features: ['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees']\n",
      "total features: 49\n",
      "selected features: 49\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_more_future_affected', 'resilience_govern_warnings_helpful', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_rel2surrounding_cat', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'elevation_building_height_cm', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_xdrop_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.81\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.75\n",
      "Mean MAE: 0.244 (0.229)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'model__tol': 0.5, 'model__solver': 'liblinear', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 2, 'model__l1_ratio': 0.5, 'model__class_weight': {0: 0.4626633165829146, 1: 0.5373366834170854}, 'model__C': 2}\n",
      "Train R^2 Score : 0.714\n",
      "CV score:  0.6384033613445379 0.7142857142857143 0.6153846153846154\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'model__tol': 0.5, 'model__solver': 'liblinear', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 2, 'model__l1_ratio': 0.5, 'model__class_weight': {0: 0.4626633165829146, 1: 0.5373366834170854}, 'model__C': 2}\n",
      "Train R^2 Score : 0.714\n",
      "Test R^2 Score : 0.615\n",
      "RMSE:  0.38 euros or in %\n",
      "Most important features: ['hh_monthly_income_cat', 'elevation_building_height_cm', 'elevation_m', 'emergency_measures.1', 'resistant_material_building_impl']\n",
      "total features: 49\n",
      "selected features: 45\n",
      "dropped features: 4\n",
      "selected features: \n",
      "['hh_monthly_income_cat', 'elevation_building_height_cm', 'elevation_m', 'emergency_measures.1', 'resistant_material_building_impl', 'shp_finance_investments', 'water_barriers_impl', 'resilience_left_alone', 'flood_experience', 'emergency_measures.3', 'water_depth_cm', 'shp_profits_last5years', 'pumping_equipment_impl', 'emergency_measures.4', 'shp_monetary_resources4prevention', 'resilience_more_future_affected', 'shp_risk_tolerance', 'elevation_building_impl', 'electricity_higher_impl', 'perception_who_responsible4protection.Rank1', 'shp_avgmonthly_sale_cat', 'emergency_measures.9', 'resilience_govern_careing', 'elevation_rel2surrounding_cat', 'emergency_measures.6', 'bage', 'protect_valuables_impl', 'b_area', 'resilience_govern_warnings_helpful', 'emergency_measures.2', 'resilience_city_protection', 'flood_protections_impl', 'contaminations_light', 'shp_suppliers_HCMC', 'emergency_measures.8', 'emergency_measures.7', 'warning_time_h', 'resilience_neighbor_management', 'perception_private_economy_future', 'contaminations_heavy', 'shp_content_value_euro', 'inundation_duration_h', 'shp_employees', 'shp_owner', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.71\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.62\n",
      "Mean MAE: 0.323 (0.222)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_us_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'model__tol': 0.001, 'model__solver': 'liblinear', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 2, 'model__l1_ratio': 0.5, 'model__class_weight': {0: 0.5820603015075377, 1: 0.41793969849246226}, 'model__C': 2}\n",
      "Train R^2 Score : 0.685\n",
      "CV score:  0.6023529411764705 0.685131195335277 0.6410256410256411\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'model__tol': 0.001, 'model__solver': 'liblinear', 'model__random_state': 42, 'model__penalty': 'l2', 'model__max_iter': 2, 'model__l1_ratio': 0.5, 'model__class_weight': {0: 0.5820603015075377, 1: 0.41793969849246226}, 'model__C': 2}\n",
      "Train R^2 Score : 0.711\n",
      "Test R^2 Score : 0.615\n",
      "RMSE:  0.38 euros or in %\n",
      "Most important features: ['resilience_more_future_affected', 'elevation_m', 'elevation_rel2surrounding_cat', 'resistant_material_building_impl', 'emergency_measures.4']\n",
      "total features: 49\n",
      "selected features: 36\n",
      "dropped features: 13\n",
      "selected features: \n",
      "['resilience_more_future_affected', 'elevation_m', 'elevation_rel2surrounding_cat', 'resistant_material_building_impl', 'emergency_measures.4', 'perception_private_economy_future', 'perception_who_responsible4protection.Rank1', 'shp_finance_investments', 'contaminations_heavy', 'overall_problem_house', 'inundation_duration_h', 'shp_monetary_resources4prevention', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_profits_last5years', 'resilience_govern_warnings_helpful', 'resilience_city_protection', 'b_area', 'resilience_govern_careing', 'resilience_neighbor_management', 'contaminations_light', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_building_height_cm', 'contaminations.0', 'warning_time_h', 'emergency_measures.2', 'emergency_measures.8', 'shp_registered_capital_euro', 'elevation_building_impl', 'electricity_higher_impl', 'flood_protections_impl']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_us_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.71\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.62\n",
      "Mean MAE: 0.302 (0.204)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_bag_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.001, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 3, 'bagging__estimator__l1_ratio': 0.25, 'bagging__estimator__class_weight': {0: 0.9104020100502512, 1: 0.08959798994974877}, 'bagging__estimator__C': 4}\n",
      "Train R^2 Score : -0.690\n",
      "CV score:  0.0 -0.6896551724137929 -0.695652173913043\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.001, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 3, 'bagging__estimator__l1_ratio': 0.25, 'bagging__estimator__class_weight': {0: 0.9104020100502512, 1: 0.08959798994974877}, 'bagging__estimator__C': 4}\n",
      "Train R^2 Score : 0.592\n",
      "Test R^2 Score : 0.590\n",
      "RMSE:  0.41 euros or in %\n",
      "Most important features: ['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees']\n",
      "total features: 49\n",
      "selected features: 49\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_more_future_affected', 'resilience_govern_warnings_helpful', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_rel2surrounding_cat', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'elevation_building_height_cm', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_bag_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.59\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.59\n",
      "Mean MAE: 0.089 (0.081)\n",
      "\n",
      "Apply Logistic Regression on Target_contentloss_euro, with pipeline pipe_ximput_us_bag_logr:\n",
      "Uses  397  records, from those have  {226}  records zero contentloss\n",
      "Dropping 15 records from entire dataset due that these values are nan in target variable\n",
      "Balancing will result in following class frequencies\n",
      "Have training size of  343\n",
      "Before balancing Counter({0: 203, 1: 140})\n",
      "After balancing Counter({0: 152, 1: 140})\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.01, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 4, 'bagging__estimator__l1_ratio': 1.0, 'bagging__estimator__class_weight': {0: 0.029849246231155778, 1: 0.9701507537688442}, 'bagging__estimator__C': 2}\n",
      "Train R^2 Score : -1.450\n",
      "CV score:  0.5797619047619047 -1.4499999999999997 -1.4374999999999996\n",
      "Create new LogisticRegression model based on best hyperparameters\n",
      "Best hyperparams: {'bagging__estimator__tol': 0.01, 'bagging__estimator__solver': 'liblinear', 'bagging__estimator__random_state': 42, 'bagging__estimator__penalty': 'l2', 'bagging__estimator__max_iter': 4, 'bagging__estimator__l1_ratio': 1.0, 'bagging__estimator__class_weight': {0: 0.029849246231155778, 1: 0.9701507537688442}, 'bagging__estimator__C': 2}\n",
      "Train R^2 Score : 0.408\n",
      "Test R^2 Score : 0.410\n",
      "RMSE:  0.59 euros or in %\n",
      "Most important features: ['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees']\n",
      "total features: 49\n",
      "selected features: 49\n",
      "dropped features: 0\n",
      "selected features: \n",
      "['inundation_duration_h', 'b_area', 'shp_owner', 'shp_sector', 'shp_employees', 'shp_avgmonthly_sale_cat', 'shp_finance_investments', 'shp_profits_last5years', 'shp_risk_tolerance', 'shp_monetary_resources4prevention', 'resilience_city_protection', 'resilience_more_future_affected', 'resilience_govern_warnings_helpful', 'resilience_govern_careing', 'resilience_left_alone', 'resilience_neighbor_management', 'perception_who_responsible4protection.Rank1', 'perception_private_economy_future', 'contaminations_light', 'contaminations_heavy', 'shp_suppliers_HCMC', 'shp_content_value_euro', 'elevation_m', 'hh_monthly_income_cat', 'bage', 'water_depth_cm', 'elevation_rel2surrounding_cat', 'contaminations.0', 'flowvelocity', 'warning_time_h', 'emergency_measures.1', 'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4', 'emergency_measures.6', 'emergency_measures.7', 'emergency_measures.8', 'emergency_measures.9', 'overall_problem_house', 'protect_valuables_impl', 'water_barriers_impl', 'pumping_equipment_impl', 'elevation_building_impl', 'resistant_material_building_impl', 'electricity_higher_impl', 'flood_protections_impl', 'flood_experience', 'elevation_building_height_cm', 'shp_registered_capital_euro']\n",
      "\n",
      "Saving model to disk: ../../../input_survey_data/fs_logr_contentloss_pipe_ximput_us_bag_logr_dscrt.xlsx\n",
      "\n",
      "Training set\n",
      "Training set score (F1): 0.41\n",
      "\n",
      "Testing set\n",
      "Test set score (F1): 0.41\n",
      "Mean MAE: 0.166 (0.136)\n"
     ]
    }
   ],
   "source": [
    "## iterate over both targets and store results \n",
    "\n",
    "zero_loss_ratio = 0.75  # 0.75\n",
    "fi_threshold = 0.000\n",
    "\n",
    "def ratio_multiplier(y, sampling_stategy=zero_loss_ratio):\n",
    "    \"\"\"\"\n",
    "    https://imbalanced-learn.org/stable/auto_examples/api/plot_sampling_strategy_usage.html#sphx-glr-auto-examples-api-plot-sampling-strategy-usage-py\n",
    "    y : y train\n",
    "    sampling_stategy = str defineing imblearn stategy or float between 0-1.0 defining ratio to which y is reduced .e.g. (0.75 >- y will be 75% of its former size)\n",
    "    \"\"\"\n",
    "    # if sampling_stategy == \"majority\": \n",
    "    #     return sampling_stategy\n",
    "    # else:\n",
    "    multiplier = {0: zero_loss_ratio} # set only zero loss class to the half or 3/4 of its size\n",
    "    target_stats = Counter(y)\n",
    "    for key, value in target_stats.items():\n",
    "        if key in multiplier:\n",
    "            target_stats[key] = int(value * multiplier[key])\n",
    "    return target_stats\n",
    "\n",
    "targets = [\"Target_contentloss_euro\", \"Target_businessreduction\"]\n",
    "importances_threshold = {\"Target_contentloss_euro\": 0.000, \"Target_businessreduction\": 0.000 }\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "for target in targets:\n",
    "\n",
    "    ## iterate over piplines. Each piplines contains precrosseing methods and several  classifier\n",
    "    pipelines = [\"pipe_ximput_us_bag_logr\", \"pipe_ximput_bag_logr\", \"pipe_xdrop_logr\", \"pipe_ximput_logr\", \"pipe_ximput_us_logr\", \"pipe_ximput_bag_logr\",\"pipe_ximput_us_bag_logr\"]\n",
    "        \n",
    "    for pipe_name in pipelines:\n",
    "\n",
    "        print( f\"\\nApply Logistic Regression on {target}, with pipeline {pipe_name}:\")\n",
    "\n",
    "        ## load sinlge pipeline\n",
    "        pipe = joblib.load(f'./pipelines/{pipe_name}.pkl')\n",
    "        \n",
    "        \n",
    "        df_candidates_t = df_candidates\n",
    "\n",
    "        ## TEST run xgb with and without nan in X\n",
    "        ## clean df from remaining records containg nan\n",
    "        #df_candidates_t = df_candidates_t.dropna()\n",
    "        #df_candidates_t = df_candidates_t[df_candidates_t[target]!=0.0]\n",
    "\n",
    "        #print(\"Amount of missing target values should be zero: \", df_candidates_t[target].isna().sum())\n",
    "        print(\"Uses \", df_candidates_t.shape[0], \" records, from those have \", \n",
    "            { (df_candidates_t[target][df_candidates_t[target]==0.0]).count() }, f\" records zero {target.split('_')[1]}\")\n",
    "    \n",
    "    \n",
    "        ## drop samples where target is nan\n",
    "        print(f\"Dropping {df_candidates_t[f'{target}'].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "        df_candidates_t = df_candidates_t[ ~df_candidates_t[f\"{target}\"].isna()]\n",
    "\n",
    "\n",
    "        ## Discretize target into binary classes    \n",
    "        df_candidates_t[target][df_candidates_t[target] > 0] = 1\n",
    "        df_candidates_t[target] = df_candidates_t[target].astype(\"Int64\")\n",
    "\n",
    "        ## clean df from remaining records containg nan or impute them\n",
    "        if pipe_name == \"pipe_xdrop_logr\":\n",
    "            ## drop instances where target is nan\n",
    "            df_candidates_t = df_candidates_t.dropna()\n",
    "        else:\n",
    "            ##impute nans in X\n",
    "            for c in df_candidates_t.drop(targets, axis=1): \n",
    "                df_candidates_t[f\"{c}\"].fillna(value=np.nanmedian(df_candidates_t[f\"{c}\"]), inplace=True)\n",
    "\n",
    "    \n",
    "        ## save distribution of discretized variable for visual check to disk\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1)  # create figure & 1 axis\n",
    "        df_candidates_t[target].hist( bins=len( df_candidates_t[target].unique())*2-1, grid=False ) # get space between single bars\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'../../../figures/logr_histo_{target}_{pipe_name}_dscrt.png')\n",
    "\n",
    "        # split into predictors and target variable\n",
    "        X_unscaled = df_candidates_t.drop( targets, axis=1)  # remove targets from X\n",
    "        y = df_candidates_t[target]\n",
    "        \n",
    "        ## test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_unscaled, y, test_size=0.10, \n",
    "            random_state=seed, shuffle=True)\n",
    "        eval_set = [(X_test, y_test)]\n",
    "\n",
    "        ## normalize data \n",
    "        X_train, X_test = fs.normalize_X(X_train, X_test)\n",
    "        \n",
    "        ## Pre-check class distribution with user-defined balancing stategy\n",
    "        if zero_loss_ratio == 1.0:\n",
    "            print(\"Using imbalanced training set\")\n",
    "            print(\"Have training size of \", y_train.shape[0] )\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Balancing will result in following class frequencies\")\n",
    "            print(\"Have training size of \", y_train.shape[0] )\n",
    "            print(\"Before balancing\", Counter(y_train))\n",
    "            print(\"After balancing\", ratio_multiplier(y_train, zero_loss_ratio))\n",
    "\n",
    "\n",
    "        ## Hyperparmaters and CV\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)#, random_state=seed)        #  StratifiedKFold = fold contains same percantega of class as in orignal training set, addresees imbalancing\n",
    "\n",
    "        if  pipe_name !=\"pipe_ximput_bag_logr\" and pipe_name!=\"pipe_ximput_us_bag_logr\":\n",
    "            #[s for s in [\"pipe_ximput_bag_logr\",\"pipe_ximput_us_bag_logr\"] \n",
    "                #if not any(x in s for x in pipe_name)]\n",
    "            ## TODO adapt repeats + find better method maybe RepeatedStratifiedKFold\n",
    "            #cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed) \n",
    "            model_cv = RandomizedSearchCV(\n",
    "                estimator=pipe, \n",
    "                param_distributions=param_grid, \n",
    "                cv=cv, \n",
    "                #scoring=\"f1_micro\",   #TODO test also e.g \"f1\" or recall, \"neg_mean_absolute_error\",\n",
    "                refit=True,   ## Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "                                ## If refit=False, clf.fit() will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after fit().\n",
    "                                ## ! When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit()\n",
    "                #random_state=seed\n",
    "            )\n",
    "            ## Fit model\n",
    "            model_cv.fit(X_train, y_train)   \n",
    "        else:\n",
    "            model_cv = RandomizedSearchCV(\n",
    "                estimator=pipe, \n",
    "                param_distributions=param_bag_grid, \n",
    "                cv=cv, \n",
    "                scoring=\"f1\",#_micro\",   \"neg_mean_absolute_error  #TODO test also e.g \"f1\" or recall, \"neg_mean_absolute_error\",\n",
    "                #refit=True,   ## Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "                #random_state=seed\n",
    "            )            \n",
    "            ## Fit model \n",
    "            model_cv.fit(X_train, y_train)   \n",
    "\n",
    "        print(f\"Best hyperparams: {model_cv.best_params_}\")\n",
    "        print('Train R^2 Score : %.3f'%model_cv.best_estimator_.score(X_train, y_train))\n",
    "        print(\"CV score: \", model_cv.best_score_ ,  model_cv.best_estimator_.score(X_train, y_train),  model_cv.best_estimator_.score(X_test, y_test))\n",
    "        ## score = coefficient of determination R2 which is (1 - (u/v)); \n",
    "        ## u= is the residual sum of squares ((y_true - y_pred)** 2).sum() and \n",
    "        ## v= is the total sum of squares ((y_true - y_true.mean()) ** 2).sum()\n",
    "        ## --> r2: variance explained by model / total variance --> higher r2= better fitted model\n",
    "\n",
    "        # fit model again with best hyperparams\n",
    "        print(\"Create new LogisticRegression model based on best hyperparameters\")\n",
    "        if  pipe_name != \"pipe_ximput_bag_logr\" and pipe_name!=\"pipe_ximput_us_bag_logr\": \n",
    "            model = LogisticRegression(\n",
    "                    penalty = model_cv.best_params_['model__penalty'], \n",
    "                    tol = model_cv.best_params_['model__tol'],  \n",
    "                    C = model_cv.best_params_['model__C'],  \n",
    "                    max_iter = model_cv.best_params_['model__max_iter'],  \n",
    "                    l1_ratio = model_cv.best_params_['model__l1_ratio'],  \n",
    "                    solver = model_cv.best_params_['model__solver'],  \n",
    "                    class_weight = model_cv.best_params_['model__class_weight'],  \n",
    "                    #random_state=seed,\n",
    "            )\n",
    "        else:\n",
    "            model = LogisticRegression(\n",
    "                penalty = model_cv.best_params_['bagging__estimator__penalty'], \n",
    "                tol = model_cv.best_params_['bagging__estimator__tol'],  \n",
    "                C = model_cv.best_params_['bagging__estimator__C'],  \n",
    "                max_iter = model_cv.best_params_['bagging__estimator__max_iter'],  \n",
    "                l1_ratio = model_cv.best_params_['bagging__estimator__l1_ratio'],  \n",
    "                solver = model_cv.best_params_['bagging__estimator__solver'],  \n",
    "                class_weight = model_cv.best_params_['bagging__estimator__class_weight'],  \n",
    "                random_state=seed,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "            \n",
    "        ## store best trained model for evaluation\n",
    "        filename = f'./models_trained/logr_{target}_{pipe_name}_dscrt.sav'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    \n",
    "        ## predict unseen data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(f\"Best hyperparams: {model_cv.best_params_}\")\n",
    "        print('Train R^2 Score : %.3f'%model.score(X_train, y_train)) # how well did the model on the training set\n",
    "        print('Test R^2 Score : %.3f'%model.score(X_test, y_test)) # .. compared to the unseen test set for overfitting \n",
    "        #print(\"MAE of best model: %.3f\" % elastic_net_cv.best_score_)  # TODO check why MAE nan \n",
    "        rmse = np.square(np.subtract(y_test, y_pred)).mean()#np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print(\"RMSE:  {:.2f}\".format(rmse), \"euros or in %\") \n",
    "\n",
    "        ## Feature importance + selection\n",
    "        # ## select significant features by Permuation feature importance\n",
    "        importances = e.permutation_feature_importance(model, X_test, y_test, repeats=5, seed=seed)\n",
    "\n",
    "        df_importance = pd.DataFrame(\n",
    "            {\"importances\" : importances[0]},\n",
    "            index=X_train.columns.to_list(),\n",
    "            ) \n",
    "        df_importance = df_importance.sort_values(\"importances\", ascending=False)  # get most important features to the top\n",
    "        print(\"Most important features:\", df_importance.iloc[:5].index.to_list())\n",
    "        df_importance = df_importance.loc[df_importance.importances >= fi_threshold, : ]\n",
    "        #df_importance.head(5)\n",
    "        # ## write selected predictors and response to disk\n",
    "        fs.save_selected_features(\n",
    "            X_train, \n",
    "            pd.DataFrame(y_train, columns=[target]), \n",
    "            df_importance.T.columns, \n",
    "            filename=f\"../../../input_survey_data/fs_logr_{target.split('_')[1]}_{pipe_name}_dscrt.xlsx\"\n",
    "        )\n",
    "\n",
    "        ## Evaluate\n",
    "        ## print evaluation report + check for overfitting \n",
    "        print(\"\\nTraining set\")\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        #y_pred_train = model_cv.best_estimator_.predict(X_train)\n",
    "        print(f\"Training set score (F1): {round(f1_score(y_train, y_pred_train, average='micro'), 2)}\") \n",
    "        # average=\"micro\"\n",
    "        #e.evaluation_report(y_train, y_pred_train)\n",
    "\n",
    "        print(\"\\nTesting set\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        #e.evaluation_report(y_test, y_pred)\n",
    "        print(f\"Test set score (F1): {round(f1_score(y_test, y_pred, average='micro'), 2)}\") \n",
    "\n",
    "        ## confusion matrix\n",
    "        #fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,8))  # create figure & 1 axis\n",
    "        #sns.heatmap(confusion_matrix(y_test, y_pred), ax=ax, annot=True)\n",
    "        #fig.tight_layout()\n",
    "        cm = f.plot_confusion_matrix(y_test, y_pred,\n",
    "                    model_name=\"logr\", \n",
    "                    target_name=target, \n",
    "                    pipe_name=pipe_name\n",
    "                    ) \n",
    "\n",
    "        \n",
    "        ## evaluate\n",
    "        # scores = cross_validate(elastic_net, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) # neg_mean_absolute_error\n",
    "        # print(scores)\n",
    "        # # force scores to be positive\n",
    "        scores = np.abs(model.coef_)\n",
    "        print('Mean MAE: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "    break  # currently run only for first target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\"bagging__estimator__penalty\": [\"none\"]}\n",
    "# #param_bagging = {'__model__bootstrap': [False]}\n",
    "\n",
    "# ensemble_model = {'model': BaggingRegressor,   # default bootstrap=True\n",
    "#         'kwargs': {'estimator': LogisticRegression()},  # TODO: pass 'random_state':seed to baggingregressor\n",
    "#     \t#'parameters': param_grid,\n",
    "#     \t#'parameters_bag': param_bagging,\n",
    "\n",
    "#         }\n",
    "# pipe_ximput_us_bag_logr = Pipeline([\n",
    "# \t\t('', RandomUnderSampler(\n",
    "# \t\t\tsampling_strategy=ratio_multiplier\n",
    "# \t\t\t#sampling_strategy=ratio_multiplier(sampling_stategy=zero_loss_ratio)\n",
    "# \t\t\t)),\n",
    "# \t\t(('bagging', ensemble_model['model'] (**ensemble_model['kwargs']) ) )\n",
    "# \t])\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)#, random_state=seed)        #  StratifiedKFold = fold contains same percantega of class as in orignal training set, addresees imbalancing\n",
    "# model_cv = RandomizedSearchCV(\n",
    "# \testimator=pipe_ximput_us_bag_logr, \n",
    "# \tparam_distributions=param_grid, \n",
    "# \tcv=cv, \n",
    "# \t#scoring=\"f1_micro\",   #TODO test also e.g \"f1\" or recall, \"neg_mean_absolute_error\",\n",
    "# \trefit=True,   ## Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "# \t\t\t\t\t## If refit=False, clf.fit() will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after fit().\n",
    "# \t\t\t\t\t## ! When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit()\n",
    "# \trandom_state=seed\n",
    "# )\n",
    "\n",
    "# model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['param_model__C', 'param_model__penalty', 'param_model__l1_ratio', 'param_model__max_iter'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anna\\Documents\\UNI\\MA_topic\\flood-loss-models-4-HCMC\\feature-selection-from-remote-fs\\model_preprocessing\\Feature_selection_discretize\\logistic_regression_feature_selection.ipynb Cell 16\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#scores = cross_validate(elastic_net, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#pd.DataFrame(scores).set_index(\"test_score\").sort_index().T\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# #Snippet from: https://ubc-cs.github.io/cpsc330/lectures/08_hyperparameter-optimization.html#exhaustive-grid-search-sklearn-model-selection-gridsearchcv\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#pd.DataFrame(elastic_net_cv.cv_results_).set_index(\"rank_test_score\").sort_index().T\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame(model_cv\u001b[39m.\u001b[39;49mcv_results_)[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         [                                   \u001b[39m# only important cv evaluation metrics \u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmean_test_score\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmean_fit_time\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrank_test_score\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparam_model__C\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparam_model__penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparam_model__l1_ratio\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mparam_model__max_iter\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna/Documents/UNI/MA_topic/flood-loss-models-4-HCMC/feature-selection-from-remote-fs/model_preprocessing/Feature_selection_discretize/logistic_regression_feature_selection.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ]\u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39mrank_test_score\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msort_index()\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\py396_c3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['param_model__C', 'param_model__penalty', 'param_model__l1_ratio', 'param_model__max_iter'] not in index\""
     ]
    }
   ],
   "source": [
    "#scores = cross_validate(elastic_net, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#pd.DataFrame(scores).set_index(\"test_score\").sort_index().T\n",
    "\n",
    "# #Snippet from: https://ubc-cs.github.io/cpsc330/lectures/08_hyperparameter-optimization.html#exhaustive-grid-search-sklearn-model-selection-gridsearchcv\n",
    "#pd.DataFrame(elastic_net_cv.cv_results_).set_index(\"rank_test_score\").sort_index().T\n",
    "pd.DataFrame(model_cv.cv_results_)[\n",
    "        [                                   # only important cv evaluation metrics \n",
    "            \"mean_test_score\",\n",
    "            \"mean_fit_time\",\n",
    "            \"rank_test_score\",\n",
    "            \"param_model__C\",\n",
    "            \"param_model__penalty\",\n",
    "            \"param_model__l1_ratio\",\n",
    "            \"param_model__max_iter\"\n",
    "        ]\n",
    "    ].set_index(\"rank_test_score\").sort_index().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload models\n",
    "\n",
    "print(targets)\n",
    "target = targets[0]\n",
    "\n",
    "logistic_reg_eval = pickle.load(open(f\"./models_trained/logisticreg_{target}.sav\", 'rb'))\n",
    "#elastic_net_eval.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2 training set', round(logistic_reg_eval.score(X_train, y_train)*100, 1), '%')\n",
    "print('R^2 test set', round(logistic_reg_eval.score(X_test, y_test)*100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"penalty term: L1=lasso, L2= rigde : \" , logistic_reg_eval.penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "## coef of feature importances\n",
    "df_importance = pd.DataFrame({\n",
    "    \"name\" : X_unscaled.columns.to_list(),\n",
    "    \"importances\" : np.absolute(logistic_reg_eval.coef_)[0],\n",
    "     }) \n",
    "\n",
    "# drop features which dont reduce the loss\n",
    "df_importance = df_importance.loc[df_importance.importances > 0.01, : ] \n",
    "df_importance = df_importance.sort_values(\"importances\", ascending=False)\n",
    "\n",
    "plt.bar(df_importance.name, df_importance.importances)\n",
    "plt.xticks(\n",
    "    #ticks = range(len(selected_feat)),\n",
    "    #labels = X_unscaled.iloc[:,selected_feat],\n",
    "    rotation = 90\n",
    "    )\n",
    "plt.title(f\"Feature Importances for {target}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef = pd.Series(elastic_net.coef_, index = X_train.columns)\n",
    "# important_features = pd.concat([coef.sort_values().head(10),\n",
    "#                      coef.sort_values().tail(10)])\n",
    "# important_features.plot(kind = \"barh\")\n",
    "# plt.title(\"Coefficients in the ElasticNet Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot optimal number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_eval.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(elastic_net_eval.cv_results_)\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
