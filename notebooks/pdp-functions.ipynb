{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Featue selection with ML models for HCMC survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"a.buch@stud.uni-heidelberg.de\"\n",
    "\n",
    "# ## Feature selection \n",
    "# Enitre workflow with all models for the target variables relative content loss and business reduction (degree of loss) as well for the binary version of relative content loss (chance of loss)\n",
    "# \n",
    "# Due to the samll sample size a nested CV is used to have the possibility to even get generalization error, in the inner CV the best hyperaparamters based on k-fold are selected; in the outer cv the generalization error across all tested models is evaluated. A seprate unseen validation set as done by train-test split would have an insufficent small sample size.\n",
    "# Nested CV is computationally intensive but with the samll sample size and a well chosen set of only most important hyperparameters this can be overcome.\n",
    "# \n",
    "# - Logistic Regression (binary rcloss)\n",
    "# - Elastic Net\n",
    "# - eXtreme Gradient Boosting\n",
    "# - Random Forest\n",
    "# \n",
    "\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "UTILS_PATH = os.path.join(os.path.abspath(\"\"), \"../\", \"utils\")\n",
    "sys.path.append(UTILS_PATH)\n",
    "\n",
    "import feature_selection as fs\n",
    "import training as t\n",
    "import evaluation as e\n",
    "import evaluation_utils as eu\n",
    "import figures as f\n",
    "import settings as s\n",
    "import pipelines as p\n",
    "import preprocessing as pp\n",
    "\n",
    "# p.main()  # create/update model settings\n",
    "seed = s.seed\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "import contextlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#### Load R packages to process Conditional Random Forest in python\n",
    "# *NOTE 1: all needed R packages have to be previously loaded in R*\n",
    "# *NOTE 2: Make sure that caret package version >= 6.0-81, otherwise caret.train() throws an error*\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "from rpy2.robjects.packages import importr, data\n",
    "\n",
    "\n",
    "# get basic R packages\n",
    "utils = importr(\"utils\")\n",
    "base = importr(\"base\")\n",
    "dplyr = importr(\"dplyr\")\n",
    "stats_r = importr(\"stats\")  # rename due to similar python package\n",
    "\n",
    "# pandas.DataFrames to R dataframes \n",
    "pandas2ri.activate()\n",
    "\n",
    "# print r df in html\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "\n",
    "# get libraries for CRF processing, ctree_controls etc\n",
    "party = importr(\"party\")        # Random Forest with Conditional Inference Trees (Conditional Random Forest)\n",
    "permimp = importr(\"permimp\")  # conditional permutation feature importance\n",
    "caret = importr(\"caret\") # package version needs to be higher than  >=  6.0-90\n",
    "nestedcv = importr(\"nestedcv\")\n",
    "tdr = importr(\"tdr\")\n",
    "\n",
    "\n",
    "targets = [(\"rcloss\", \"degree of rcloss\"), (\"rbred\", \"rbred\")]\n",
    "target, target_plot = targets[1]\n",
    "pred_target = f\"pred_{target}\"\n",
    "\n",
    "\n",
    "# Get logger  # test: init application\n",
    "main_logger = f\"__feature_extraction___\"\n",
    "logger = s.init_logger(main_logger)\n",
    "\n",
    "## settings for cv\n",
    "kfolds_and_repeats = 5, 2 # 3, 1  # <k-folds, repeats> for nested cv\n",
    "inner_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=kfolds_and_repeats[1], random_state=seed)\n",
    "# outer_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=kfolds_and_repeats[1], random_state=seed)\n",
    "outer_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=1, random_state=seed) # make same as for R nestedcv.train()\n",
    "\n",
    "\n",
    "## TODO make base outdir ../model_results/degree_of_loss\n",
    "##  out_dir = os.path.join(base_dir, \"path\")\n",
    "\n",
    "## save models and their evaluation in following folders:\n",
    "Path(f\"../model_results/models_trained/rbred/nested_cv_models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../model_results/models_trained/rbred/final_models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../model_results/models_evaluation/rbred\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../model_results/selected_features/rbred\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# df_candidates = pd.read_excel(\"../input_survey_data/input_data_contentloss_tueb_cantho.xlsx\")\n",
    "# df_candidates = pd.read_excel(\"../input_survey_data/input_data_contentloss_tueb.xlsx\")\n",
    "df_candidates = pd.read_excel(\"../input_survey_data/input_data_businessreduction_tueb.xlsx\")\n",
    "\n",
    "##  use nice feature names\n",
    "df_candidates.rename(columns=s.feature_names_plot, inplace=True)\n",
    "\n",
    "print(df_candidates.columns)\n",
    "# with contextlib.suppress(Exception):\n",
    "#     df_candidates.drop([\"hh_monthly_income_euro\", \"shp_content_value_euro\"], axis=1, inplace=True)\n",
    "\n",
    " \n",
    "logger.info(df_candidates.shape)\n",
    "\n",
    "## Evaluation metrics \n",
    "score_metrics = {\n",
    "    \"MAE\": make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    \"RMSE\": make_scorer(eu.root_mean_squared_error, greater_is_better=False),\n",
    "    \"MBE\": make_scorer(eu.mean_bias_error, greater_is_better=False),\n",
    "    # \"R2\": \"r2\",\n",
    "    \"SMAPE\": make_scorer(eu.symmetric_mean_absolute_percentage_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## empty variables to store model outputs\n",
    "eval_sets = {}\n",
    "models_trained = {}\n",
    "final_models_trained = {}\n",
    "models_coef = {}\n",
    "predicted_values = {}\n",
    "df_feature_importances = pd.DataFrame(index=df_candidates.drop(target, axis=1).columns.to_list())\n",
    "models_scores = {}\n",
    "\n",
    "## iterate over piplines. Each pipline contains a scaler and regressor (and optionally a bagging method) \n",
    "pipelines = [\"pipe_en\", \"pipe_crf\", \"pipe_xgb\"]  \n",
    "# pipelines = [\"pipe_en\"]  \n",
    "\n",
    "## Load set of hyperparamters\n",
    "hyperparams_set = pp.load_config(f\"{UTILS_PATH}/hyperparameter_sets.json\")\n",
    "\n",
    "\n",
    "for pipe_name in pipelines:\n",
    "\n",
    "    TIME0 = datetime.now()\n",
    "\n",
    "    ## load model pipelines\n",
    "    pipe = joblib.load(f\"{UTILS_PATH}/pipelines/{pipe_name}.pkl\")\n",
    " \n",
    "    try:\n",
    "        model_name = re.findall(\"[a-zA-Z]+\", str(pipe.steps[1][1].__class__).split(\".\")[-1])[0] # get model name for python models  \n",
    "    except AttributeError:\n",
    "        model_name = pipe # get R model name\n",
    "    \n",
    "    ## load respective hyperparameter space\n",
    "    param_space = hyperparams_set[f\"{model_name}_hyperparameters\"]\n",
    "\n",
    "    ## if bagging fro model training is used , rename hyperparmeters\n",
    "    if \"bag\" in pipe_name.split(\"_\"):\n",
    "        logger.info(f\"Testing {model_name} with bagging\")\n",
    "        param_space = { k.replace(\"model\", \"bagging__estimator\") : v for (k, v) in param_space.items()}\n",
    "\n",
    "\n",
    "    logger.info( f\"\\n\\n############ Applying {model_name} on {target} ############\\n \")\n",
    "\n",
    "    # save original df for later\n",
    "    df_Xy = df_candidates\n",
    "\n",
    "    # rm geometry column which only needed for visualization\n",
    "    df_Xy = df_Xy.drop(\"geometry\", axis=1)\n",
    "\n",
    "    ## drop content value var due its only needed to recalculate losses after BN\n",
    "    with contextlib.suppress(Exception):\n",
    "        df_Xy.drop([\"shp_content_value_euro\"], axis=1, inplace=True)\n",
    "   \n",
    "  \n",
    "    # get predictor names\n",
    "    X_names = df_Xy.drop(target, axis=1).columns.to_list()\n",
    "\n",
    "    ## remove zero-loss records only for combined dataset\n",
    "    if target == \"Target_relative_contentloss_euro\":\n",
    "        logger.info(f\"Removing {df_Xy.loc[df_Xy[target]==0.0,:].shape[0]} zero loss records\")\n",
    "        df_Xy = df_Xy.loc[df_Xy[target]!=0.0,:]\n",
    "\n",
    "\n",
    "    ## drop samples where target is nan\n",
    "    logger.info(f\"Removing {df_Xy[target].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "    df_Xy = df_Xy[ ~df_Xy[target].isna()]\n",
    "\n",
    "    ## Elastic Net and Random Forest: drop samples where any value is nan\n",
    "    if (model_name == \"ElasticNet\") | (model_name == \"cforest\"):\n",
    "        print(\"Dropping records with missing values\")\n",
    "        df_Xy.dropna(inplace=True)\n",
    "        #df_Xy = df_Xy.apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "        # print(\"Impute records with missing values for Elastic Net or cforest\",\n",
    "        #        f\"keeping {df_Xy.shape} damage cases for model training and evaluation\")\n",
    "\n",
    "\n",
    "    logger.info(\n",
    "        f\"Finally use {df_Xy.shape[0]} records for feature extraction, from those are {(df_Xy[target][df_Xy[target] == 0.0]).count()} cases with zero-loss or zero-reduction\",\n",
    "    )\n",
    "\n",
    "    X = df_Xy[X_names]\n",
    "    y = df_Xy[target]\n",
    "\n",
    "    logger.info( f\"Use for feature extraction {X.shape[1]} features to predict {target_plot}\")\n",
    "\n",
    "\n",
    "    ## run sklearn model\n",
    "    if model_name != \"cforest\":\n",
    "\n",
    "\n",
    "        ## fit model for unbiased model evaluation and for final model used for Feature importance, Partial Dependence etc.\n",
    "        mf = t.ModelFitting(\n",
    "            model=pipe, \n",
    "            Xy=df_Xy,\n",
    "            target_name=target,\n",
    "            param_space=param_space,\n",
    "            tuning_score=score_metrics[\"MAE\"], # tune by getting reducing MAE\n",
    "            cv=inner_cv,\n",
    "            kfolds_and_repeats=kfolds_and_repeats,\n",
    "            seed=seed,\n",
    "        )\n",
    "        models_trained_ncv = mf.model_fit_ncv()\n",
    "\n",
    "        # save models from nested cv and final model on entire ds\n",
    "        joblib.dump(models_trained_ncv, f\"../model_results/models_trained/rbred/nested_cv_models/{model_name}_{target}.joblib\")\n",
    "            \n",
    "        ## evaluate model    \n",
    "        me = e.ModelEvaluation(\n",
    "            models_trained_ncv=models_trained_ncv, \n",
    "            Xy=df_Xy,\n",
    "            target_name=target,\n",
    "            score_metrics=score_metrics,\n",
    "            cv=outer_cv,\n",
    "            kfolds=kfolds_and_repeats[0],\n",
    "            seed=seed,\n",
    "        )\n",
    "        model_evaluation_results = me.model_evaluate_ncv()\n",
    "\n",
    "        ## reverse sklearn.cross_validate() outputted regression scores (e.g. MAE, RMSE, SMAPE, R2)\n",
    "        model_evaluation_results = me.negate_scores_from_sklearn_cross_valdiate(  # TODO impl directly as method in ModelEvalaution()\n",
    "            model_evaluation_results, \n",
    "            metric_names=(\"test_MAE\", \"test_MBE\", \"test_RMSE\", \"test_SMAPE\"))\n",
    "\n",
    "  \n",
    "        ## visual check if hyperparameter ranges are good or need to be adapted\n",
    "        logger.info(f\"Parameter sets of best estimators outer test-sets:\") \n",
    "        for i in range(len(model_evaluation_results[\"estimator\"])):\n",
    "            print(f\"{model_name}: \", model_evaluation_results[\"estimator\"][i].best_params_)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        ## store models evaluation \n",
    "        models_scores[model_name] =  {\n",
    "            k: model_evaluation_results[k] for k in tuple(\"test_\" + s for s in list(score_metrics.keys()))\n",
    "        } # get evaluation scores, metric names start with \"test_<metricname>\"\n",
    "\n",
    "  \n",
    "  \n",
    "        ## Final model\n",
    "\n",
    "        ## get  and save final model based on best MAE score during outer cv\n",
    "        best_idx = list(models_scores[model_name][\"test_MAE\"]).index(min(models_scores[model_name][\"test_MAE\"]))\n",
    "        final_model = model_evaluation_results[\"estimator\"][best_idx]\n",
    "        logger.info(f\"Params of best model: {final_model.best_params_}\") \n",
    "        final_model = final_model.best_estimator_\n",
    "\n",
    "         \n",
    "        ## print performance of best estimator   \n",
    "        logger.info(f\"Performance of best estimator\") \n",
    "        for metric in models_scores[model_name].keys():\n",
    "            print(metric, models_scores[model_name][metric][best_idx])\n",
    "         \n",
    "\n",
    "        final_models_trained[model_name] = final_model \n",
    "        joblib.dump(final_model, f\"../model_results/models_trained/rbred/final_models/{model_name}_{target}.joblib\")\n",
    "\n",
    "\n",
    "        ## get predictions of final model from respective outer test set\n",
    "        test_set_best = df_Xy.iloc[model_evaluation_results[\"indices\"][\"test\"][best_idx], :]\n",
    "        finalmodel_X_test = test_set_best.drop(target, axis=1)\n",
    "        finalmodel_y_test = test_set_best[target]\n",
    "        finalmodel_y_pred = final_model.predict(finalmodel_X_test)  # get predictions from final model for its test-set (should be the same as done during model evluation with ncv)\n",
    "\n",
    "\n",
    "        ## Learning curve of train and test set of final model\n",
    "        train_set_best = df_Xy.iloc[model_evaluation_results[\"indices\"][\"train\"][best_idx], :]\n",
    "        f.plot_learning_curves(\n",
    "            final_model, train_set_best, test_set_best, target,\n",
    "            f\"../model_results/models_evaluation/rbred/learning_curves{target}_{model_name}.png\", \n",
    "            model_name)\n",
    "        \n",
    "        \n",
    "        ## Feature importance of best model on its test set\n",
    "        importances = me.permutation_feature_importance(\n",
    "            final_model, \n",
    "            finalmodel_X_test, finalmodel_y_test, \n",
    "            repeats=5)\n",
    "\n",
    "\n",
    "        ## regression coefficients for linear models from best estimator\n",
    "        with contextlib.suppress(Exception):\n",
    "\n",
    "            models_coef[model_name] = me.calc_regression_coefficients(final_model, finalmodel_y_test, finalmodel_y_pred)\n",
    "\n",
    "            outfile = f\"../model_results/models_evaluation/rbred/regression_coefficients_{model_name}_{target}.xlsx\"\n",
    "            models_coef[model_name].round(3).to_excel(outfile, index=True)\n",
    "            logger.info(f\"Regression Coefficients:\\n {models_coef[model_name].sort_values('probabilities', ascending=False)} \\n .. saved to {outfile}\")\n",
    "            \n",
    "            ## check if any regression coefficient is significant \n",
    "            if np.min(models_coef[model_name][\"probabilities\"]) >= 0.05:\n",
    "                ## non permanent decorator, extending with creation of log file for warnings\n",
    "                logger = s.decorate_init_logger(s.init_logger)(\"__warning_coefs__\") \n",
    "                logger.info(\"non of the regression coefficients is significant\")\n",
    "                logger = s.init_logger(main_logger)  # reset to previous state\n",
    "\n",
    "\n",
    "    ## run R model\n",
    "    else:\n",
    "         ## define model settings\n",
    "        mf = t.ModelFitting(\n",
    "            model=pipe,  # pipe contains only name of applied R algorithm \n",
    "            Xy=df_Xy,\n",
    "            target_name=target,\n",
    "            param_space=param_space,\n",
    "            tuning_score=score_metrics[\"MAE\"],\n",
    "            cv=inner_cv,\n",
    "            kfolds_and_repeats=kfolds_and_repeats,\n",
    "            seed=s.seed\n",
    "        )\n",
    "        # NOTE: normalization is not mandatory for decision-trees but might decrease processing time\n",
    "        models_trained_ncv = mf.r_model_fit_ncv()  # pipe\n",
    "        joblib.dump(models_trained_ncv, f\"../model_results/models_trained/rbred/nested_cv_models/{model_name}_{target}.joblib\")\n",
    "\n",
    "\n",
    "        me = e.ModelEvaluation(\n",
    "            models_trained_ncv=models_trained_ncv, \n",
    "            Xy=df_Xy,\n",
    "            target_name=target,\n",
    "            score_metrics=score_metrics,  # make optional in ModelEvlaution() class\n",
    "            cv=outer_cv,\n",
    "            kfolds=kfolds_and_repeats[0],\n",
    "            seed=s.seed\n",
    "        )\n",
    "        model_evaluation_results = me.r_model_evaluate_ncv()\n",
    "\n",
    "\n",
    "        ## get std of CRF from inner folds\n",
    "        ## TODO shorter name for r_model_evaluation_dict        \n",
    "        r_model_evaluation_dict =  {a : [] for a in [\"test_MAE\", \"test_RMSE\", \"test_MBE\", \"test_SMAPE\"]}\n",
    "        # r_model_evaluation_dict =  {a : [] for a in [\"test_MAE\", \"test_RMSE\", \"test_MBE\", \"test_R2\", \"test_SMAPE\"]}\n",
    "        for idx in range(1, kfolds_and_repeats[0]+1):  # number of estimators , R counts starting from 1\n",
    "            df = me.r_models_cv_predictions(idx)  # get all crf estimators from outer cv\n",
    "            r_model_evaluation_dict[\"test_MAE\"].append(mean_absolute_error(df.testy, df.predy))\n",
    "            r_model_evaluation_dict[\"test_RMSE\"].append(eu.root_mean_squared_error(df.testy,df.predy)) #(df.testy, df.predy)\n",
    "            r_model_evaluation_dict[\"test_MBE\"].append(eu.mean_bias_error(df.testy, df.predy))\n",
    "            # r_model_evaluation_dict[\"test_R2\"].append(eu.r2_score(df.testy, df.predy))\n",
    "            r_model_evaluation_dict[\"test_SMAPE\"].append(eu.symmetric_mean_absolute_percentage_error(df.testy, df.predy))\n",
    "      \n",
    "      \n",
    "        ## Final CRF model\n",
    "        robjects.r(\"\"\"\n",
    "            r_final_model <- function(model, verbose=FALSE) {\n",
    "                model$final_fit$finalModel\n",
    "            }\n",
    "        \"\"\")\n",
    "        r_final_model = robjects.globalenv[\"r_final_model\"] \n",
    "        final_model = r_final_model(models_trained_ncv)\n",
    "        # final_model = mf.r_final_model()  # select final model from models_trained_ncv\n",
    "        best_idx = list(r_model_evaluation_dict[\"test_MAE\"]).index(min(r_model_evaluation_dict[\"test_MAE\"]))\n",
    "\n",
    "        ## performance of final CRF model \n",
    "        logger.info(f\"Performance of best CRF model: \")\n",
    "        for metric in r_model_evaluation_dict.keys():\n",
    "            print(f\"{metric}: {r_model_evaluation_dict[metric][best_idx]}\")\n",
    "        \n",
    "        ## plot cforest learning curve        \n",
    "        f.plot_r_learning_curve(\n",
    "            df_Xy, target, \n",
    "            f\"../model_results/models_evaluation/rbred/learning_curves{target}_{model_name}.png\")\n",
    "\n",
    "        ## Feature importance of best model\n",
    "        importances = me.r_permutation_feature_importance(final_model)\n",
    "\n",
    "        ## store model evaluation and final model\n",
    "        models_scores[model_name] = r_model_evaluation_dict ## store performance scores from R estimators        \n",
    "        final_models_trained[model_name] = final_model\n",
    "        joblib.dump(final_model, f\"../model_results/models_trained/rbred/final_models/{model_name}_{target}.joblib\")\n",
    "\n",
    "\n",
    "\n",
    "    # ## Collect all models and their evaluation\n",
    "\n",
    "    ## store fitted models and their evaluation results for later \n",
    "    eval_sets[model_name] = df_Xy\n",
    "    models_trained[f\"{model_name}\"] = models_trained_ncv\n",
    "    predicted_values[model_name] = me.residuals  # y_true, y_pred and residual from outer cv\n",
    "\n",
    "    ## store Feature Importances of each model\n",
    "    logger.info(\"\\nSelect features based on permutation feature importance\")\n",
    "    df_importance = pd.DataFrame(\n",
    "        {\n",
    "            f\"{model_name}_importances\" : importances[0],   # averaged importnace scores across repeats\n",
    "            f\"{model_name}_importances_std\" : importances[1]\n",
    "        },\n",
    "        index=X_names,\n",
    "    )\n",
    "    df_feature_importances = df_feature_importances.merge(\n",
    "        df_importance[f\"{model_name}_importances\"],   # only use mean FI, drop std of FI\n",
    "        left_index=True, right_index=True, how=\"outer\")\n",
    "    df_feature_importances = df_feature_importances.sort_values(f\"{model_name}_importances\", ascending=False)  # get most important features to the top\n",
    "    logger.info(f\"5 most important features: {df_feature_importances.iloc[:5].index.to_list()}\")\n",
    "\n",
    "\n",
    "    logger.info(\n",
    "    f\"\\nTraining and evaluation of {model_name} took {(datetime.now() - TIME0).total_seconds() / 60} minutes\\n\"\n",
    "    )\n",
    "            \n",
    "\n",
    "\n",
    "## Plot performance ranges of all evaluated estimators from outer cross-validation \n",
    "logger.info(\"Creating boxplots for range of performane scores from outer folds of nested cross-validation\")\n",
    "f.boxplot_outer_scores_ncv(\n",
    "    models_scores,\n",
    "    outfile=f\"../model_results/models_evaluation/rbred/boxplot_scores4ncv_{target}.png\",\n",
    "    target_name=target_plot)\n",
    "\n",
    "\n",
    "# store avergaed scores and std for later usage\n",
    "## TODO remove overhead  -> store avergaed scores and std during loop\n",
    "xgb_model_evaluation = pd.DataFrame(models_scores[\"XGBRegressor\"]).mean(axis=0)  # get mean of outer cv metrics (negative MAE and neg RMSE, pos. R2, pos MBE, posSMAPE)\n",
    "xgb_model_evaluation_std = pd.DataFrame(models_scores[\"XGBRegressor\"]).std(axis=0)   # get respective standard deviations\n",
    "crf__model_evaluation = pd.DataFrame(models_scores[\"cforest\"]).mean(axis=0)\n",
    "crf_model_evaluation_std = pd.DataFrame(models_scores[\"cforest\"]).std(axis=0)\n",
    "en_model_evaluation = pd.DataFrame(models_scores[\"ElasticNet\"]).mean(axis=0)\n",
    "en_model_evaluation_std = pd.DataFrame(models_scores[\"ElasticNet\"]).std(axis=0)\n",
    "# xgb_model_evaluation = pd.DataFrame(models_scores[\"XGBRegressor\"]).median(axis=0)  # get median of outer cv metrics (negative MAE and neg RMSE, pos. R2, pos MBE, posSMAPE)\n",
    "# xgb_model_evaluation_std = pd.DataFrame(models_scores[\"XGBRegressor\"]).std(axis=0)   # get respective standard deviations\n",
    "# crf__model_evaluation = pd.DataFrame(models_scores[\"cforest\"]).median(axis=0)\n",
    "# crf_model_evaluation_std = pd.DataFrame(models_scores[\"cforest\"]).std(axis=0)\n",
    "# en_model_evaluation = pd.DataFrame(models_scores[\"ElasticNet\"]).median(axis=0)\n",
    "# en_model_evaluation_std = pd.DataFrame(models_scores[\"ElasticNet\"]).std(axis=0)\n",
    "\n",
    "\n",
    "model_evaluation = pd.concat([en_model_evaluation, en_model_evaluation_std, crf__model_evaluation, crf_model_evaluation_std, xgb_model_evaluation, xgb_model_evaluation_std], axis=1)\n",
    "model_evaluation.columns = [\"ElasticNet_score\", \"ElasticNet_score_std\", \"cforest_score\", \"cforest_score_std\", \"XGBRegressor_score\", \"XGBRegressor_score_std\"]\n",
    "\n",
    "\n",
    "## rename metrics\n",
    "model_evaluation.index = model_evaluation.index.str.replace(\"test_\", \"\")\n",
    "\n",
    "outfile = f\"../model_results/models_evaluation/rbred/performance_{target}.xlsx\"\n",
    "model_evaluation.round(3).to_excel(outfile, index=True)\n",
    "logger.info(f\"Outer evaluation scores of nested cross-validation (mean) :\\n {model_evaluation.round(3)} \\n.. saved to {outfile}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feature Importances \n",
    "\n",
    "#### prepare Feature Importances \n",
    "## Have the same feature importance method across all applied ML models\n",
    "## Weight Importances by model performance on outer loop (mean MAE)\n",
    "## **Overall FI ranking (procedure similar to RÃ¶zer et al 2019; Brill 2022)**\n",
    "\n",
    "## weight FI scores based on performance ; weigth importances from better performed models stronger\n",
    "model_weights =  {\n",
    "    \"XGBRegressor_importances\" : np.mean(models_scores[\"XGBRegressor\"][\"test_MAE\"]),\n",
    "    \"ElasticNet_importances\" : np.mean(models_scores[\"ElasticNet\"][\"test_MAE\"]),\n",
    "    \"cforest_importances\" : np.mean(models_scores[\"cforest\"][\"test_MAE\"]),\n",
    "}\n",
    "df_feature_importances_w = fs.calc_weighted_sum_feature_importances(df_feature_importances, model_weights)\n",
    "\n",
    "\n",
    "####  Plot Feature importances\n",
    "\n",
    "## the best model has the highest weighted feature importance value\n",
    "df_feature_importances_plot = df_feature_importances_w\n",
    "\n",
    "## drop features which dont reduce the loss\n",
    "df_feature_importances_plot = df_feature_importances_plot.loc[df_feature_importances_plot.weighted_sum_importances > 0.0, : ] \n",
    "\n",
    "## plot stacked FI\n",
    "f.plot_stacked_feature_importances(\n",
    "    df_feature_importances_plot[[\"ElasticNet_importances_weighted\", \"cforest_importances_weighted\", \"XGBRegressor_importances_weighted\",]],\n",
    "    target_name=target_plot,\n",
    "    model_names_plot = (\"Elastic Net\", \"Conditional Random Forest\", \"XGBRegressor\"),\n",
    "    outfile=f\"../model_results/models_evaluation/rbred/feature_importances_{target}.png\"\n",
    ")\n",
    "\n",
    "\n",
    "## Save final feature space \n",
    "### The final selection of features is used later for the non-parametric Bayesian Network\n",
    "\n",
    "## drop records with missing target values\n",
    "logger.info(f\"Dropping {df_candidates[f'{target}'].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "df_candidates = df_candidates[ ~df_candidates[target].isna()]\n",
    "logger.info(f\"Keeping {df_candidates.shape[0]} records and {df_candidates.shape[1]} features\")\n",
    "\n",
    "\n",
    "## sort features by their overall importance (weighted sum across across all features) \n",
    "final_feature_names = df_feature_importances_w[\"weighted_sum_importances\"].sort_values(ascending=False).index##[:10]\n",
    "\n",
    "## save important features, first column contains target variable\n",
    "fs.save_selected_features(\n",
    "    df_candidates.drop(target, axis=1), # TODO adpat function that target is only once added\n",
    "    pd.DataFrame(df_candidates, columns=[target]), \n",
    "    final_feature_names,\n",
    "    filename=f\"../model_results/selected_features/rbred/final_predictors_{target}.xlsx\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial dependence\n",
    "## PDP shows the marginal effect that one or two features have on the predicted outcome.\n",
    "\n",
    "\n",
    "## store partial dependences for each model\n",
    "pdp_features = {a : {} for a in [\"ElasticNet\", \"XGBRegressor\", \"cforest\"]}\n",
    "\n",
    "for model_name in [\"ElasticNet\", \"XGBRegressor\", \"cforest\"]:\n",
    "\n",
    "    Xy_pdp = eval_sets[model_name].dropna() #  solve bug on sklearn.partial_dependece() which can not deal with NAN values\n",
    "    X_pdp, y_pdp = Xy_pdp[Xy_pdp.columns.drop(target)], Xy_pdp[target]\n",
    "\n",
    "    ## NOTE scaling mandatory for cforest model (which dont accepts the rescaled values for R PDPs function)\n",
    "    ## all PDs are based on only complete records - therefore XGB, EN needs \"scale\":True , and CRF is scaled directly \n",
    "    if model_name == \"cforest\": \n",
    "        scaler = MinMaxScaler()\n",
    "        X_pdp = pd.DataFrame(\n",
    "                scaler.fit_transform(X_pdp), # for same x-axis scaled pd plots across models\n",
    "                columns=X.columns\n",
    "                )\n",
    "    Xy_pdp = pd.concat([y_pdp, X_pdp], axis=1)\n",
    "\n",
    "    for predictor_name in X.columns.to_list(): \n",
    "        features_info =  {\n",
    "            \"model\" : final_models_trained[model_name], \n",
    "            \"Xy\" : Xy_pdp, \n",
    "            \"y_name\" : target, \n",
    "            \"feature_name\" : predictor_name, \n",
    "            # \"percentiles\" : (0.05, .95), # causes NAN for some variables for XGB if (0, 1)\n",
    "            \"scale\"  : True\n",
    "        }  \n",
    "        # get Partial dependences for sklearn models      \n",
    "        if model_name != \"cforest\": \n",
    "            partial_dep = me.get_partial_dependence(**features_info)\n",
    "\n",
    "        # get Partial dependences for R models      \n",
    "        else:  \n",
    "            # features_info.pop(\"percentiles\")\n",
    "            ##  change get_partial_dependence() function only temporary to process R model instead of sklearn models\n",
    "            partial_dep = me.decorator_func(**features_info) (me.get_partial_dependence)()  \n",
    "            # R partial func scales predictor values differenctly, thus rescale them back to range between 0 and 1\n",
    "            scaler = MinMaxScaler()\n",
    "            partial_dep[predictor_name] = scaler.fit_transform(partial_dep[[predictor_name]])\n",
    "          \n",
    "        pdp_features[model_name][predictor_name] = partial_dep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot PDP\n",
    "\n",
    "most_important_features = df_feature_importances_plot.sort_values(\"weighted_sum_importances\", ascending=False).index\n",
    "# most_important_features = df_feature_importances.index\n",
    "categorical = [] # e.g. [\"flowvelocity\", \"further_variables ..\"]\n",
    "ncols = 3\n",
    "nrows = len(most_important_features[:9])  # currently plots the 9 most important features\n",
    "idx = 0\n",
    "\n",
    "plt.figure(figsize=(10, 25))\n",
    "plt.suptitle(f\"Partial Dependences for {target_plot}\", fontsize=16, fontweight=\"bold\", y=.99)\n",
    "# plt.subplots_adjust(top=0.97)\n",
    "\n",
    "## legend\n",
    "top_bar = mpatches.Patch(color=\"steelblue\", label=\"Elastic Net\", alpha=.7)  #TODO update with s.color_palette_models from settings\n",
    "middle_bar = mpatches.Patch(color=\"darkblue\", label=\"Conditional Random Forest\", alpha=.7)\n",
    "bottom_bar = mpatches.Patch(color=\"grey\", label=\"XGBRegressor\", alpha=.7)\n",
    "plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "plt.tick_params(axis='y', which='major', labelsize=12)\n",
    "plt.legend(handles=[top_bar, middle_bar, bottom_bar], loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "## create PDP for all three models\n",
    "for feature in most_important_features[:9]:\n",
    "    # for model_name, color, idx_col in zip([\"ElasticNet\", \"ElasticNet\", \"ElasticNet\"], [\"steelblue\", \"darkblue\", \"grey\"], [0, 0, 0]):\n",
    "    for model_name, color, idx_col in zip([\"ElasticNet\", \"cforest\", \"XGBRegressor\"], [\"steelblue\", \"darkblue\", \"grey\"], [0, 0, 0]):\n",
    "\n",
    "        # idx position of subplot and plot settings\n",
    "        sns.set_style(\"whitegrid\", {\"grid.linestyle\": \":\"})\n",
    "        ax = plt.subplot(nrows, ncols, idx + 1 + idx_col)\n",
    "        feature_info = {\"color\" : color, \"ax\" : ax} \n",
    "\n",
    "        ## partial dependence of one feature for one model to plot\n",
    "        df_pd_feature = pdp_features[model_name][feature]  \n",
    "        \n",
    "        # plot\n",
    "        p = f.plot_partial_dependence(\n",
    "            df_pd_feature, \n",
    "            feature_name=feature, \n",
    "            partial_dependence_name=\"yhat\", \n",
    "            categorical=[],\n",
    "            outfile=f\"../model_results/models_evaluation/rbred/pdp_{target}.png\",\n",
    "            **feature_info\n",
    "            )\n",
    "        p\n",
    "        plt.ylim(0,30)\n",
    "        # plt.title(feature)\n",
    "        visible_ticks = {\"top\": False, \"right\": False}\n",
    "        plt.tick_params(axis=\"x\", which=\"both\", **visible_ticks)\n",
    "        \n",
    "    sns.rugplot(df_pd_feature, x=feature, height=.02, color=\"black\")\n",
    "    idx = idx + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Empirical ~ predicted\n",
    "# ## use y_pred cross-valdiated from outer folds, mulitplied by 100 for more readable output\n",
    "# for k,v in predicted_values.items():\n",
    "#     print(f\"\\n{k} predicted target from cross-valdiated outer folds:\")\n",
    "#     print(eu.empirical_vs_predicted(predicted_values[k][\"y_true\"], predicted_values[k][\"y_pred\"]))\n",
    "\n",
    "\n",
    "# # ### Plot prediction error from outer cv\n",
    "# f.plot_residuals(\n",
    "#     df_residuals=predicted_values, \n",
    "#     model_names_abbreviation=[\"ElasticNet\", \"cforest\", \"XGBRegressor\"],  \n",
    "#     model_names_plot=[\"Elastic Net\", \"Conditional Random Forest\", \"XGBoost\"],\n",
    "#     outfile=f\"../model_results/models_evaluation/rbred/residuals_{target}.png\"\n",
    "# )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
