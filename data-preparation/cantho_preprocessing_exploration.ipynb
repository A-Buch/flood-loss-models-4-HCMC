{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing of Can Tho Dataset and compariosn with HCMC dataset\n",
    "AIm: Valdiate that both datasets dont differ too much from each other, so that they can be compared with each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data preprocessing for Can Tho survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"anna.buch@uni-heidelberg.de\"\n",
    "\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "import src.settings as s\n",
    "\n",
    "SRC_PATH = os.path.join(os.path.abspath(\"\"), \"../\", \"src\")\n",
    "OUTPATH_FIGURES = Path(\"../../figures\")\n",
    "OUTPATH_FIGURES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#s.init()\n",
    "# seed = s.seed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load DS for flood event from 2011 in CanTHo\n",
    "df_cantho_raw = pd.read_excel(f\"../{s.INPATH_DATA}/input_data_cantho_2013_raw_no_multiheader.xlsx\") # removed mulitple header rows\n",
    "\n",
    "# ## load HCMC DS for flood events between 2010-2020\n",
    "df_hcmc_rloss = pd.read_excel(f\"../{s.INPATH_DATA}/input_data_contentloss_tueb.xlsx\")\n",
    "df_hcmc_bred = pd.read_excel(f\"../{s.INPATH_DATA}/input_data_businessreduction_tueb.xlsx\")\n",
    "\n",
    "df_hcmc_bothtargets = pd.read_excel(f\"../{s.INPATH_DATA}/input_data_bothtargets_tueb.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho = df_cantho_raw\n",
    "\n",
    "## missing values\n",
    "df_cantho = df_cantho.replace({99.0: np.nan, 88.0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cantho_raw[[\"1.5\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename columns \n",
    "rename cols to the ones used in HCMC ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## assigne new columns which have to be created in this script, eg relative contnet loss\n",
    "df_cantho[\"Target_relative_contentloss_euro\"] = None\n",
    "df_cantho[\"Target_contentloss_euro\"] = None\n",
    "\n",
    "\n",
    "\n",
    "col_names = {\n",
    "       r'3.31$' : 'Target_eloss_VND', \n",
    "       r'^3.34$' : 'Target_gloss_VND', \n",
    "       r'3.40$' : 'Target_businessreduction',\n",
    "\n",
    "       r'3.13.b$' : 'inundation_duration_day/month',\n",
    "       r'3.13.c$' :'inundation_duration_hour/day',\n",
    "       r'^3.12$' : 'water_depth_cm', \n",
    "       r'3.16$' : 'flowvelocity', \n",
    "       r'3.14$' : 'contaminations', \n",
    "       r'^2.2$' : 'flood_experience', \n",
    "\n",
    "       '3.7.' : 'emergency_measures', \n",
    "       '4.1.' : 'precautionary_measures',  \n",
    "       # not incl pumping euqipmentn and saving of valuables, \n",
    "       # but incl cheap funriture and low-value usage\n",
    "\n",
    "       r'7.3$' : 'bage', \n",
    "       r'11.8$' : 'b_area',\n",
    "       r'11.9$' : 'floors',\n",
    "       r'7.4$' : 'building_value_cat',\n",
    "       '3.18.' : 'overall_problem_house',\n",
    "       r'11.6$' : 'content_value_g_VND',\n",
    "       r'11.7$' : 'content_value_e_VND',\n",
    "                  #  \"P1Q5.8.1\":\"shp_business_limitation_r\",\n",
    "                  #    \"P1Q5.8.2\":\"shp_business_limitation_s\", # needed for modelling monetary loss (abs. loss) of business reduction \n",
    "\n",
    "       r'11.2$' : 'shp_employees', \n",
    "       r'11.3.1$' : 'shp_avgmonthly_sale_VND', \n",
    "       r'10.5$' : 'hh_monthly_income_cat',\n",
    "       r'11.5$' : 'shp_registered_capital_VND_cat', # 11.5.\tHow much did you pay for opening your shop\n",
    "\n",
    "\n",
    "       ## further variables which might be intresting but not needed for feature selection\n",
    "       r'^1.4$' : 'shp_sector',\n",
    "       r'^3.2$' : 'flood_type',\n",
    "       r'^3.5$' : 'warning_time_day',  # for HCMC Q1P2.9 wanring_time_hours\n",
    "       r'3.9$' : 'effect_emergency_measures',\n",
    "       r'3.32$' : 'replaced_cost_e',\n",
    "       r'3.34$' : 'replaced_cost_g',\n",
    "\n",
    "        # Risk perception and resilience\n",
    "       r'^5.1$' : 'risk_future_flood',\n",
    "       r'^5.2$' : 'risk_consequents_future_flood',\n",
    "       r'4.9.1$' : 'perception_too_destructive_floods',  # NOTE == resilienceLeftAlone\n",
    "       r'4.1.3' : 'resilience_joined_neighborhood_network', # 3 categories [Year\tBefore flood 2011\tAfter flood 2011]\n",
    "       r'6.1.1' : 'resilience_flood_management', # binary\n",
    "       r'6.1.2' : 'resilience_govern_careing', # binary\n",
    "       r'6.3$' : 'resilience_city_protection', # scaled [1-6]\n",
    "\n",
    "       # '6.1' : 'resilience_govern_management',\n",
    "       # '6.2' : 'perception_govern_increase_management',\n",
    "       #'6.3' : 'perception_govern_protection', # is scaled 1-6\n",
    "       #'resilience', \n",
    "\n",
    "       r'7.2$' : 'ownership',\n",
    "       r'11.15$' : 'builing_elevation',\n",
    "       'Type of house' : 'builing_type',\n",
    "}\n",
    "\n",
    "for k, v in col_names.items():\n",
    "    df_cantho.rename(columns ={ i: re.sub(k, v, i) for i in  df_cantho.columns }, inplace=True )\n",
    "\n",
    "## drop unneeded columns \n",
    "df_cantho = df_cantho[df_cantho.columns[ ~df_cantho.columns.str.match('^\\d')]]\n",
    "\n",
    "\n",
    "# target vars for relative and absolute costs on content loss [VND]\n",
    "df_cantho.insert(0, \"Target_eloss_VND\", df_cantho.pop(\"Target_eloss_VND\")) \n",
    "df_cantho.insert(1, \"Target_gloss_VND\", df_cantho.pop(\"Target_gloss_VND\"))\n",
    "df_cantho.insert(2, \"Target_contentloss_euro\", df_cantho.pop(\"Target_contentloss_euro\"))\n",
    "df_cantho.insert(3, \"Target_relative_contentloss_euro\", df_cantho.pop(\"Target_relative_contentloss_euro\"))\n",
    "\n",
    "# explanatory var: monthly reduction of business [%] \n",
    "df_cantho.insert(4, \"Target_businessreduction\", df_cantho.pop(\"Target_businessreduction\"))  \n",
    "\n",
    "\n",
    "df_cantho.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE resilience_flood_management == 6.1.1 is not binary (as in questionary.docx)\n",
    "df_cantho_raw[\"6.1.1\"].value_counts()\n",
    "df_cantho.resilience_flood_management.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge both absolute content loss targets\n",
    "df_cantho[\"Target_contentloss_VND\"]  = df_cantho.Target_eloss_VND + df_cantho.Target_gloss_VND\n",
    "np.round(df_cantho[\"Target_contentloss_VND\"], 2).describe()\n",
    "\n",
    "## merge content values \n",
    "df_cantho[\"shp_content_value_VND\"]  = df_cantho.content_value_g_VND + df_cantho.content_value_e_VND\n",
    "\n",
    "df_cantho.drop([\"Target_eloss_VND\", \"Target_gloss_VND\", \"content_value_g_VND\", \"content_value_e_VND\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrological variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[\"inundation_duration_h\"] =  df_cantho[\"inundation_duration_hour/day\"] # <<-- more similar to HCMC, than df_cantho[\"inundation_duration_day/month\"] * inundation_duration_hour/day\n",
    "\n",
    "\n",
    "## verify with HCMC\n",
    "print(\"CanTHo\", df_cantho.inundation_duration_h.describe(), \"\\n\", \"HCMC\", df_hcmc_rloss.inundation_duration_h.describe())\n",
    "\n",
    "# # water depth [cm]\n",
    "print(\"CanTHo\", df_cantho.water_depth_cm.describe(), \"\\n\", \"HCMC\", df_hcmc_rloss.water_depth_cm.describe())\n",
    "# median: 0.20cm, mean: 0.25m, max: 0.80m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contamination\n",
    "## rank contamination according to their occurence and damage potential: 0:no contamination, 1:light contamination and 2:heavy contamination\n",
    "\n",
    "df_contaminations = df_cantho.filter(like=\"contaminations\", axis=1)\n",
    "\n",
    "df_cantho[\"contaminations\"] = df_cantho[\"contaminations\"].replace(\n",
    "    {\n",
    "    0.00 : 0, #  no contamination\n",
    "    4.00 : 0, # no contamination  \n",
    "    1.00 : 1, # light\n",
    "    2.00 : 2,  # heavy\n",
    "    3.00 : 2,  # heavy\n",
    "    }\n",
    ")\n",
    "df_cantho.contaminations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign according to HCMC distribution\n",
    "\n",
    "\n",
    "## HCMC scales\n",
    "# all_input.flood_experience = all_input.flood_experience.replace(\n",
    "#     {\n",
    "#       1:3,     # less than once a year , freq: 1\n",
    "#       2:8,     # about once a year, freq: 14\n",
    "#       3:16,      # twice a year, freq: 37\n",
    "#       4:36,      # 4 times a year, freq: 58\n",
    "#       5:76,      # 8 times a year, freq: 98\n",
    "#       6:151     # around 15 times a year - used range 100-200, freq: 112 (more than 10 times a year, 100 times since 2010)\n",
    "#     }\n",
    "# )\n",
    "\n",
    "## adapt to flood experience scales in HCMC\n",
    "df_cantho.flood_experience = df_cantho.flood_experience.replace(\n",
    "    {\n",
    "        #  less than once a year doesnot exist in Can Tho\n",
    "        1.0 : 8,    # once a year\n",
    "        2.0 : 16,   # twice a year\n",
    "        4.0 : 26,    # three times a year (doesnt exist for HCMC)\n",
    "        4.0 : 36,   # 4 times a year\n",
    "        5.0 : 76,    # More than four times a year\n",
    "        # around 15 times a year does nto exist in cantho\n",
    "    }\n",
    ")\n",
    "df_cantho.flood_experience.value_counts()\n",
    "\n",
    "\n",
    "## TODOD  bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flowvelo\n",
    "\n",
    "## Paprotny 2021:  As for flow velocity, the respondents assessed i based on a qualitative scale, \n",
    "# providing a value from 1 to 6, with half-points possible (Thieken et al. 2005). \n",
    "# A value of 0.1 m/s was assigned to each full step of this qualitative scale. \n",
    "\n",
    "\n",
    "## flowvelocity high depends on the exepience and Einschätzung of the intrviewee\n",
    "## e.g. has the interviewee ever seen turbulent water\n",
    "## still the variable shows the general characteristics, if flood water is fast or not\n",
    "## Extrapolation: use fv as input and also for damage-funs(due that maybe most important var in FI) \n",
    "## - flowvelo is highly related to inundation duration --> from inundation duration possible to derive flovelo \n",
    "\n",
    "## adapt to v scales in HCMC\n",
    "df_cantho.flowvelocity = df_cantho.flowvelocity.replace(        # 1 - 6 # calm - torrential\n",
    "    {\n",
    "        0 : np.nan,  \n",
    "        1 : 0.1,  # calm\n",
    "        2 : 0.1,  # calm\n",
    "        3 : 0.2, \n",
    "        4 : 0.3, \n",
    "        5 : 0.4,\n",
    "        6 : 0.5  # torrential\n",
    "    }\n",
    ")\n",
    "\n",
    "#df_cantho.flowvelocity.value_counts()\n",
    "df_cantho.flowvelocity.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[\"building_value_mVND\"] = df_cantho[\"building_value_cat\"].replace(\n",
    "    {\n",
    "        1.0 : 2.5,\n",
    "        2.0 : 7.5,\n",
    "        3.0 : 15,\n",
    "        4.0 : 35,\n",
    "        5.0 : 75,\n",
    "        6.0 : 150,\n",
    "        7.0 : 350,\n",
    "        8.0 : 750,\n",
    "        9.0 : 1500,\n",
    "        10.0 : 3500,\n",
    "    }\n",
    ")\n",
    "df_cantho = df_cantho.drop(\"building_value_cat\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bv\n",
    "# median: 75 mVND, mean: 75 mVND, max 3500 mVND\n",
    "# print(df_cantho.floors.describe() ) # meaningless or only flat buildings (or count ground floor = 0, first level = 1 ?)\n",
    "    \n",
    "## fix floors :\n",
    "## problably 0 : bungalow with only ground floor --> in HCMC would be this 1 floor\n",
    "df_cantho[\"floors\"] = df_cantho[\"floors\"] +1 \n",
    "\n",
    "## remove records with more than 3 floors\n",
    "df_cantho = df_cantho.loc[df_cantho[\"floors\"] <= 3, :]  # rm ~ 20 records\n",
    "\n",
    "\n",
    "\n",
    "## verify with HCMC\n",
    "print(\"CanTHo\", df_cantho.floors.describe(), \"\\n\", \"HCMC: median and mean are at 2 floors \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[\"bage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in HMCC oldest bui is around 90 to 100 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bage\n",
    "# convert fro mcategrical scale to relative scale with bage at time of flood event\n",
    "\n",
    "# print(df_cantho[\"bage\"].describe())\n",
    "\n",
    "## repalce missing info\n",
    "df_cantho.bage = df_cantho.bage.replace(0, np.nan)  # P4Q2.2: when was the house constructed [year]?\n",
    "df_cantho.bage = df_cantho.bage.replace(1, np.nan)  # existent but not mention year\n",
    "\n",
    "## categories --> continous scales [year]\n",
    "df_cantho[\"bage\"]  = df_cantho[\"bage\"].replace(\n",
    "    {\n",
    "       # 1.0 : specific year,  # existent but not mention year\n",
    "        2 : 1950,\n",
    "        3 : 1965,\n",
    "        4 : 1975,\n",
    "        5 : 1985,\n",
    "        6 : 1995,\n",
    "        7 : 2003,\n",
    "        #8 : 2010,  # not in repsonded in cantho survey\n",
    "    }\n",
    ")\n",
    "\n",
    "flood_year = 2011  # year of flood event in Can Tho\n",
    "\n",
    "# ## NOTE [\"bage\"] is here converted to relative scale: its then the b.age at the time of flood event\n",
    "df_cantho[\"bage\"] = flood_year - df_cantho.bage.astype(\"Int64\")  # building age at time of flood event\n",
    "df_cantho[\"bage\"] = df_cantho[\"bage\"].astype(float)\n",
    "\n",
    "print(df_cantho[\"bage\"].describe())\n",
    "\n",
    "## --> most bui buid in 90s (and in 80s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### socio economic status\n",
    "check if income , monthly sale etc is similar to the HCMC shophouses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[\"hh_monthly_income_mVND\"]  = df_cantho[\"hh_monthly_income_cat\"].replace(\n",
    "    {\n",
    "        1.0 : 0.25,  # in mVND\n",
    "        2.0 : 0.75,\n",
    "        3.0 : 1.5,\n",
    "        4.0 : 3.5,\n",
    "        5.0 : 7.5,\n",
    "        6.0 : 15,\n",
    "        7.0 : 35,\n",
    "        8.0 : 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_cantho.drop(\"hh_monthly_income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shp_registered_capital_VND\n",
    "\n",
    "df_cantho[\"shp_registered_capital_mVND\"]  = df_cantho[\"shp_registered_capital_VND_cat\"].replace(\n",
    "    {\n",
    "        1.0 : 25,  # in mVND\n",
    "        2.0 : 75,\n",
    "        3.0 : 150,\n",
    "        4.0 : 350,\n",
    "        5.0 : 750,\n",
    "        6.0 : 1500,\n",
    "        7.0 : 3500,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_cantho.drop(\"shp_registered_capital_VND_cat\", axis=1, inplace=True)\n",
    "# df_cantho[\"shp_registered_capital_mVND\"].describe()\n",
    "\n",
    "## compared to HCMC\n",
    "##  --> medians are similar, 1. and 3.Q: are also more or less similar when considering the categrical sturcutre for CanTho\n",
    "## a bit too high values for CanTHo probably due to the categorical intervals were smallest registed captial value caputres most cases ,\n",
    "# ## while for HCMC a continous scale was used in the questionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cantho[[\"hh_monthly_income_mVND\", \"shp_employees\", \"shp_avgmonthly_sale_VND\", \"shp_registered_capital_mVND\"]].describe()\n",
    "# df_cantho[df_cantho.number_employees>=20]  # 5 businesses with more than 20 employees\n",
    "\n",
    "## --> first keep also larger shops due that their absolute losses ae similar high aas for HCMC around 2500 €\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cantho[df_cantho.number_employees>=20]  # 5 businesses with more than 20 employees\n",
    "\n",
    "## --> first keep also larger shops due that their absolute losses ae similar high aas for HCMC around 2500 €"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business-characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HCMC\n",
    "# hh_income\t[category]\n",
    "# median 300 €, mean: 425 €, max: 3320 €\n",
    "\n",
    "# number_employees\t\n",
    "# median: 2,  mean: 2,  max: 10\n",
    "\n",
    "# shp_avg_monthly_sale\n",
    "# median: 290 €, mean: 370 €, max: 2760 €\n",
    "\n",
    "\n",
    "\n",
    "## Can Tho\n",
    "# hh_income\t[category]\n",
    "# median: ~ 140€, mean 140€-380€, max > 2000€\n",
    "\n",
    "# number_employees\t\n",
    "# median: 1 mean: 1,  max: 37\n",
    "\n",
    "# shp_avg_monthly_sale [mVND]\n",
    "# median: 192 €, mean: 1670 €, max: 346500 €\n",
    "\n",
    "## --> surveyed people in CanTHo have smaller income than in HCMC --> TODO check ownership (more employee stauts than in HCMC ? )\n",
    "## --> maybe remove shop in CanTho with many employees and high sale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset idnex to avoid problem when re-merging with indicators \n",
    "df_cantho = df_cantho.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emergency\n",
    "\n",
    "# df_cantho.emergency_measures1 - 9\n",
    "\n",
    "## emergency measures as indicator : range: 0:no measures - 6: applied all measures \n",
    "\n",
    "pattern = [r\"^emergency_measures.?\"] \n",
    "pattern_cols = re.compile('|'.join(pattern))\n",
    "df_emergency = df_cantho.filter(regex=pattern_cols, axis=1)\n",
    "df_emergency\n",
    "\n",
    "## create indicator as ratio between implemented and potentially implemented emergency measures\n",
    "df_cantho[\"emergency_measures\"] = None\n",
    "df_cantho[\"emergency_measures\"] = df_emergency.eq(1).sum(axis=1) / len(df_emergency.columns)\n",
    "\n",
    "# keep only indicator in final df\n",
    "df_cantho.drop(\n",
    "    df_emergency.filter(\n",
    "        regex=r\"^(?:.+\\d)$\"\n",
    "        ).columns, \n",
    "    axis=1, inplace=True\n",
    ")\n",
    "df_cantho.emergency_measures.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## overall_problem_house\n",
    "\n",
    "## Cantho survey question\n",
    "# 1: My house was collapsed or washed away \n",
    "# 2: My house was damage partly (specify)\n",
    "# 3: My house was full of water which caused damage to the floor, walls, etc.\n",
    "\n",
    "\n",
    "pattern = [r\"overall_problem_house.(?<!4)$\"] # get all columns but not overall_problem_house_r4: no problem/ problem \n",
    "pattern_cols = re.compile('|'.join(pattern))\n",
    "df_problem_house = df_cantho.filter(regex=pattern_cols, axis=1)\n",
    "df_problem_house.describe()\n",
    "\n",
    "# ## create indicator (based on ranking scheme from HCMC)\n",
    "df_cantho[\"overall_problem_house\"] = None\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house1\"]==1)\n",
    "df_cantho.overall_problem_house[idx[0].tolist()] = 6  # heavy building damage\n",
    "\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house2\"]==1)\n",
    "df_cantho.overall_problem_house[idx[0].tolist()] = 4  # partly building damage\n",
    "\n",
    "## fix typos in overall_problem_house3\n",
    "df_problem_house[\"overall_problem_house3\"] = df_problem_house[\"overall_problem_house3\"].replace({10:1, 3:1})\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house3\"]==1)\n",
    "df_cantho.overall_problem_house[idx[0].tolist()] = 3  # floor and wall damage\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df_cantho.drop(\n",
    "    df_cantho.filter(regex=r\"^overall_problem_house.$\").columns,\n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df_cantho.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # df_cantho.precautionary_measures1 - 10\n",
    "# # #df_cantho.precautionary_measures1.value_counts()\n",
    "# # df_precautionary.describe()\n",
    "\n",
    "# pattern = [r\"^precautionary_measures.*(?<![2,3])$\"] # exclude information measures\n",
    "# pattern_cols = re.compile('|'.join(pattern))\n",
    "# df_precautionary = df_cantho.filter(regex=pattern_cols, axis=1)\n",
    "# print(df_precautionary.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precaution measures \n",
    "# ## --> test first as seperate features to keep as much information as possible\n",
    "\n",
    "pattern = [r\"^precautionary_measures.*(?<![2,3])$\"] # exclude information measures\n",
    "pattern_cols = re.compile('|'.join(pattern))\n",
    "df_precautionary = df_cantho.filter(regex=pattern_cols, axis=1)\n",
    "# print(df_precautionary.columns)\n",
    "df_precautionary.drop(\"precautionary_measures1\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "## fix typo\n",
    "df_precautionary = df_precautionary.replace({40.00:np.nan})\n",
    "\n",
    "# ## create ratio: \n",
    "df_precautionary = df_precautionary.replace(\n",
    "    {\n",
    "        4:1,\n",
    "        3:1,    # impl before \n",
    "        2:1,    \n",
    "        1:1,     \n",
    "        0:0     # not at all  \n",
    "    }\n",
    ")\n",
    "# ## devide into expensive and low cost precautionary measures due that expensive meausres seems to be better in flood-loss-reduction\n",
    "df_precautionary_expensive = df_precautionary[[\"precautionary_measures9\", \"precautionary_measures10\", \"precautionary_measures7\"]]\n",
    "df_precautionary_low = df_precautionary.drop([\"precautionary_measures9\", \"precautionary_measures10\", \"precautionary_measures7\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# create indicator as ratio between implemented and potentially implemented measures (in total 7 measures exist)\n",
    "# range: 0.0: none measure were implemented before the flood event - 1.0: all potential measures were implemented before the reported flood\n",
    "# --> splitting in low and expensive precaution measures reduces the importance of the feautres for the models remarkable\n",
    "df_cantho[\"precautionary_measures_lowcost\"] = None\n",
    "df_cantho[\"precautionary_measures_lowcost\"] = df_precautionary_low.sum(axis=1) / len(df_precautionary_low.columns)\n",
    "df_cantho[\"precautionary_measures_expensive\"] = None\n",
    "df_cantho[\"precautionary_measures_expensive\"] = df_precautionary_expensive.sum(axis=1) / len(df_precautionary_expensive.columns)\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df_cantho.drop(\n",
    "    df_cantho.filter(regex=r\"\\d$\").columns,\n",
    "    # df_cantho.filter(regex=r\"^precautionary_measures.$\").columns,  # not caputere precuation_10\n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df_cantho.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[[\"precautionary_measures_lowcost\", \"precautionary_measures_expensive\"]].describe()\n",
    "\n",
    "## HCMC for both precautionary indicators\n",
    "# keys stats are similar between both DS (similar mean and median ) \n",
    "## --> for CanTHo it seems that at least lesss often all types of low and expensive meausres were implemented compared to HCMC, but on average more than in HCMC (compare medians between both DS for low and expensive precautions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_resilience.resilience_flood_management.value_counts()\n",
    "\n",
    "# has one anser with 2 , and one reply with 6 \n",
    "# --> seems like outliers whiche are removed or entire variable is not used in reslience indeicator due that its only binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## resilience as indicator:\n",
    "# ## 1 : strong disagree - 5 : strong agree\n",
    "\n",
    "\n",
    "## select based on findings from PCA\n",
    "# for HCMC DS: df_resilience = df_cantho[[\"resilience_city_protection\", \"resilience_govern_careing\",\"resilience_neighbor_management\"]]\n",
    "# for CanTHo DS\n",
    "df_resilience = df_cantho[\"resilience_city_protection\"]\n",
    "# df_resilience = df_cantho[[\"resilience_city_protection\",\"resilience_flood_management\", \"resilience_govern_careing\"]]\n",
    "print(df_resilience.describe())\n",
    "\n",
    "# convert binary resilience_govern_careing to scaled [1, 6]\n",
    "# df_resilience[\"resilience_govern_careing\"] = df_resilience[\"resilience_govern_careing\"].replace({1:6, 0:1})\n",
    "\n",
    "\n",
    "df_cantho[\"resilience\"] = None\n",
    "df_cantho[\"resilience\"] = df_cantho[\"resilience_city_protection\"] # only one variable is currently used\n",
    "# df_cantho[\"resilience\"] = df_resilience.sum(axis=1) / len(df_resilience.columns)\n",
    "print(df_cantho[\"resilience\"].value_counts())\n",
    "\n",
    "\n",
    "# ## rather pessimistic resilience variables\n",
    "# df_resilience_leftalone = df_cantho[[\"resilience_left_alone\", \"resilience_more_future_affected\"]]\n",
    "# print(df_resilience_leftalone.columns)\n",
    "\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "# df_cantho.drop(\"perception_too_destructive_floods\", axis=1, inplace=True)\n",
    "df_cantho.drop(\n",
    "    df_cantho.filter(like=\"resilience_\").columns,\n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df_cantho.columns)\n",
    "## 1: disagree, 5: agree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## perception as indicator\n",
    "# ## o\tOrderer rank okay as long its  quantitative\n",
    "\n",
    "# # Reduced /Porrer  -> 1\n",
    "# # Maintained /Same  -> 2\n",
    "# # Increased /Richer -> 3\n",
    "\n",
    "\n",
    "# # ## based on findings from PCA\n",
    "# # df_perception = df[[\"perception_govern_support_past\", \"perception_govern_support_future\"]]\n",
    "# # print(df_perception.columns)\n",
    "\n",
    "# # df[\"perception\"] = None\n",
    "# # df[\"perception\"] = df_perception.sum(axis=1) / len(df_perception.columns)\n",
    "\n",
    "# ## keep only indicator in final df\n",
    "# df.drop(\n",
    "#     df.filter(like=\"perception_\").columns, \n",
    "#     axis=1, inplace=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify monetary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## check for very small registered capital\n",
    "# vars_money = df_cantho.filter(regex=\"_mVND\", axis=1)\n",
    "# try:\n",
    "#     vars_money[vars_money.shp_registered_capital_mVND <=1.0]  # less than 40 euros\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Variables inflation corrected for 2020 (align with HCMC ds)\n",
    "\n",
    "Survey was done 2012 therefore all monetary variables need inflation corrected for the time period from 2012-2020 (reference year), except damage variables need to be inflation corrected from year of flood event (which was 2011) to price level of 2020. \n",
    "\n",
    "\n",
    "**Variables inflation corrected for year when survey was conducted**\n",
    "\n",
    "- building_value_mVND\t- price level for 2012 (year when survey was done)\n",
    "- shp_building_value_mVND\t- price level for 2012\n",
    "- shp_content_value_VND\t- price level for 2012\n",
    "- shp_registered_capital_mVND  - price level for 2012\n",
    "- hh_monthly_income_mVND     - continous [value ranges in mVND], # price level for 2012\n",
    "- shp_avgmonthly_sale_VND   - continous [value ranges in mVND], # price level for 2012 \n",
    "\n",
    "\n",
    "**Variables inflation corrected for flood year**\n",
    "Damage variables ('Target_eloss_VND', 'Target_gloss_VND' -- merged in Target_contentloss_VND) need to be inflation corrected based on flood time which was 2011\n",
    "- 'Target_contentloss_VND'  - price levels based on flood time (2011)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for very small registered capital\n",
    "vars_money = df_cantho.filter(regex=\"_mVND\", axis=1)\n",
    "\n",
    "\n",
    "## covnert all columns with million VND --> VND\n",
    "\n",
    "vars_money = np.where( (vars_money.values != np.nan),\n",
    "            vars_money.values * 1000000, # convert to VND\n",
    "            vars_money.values)\n",
    "\n",
    "## rename columns\n",
    "new_cols = df_cantho.filter(regex=\"_mVND\", axis=1).columns.str.replace(\"_mVND\", \"_VND\")\n",
    "vars_money = pd.DataFrame(vars_money, columns=new_cols)\n",
    "print(vars_money.columns)\n",
    "\n",
    "df_cantho.drop( df_cantho.filter(regex=\"_mVND\", axis=1).columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add remaining monetary variables\n",
    "vars_money = pd.concat(\n",
    "    [vars_money, \n",
    "    df_cantho[df_cantho.filter(regex=\"VND\", axis=1).columns.tolist()]\n",
    "    ], axis=1)\n",
    "vars_money.columns  # sholuld all end with _VND (no _mVND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conversion of VND to euro (or US$)*\n",
    "\n",
    "Based on JRC, p.8 and Paprotny2018, eg.p245\n",
    "The reported damage values have been converted to Euro using the the exchange rate for the year 2012 (mean annual value)\n",
    "\n",
    "*Source:* \n",
    "- www.oanda.com/currency/historical-rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GDP deflator source : https://jp.tradingeconomics.com/vietnam/gdp-deflator\n",
    "gdp_price_index_year_of_flood = 121.26  # 2011\n",
    "gdp_price_index_year_of_survey = 140.91  # 2012\n",
    "gdp_price_index_2020 = 163.58 # price level 2020 based on GDP-deflation\n",
    "\n",
    "## exchange rate \n",
    "exchange_rate = 1 / 27155  # ~ 3.68e-05  dong-> euro in 2020 \n",
    "## (based on https://www.oanda.com/currency-converter/de/?from=VND&to=EUR&amount=1 )\n",
    "\n",
    "## set price index on flood-year - all records reference on the severe flood event in CanTHo in 2011\n",
    "gdp_price_index_year_of_flood = np.full( vars_money.shape[0], gdp_price_index_year_of_flood, dtype=float) # flood cases are all from 2011 flood in CanTHo\n",
    "\n",
    "## set price index on survey-year - 2012\n",
    "gdp_price_index_year_of_survey = np.full( vars_money.shape[0], gdp_price_index_year_of_survey, dtype=float) \n",
    "\n",
    "## when different flood-years exist\n",
    "# gdp_price_index_year = data_ip2[\"flood_year\"].astype(\"Int64\").map(gdp_price_index_year_of_issue)  # series of cpi for each year of flood event\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inflation correction via GDP-deflator*\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{align*}\n",
    "\n",
    "&uninflated_{2020} = losses_y * exchangerate_{2020} \\\\\n",
    "&inflationrate = uninflated_{2020} * pindex_{2020} / pindex_y\\\\\n",
    "\n",
    "\\end{align*}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- losses_y : losses in VND for year y\n",
    "- uninflated_{2020} : uninflated losses in euro for 2020\n",
    "- exchangerate_{2020} : exchang erate for VND to euro in year 2020 \n",
    "- pindex_{2020} : price index from GDP-deflator for 2020 \n",
    "- pindex_y : price index from GDP-deflator in year y\n",
    "\n",
    "Given that inflation is the percentage change in the overall price of an item in an economy, we can use the GDP deflator to calculate the inflation rate since its a measure of the price level.\n",
    "\n",
    "\n",
    "*Further sources* \\\n",
    "*Paprotny 2018*: also used country-level GDP deflators for adjusting nominal to real losses in 2011 prices , p153, p244 \\\n",
    "*Sairam et al. 2020*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GDP-deflator\n",
    "\n",
    "vars_money[\"Target_contentloss_VND_gdp\"] = None\n",
    "\n",
    "##  only direct losses needs inflation correction in respect to flood time\n",
    "for r in range(len(vars_money.Target_contentloss_VND)):\n",
    "\n",
    "    ## exchange rate: convert VND in certain year to € in the same year\n",
    "    uninflated_losses = (vars_money.Target_contentloss_VND[r] * exchange_rate) # get uninflated losses in euros for year 2020\n",
    "    \n",
    "    ## price index from GDP-deflator\n",
    "    vars_money[\"Target_contentloss_VND_gdp\"][r] = round(uninflated_losses * gdp_price_index_2020 / gdp_price_index_year_of_flood[r], 1)\n",
    "\n",
    "\n",
    "\n",
    "# ##  for all other monetary continous vars: need exchange conversion and inflation correction for time period 2012 (year of survey) to 2020 (ref.year)\n",
    "for c in vars_money.drop([\"Target_contentloss_VND_gdp\",\"Target_contentloss_VND\"], axis=1).columns:\n",
    "    vars_money[c] = vars_money[c].apply(pd.to_numeric)\n",
    "    for r in range(len(vars_money[c])):\n",
    "        ## exchange rate: convert VND_2020 to €_2020\n",
    "        uninflated = round((vars_money[c][r] * exchange_rate), 1)#.astype(int) \n",
    "        ## price index from GDP-deflator\n",
    "        vars_money[c][r] = round(uninflated  * gdp_price_index_2020 / gdp_price_index_year_of_survey[r], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## rename columns\n",
    "new_cols = vars_money.filter(regex=\"_VND\", axis=1).columns.str.replace(\"_VND\", \"_euro\")\n",
    "vars_money.columns = new_cols\n",
    "vars_money = vars_money.apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all_input with unified and inflated currencies\n",
    "df_cantho.drop(df_cantho.filter(regex=r\"_mVND|_VND\", axis=1).columns, axis=1, inplace=True) \n",
    "df_cantho = pd.concat([df_cantho, vars_money], axis=1)\n",
    "df_cantho.drop(\"Target_contentloss_euro\", axis=1, inplace=True)\n",
    "df_cantho.rename(columns={\"Target_contentloss_euro_gdp\" : \"Target_contentloss_euro\"}, inplace=True)\n",
    "\n",
    "df_cantho.filter(regex=r\"euro|VND\", axis=1).columns  # should be only euros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[[\"hh_monthly_income_euro\", \"shp_employees\", \"shp_avgmonthly_sale_euro\", \"shp_registered_capital_euro\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets\n",
    "### absolute loss and Target business reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute content loss\n",
    "df_cantho[\"Target_contentloss_euro\"].describe()\n",
    "\n",
    "\n",
    "# Target business reduction\n",
    "df_cantho[\"Target_businessreduction\"].describe()\n",
    "\n",
    "\n",
    "## verify with HCMC\n",
    "print(\"*CanTHo*\\n\", (df_cantho.Target_contentloss_euro).describe(), \"\\n\\n\", \"*HCMC*\\n: median: 0€ , mean:131€, 3.Q:78€, max:2600 €\")\n",
    "## --> abs losses are very similar in both DS\n",
    "## -->  very similar in all keys-statistics  in min-max, median :-) interesting that fraction of zero-loss records is similar high as in HCMC\n",
    "\n",
    "## verify with HCMC\n",
    "print(\"*CanTHo*\\n\", df_cantho.Target_businessreduction.describe(), \"\\n\\n\", \"*HCMC*\\n\", df_hcmc_bred.Target_businessreduction.describe())\n",
    "## --> business reduction differ quite much: between avg < 10% in reduction in HCMC and 40% in Can THo \n",
    "## --> might not good to enrich bred ds with CanTHo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### content value\n",
    "verify cv calculation for HCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target relative closs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv from survey\n",
    "df_cantho[\"Target_relative_contentloss_euro\"] = df_cantho[\"Target_contentloss_euro\"] / df_cantho[\"shp_content_value_euro\"]\n",
    "# set all zero-loss cases to 0\n",
    "df_cantho = df_cantho.apply(pd.to_numeric)\n",
    "df_cantho.Target_relative_contentloss_euro[df_cantho.Target_relative_contentloss_euro.isna()] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rloss > cv\n",
    "print(\"Records with relative content loss > total content value :\", sum(df_cantho.Target_relative_contentloss_euro > 1.0) )\n",
    "\n",
    "## drop these records where rloss > cv\n",
    "df_cantho = df_cantho.loc[~(df_cantho.Target_relative_contentloss_euro >= 1.0), :]\n",
    "\n",
    "\n",
    "# ## drop these records where rloss > cv\n",
    "# df_cantho = df_cantho.loc[~(df_cantho.Target_rcloss >= 0.5), :]\n",
    "\n",
    "df_cantho[[\"Target_contentloss_euro\", \"Target_relative_contentloss_euro\", \"Target_relative_contentloss_euro_self\", \n",
    "           \"shp_content_value_euro\", \"shp_content_value_euro_estimated\", \"Target_businessreduction\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv self** is similar to the cv from questionary, therefore remove later one and the **respective Target version for loss ratio derived from it**, to have common approach of cv estimation in both DS (HCMC, CanTho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho = df_cantho.drop([\"Target_relative_contentloss_euro\"], axis=1) #, \"shp_content_value_euro\"], axis=1)\n",
    "df_cantho.rename(columns={\n",
    "    \"Target_relative_contentloss_euro_self\" : \"Target_relative_contentloss_euro\",\n",
    "    \"shp_content_value_euro\": \"shp_content_value_euro_obs\",  # obs\n",
    "    \"shp_content_value_euro_estimated\" : \"shp_content_value_euro\", # pred\n",
    "    }, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## remove shp sector # due that no expalnation which category revers to which sector\n",
    "# df_cantho.shp_sector.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore variable's key statistics - are they similar to HCMC ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete features with more than 10% missing values\n",
    "print(\"Percentage of missing values per feature [%]\\n\", round(df_cantho.isna().mean().sort_values(ascending=False)[:15]  * 100), 2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microbusinesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Businees which are not microbusinesses (> 10 employees)\", len(df_cantho.loc[df_cantho.shp_employees >= 10, :]))\n",
    "df_cantho = df_cantho.loc[df_cantho.shp_employees < 10, :]\n",
    "df_cantho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation between value of commercial building part and business content value\n",
    "see if content value can be used as proxy for building value (apporach of thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[\"bv_commercial\"] = df_cantho[\"building_value_euro\"] / df_cantho.floors\n",
    "df_cantho[[\"bv_commercial\", \"shp_content_value_euro_obs\"]].corr(method=\"pearson\")\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.lmplot(x=\"building_value_euro\", y=\"shp_content_value_euro_obs\", data=df_cantho, ci=None)\n",
    "# plt.xlim(0,5000)\n",
    "# plt.ylim(0,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation between building area and business content value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho[[\"b_area\", \"shp_content_value_euro_obs\"]].corr(method=\"pearson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single dataset for bred and rloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cantho.rename(columns={\n",
    "#     \"shp_avgmonthly_sale_euro\": \"shp_avgmonthly_sale_euro\",\n",
    "# }, inplace=True)\n",
    "df_cantho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make to same unit as Bred\n",
    "df_cantho[\"Target_relative_contentloss_euro\"] = df_cantho[\"Target_relative_contentloss_euro\"] * 100\n",
    "\n",
    "\n",
    "df_cantho_rloss = df_cantho\n",
    "df_cantho_rloss = df_cantho_rloss.drop(\n",
    "    [\n",
    "   # \"Target_contentloss_euro\",  # keep to calc abs losses\n",
    "    \"Target_businessreduction\",\n",
    "    # \"shp_content_value_euro\",  # keep to estimate abs loss after BN\n",
    "    ]\n",
    "    , axis=1)\n",
    "print(df_cantho_rloss.shape)\n",
    "df_cantho_rloss.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_bred = df_cantho\n",
    "df_cantho_bred = df_cantho_bred.drop(\n",
    "    [\n",
    "    \"Target_contentloss_euro\",\n",
    "    \"Target_relative_contentloss_euro\"\n",
    "    ],\n",
    "    axis=1)\n",
    "\n",
    "print(df_cantho_bred.shape)\n",
    "df_cantho_bred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_rloss.insert(0, \"Target_relative_contentloss_euro\", df_cantho_rloss.pop(\"Target_relative_contentloss_euro\"))\n",
    "df_cantho_bred.insert(0, \"Target_businessreduction\", df_cantho_bred.pop(\"Target_businessreduction\"))\n",
    "\n",
    "df_cantho_rloss.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify estimation of CV by error rate of estimated CV and obs. Cv in Can THo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 40000, 100)\n",
    "\n",
    "plt.hist((df_cantho.shp_content_value_euro_obs), bins, alpha=0.5, label='reported content values' )\n",
    "# plt.hist((df_cantho.shp_content_value_euro_estimated_nocorrection), bins, alpha=0.5, label='estimated not corrected')\n",
    "plt.hist((df_cantho.shp_content_value_euro), bins, alpha=0.5, label='estimated content values')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Verification of the approach for deriving content values (Can Tho)\")\n",
    "#plt.ylim(0, 80)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.savefig( OUTPATH_FIGURES / \"histo_estimation_contentvalues.png\", dpi=300, bbox_inches=\"tight\")  #format='jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_residuals = pd.DataFrame(\n",
    "    {\n",
    "        \"cv_true\": df_cantho[\"shp_content_value_euro\"],\n",
    "        \"cv_pred\": df_cantho[\"shp_content_value_euro_estimated_nocorrection\"],\n",
    "        \"cv_residual_notcorr\": df_cantho[\"shp_content_value_euro_estimated_nocorrection\"] - df_cantho_rloss[\"shp_content_value_euro\"],\n",
    "    }\n",
    ")\n",
    "print(df_cv_residuals.describe())\n",
    "\n",
    "\n",
    "sns.violinplot(data=df_cv_residuals, x=\"cv_residual_notcorr\", split=True)#, inner=\"quart\")#,split=True, gap=.1, inner=\"quart\")#, inner=\"stick\")\n",
    "# plt.xlim(-10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_rloss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_residuals = pd.DataFrame(\n",
    "    {\n",
    "        \"cv_true\": df_cantho_rloss[\"shp_content_value_euro_obs\"],\n",
    "        \"cv_pred\": df_cantho_rloss[\"shp_content_value_euro\"],\n",
    "        \"cv_residual\": df_cantho_rloss[\"shp_content_value_euro\"] - df_cantho_rloss[\"shp_content_value_euro_obs\"],\n",
    "    }\n",
    ")\n",
    "print(df_cv_residuals.describe())\n",
    "\n",
    "sns.violinplot(data=df_cv_residuals, x=\"cv_residual\", split=True)#, inner=\"quart\")#,split=True, gap=.1, inner=\"quart\")#, inner=\"stick\")\n",
    "# plt.semilogx(df_cv_residuals.cv_residual)\n",
    "# semilogx([x], y, [fmt], data=None, **kwargs)\n",
    "# semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
    "# formatter = ticker.FuncFormatter(lambda y, _: f'{y:.3g}')\n",
    "# ax.xaxis.set_major_formatter(formatter)\n",
    "plt.xlabel(\"Residual between estimated and reported content value [in €]\")\n",
    "plt.xlim(-10000, 10000)\n",
    "\n",
    "# around 50% of records have an underestimation of 300€ or overestimation of less than 500€, median is ~60€\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cv_residuals.loc[df_cv_residuals.cv_residual < 2000, : ].shape)\n",
    "\n",
    "df_cv_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major cleaning \n",
    "Drop variables which are not in HCMC ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_rloss = df_cantho_rloss[df_hcmc_rloss.drop([\"geometry\", \"shp_business_limitation\"], axis=1).columns] # cantho has no GPS-information\n",
    "df_cantho_bred = df_cantho_bred[df_hcmc_bred.drop([\"geometry\", \"shp_business_limitation\"], axis=1).columns]\n",
    "\n",
    "print(df_cantho_rloss.columns, \"\\n\", df_cantho_bred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_rloss.describe() # 313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with HCMC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_cols_rloss = list(set.intersection(*(set(df.columns) for df in [df_hcmc_rloss, df_cantho_rloss])))\n",
    "# common_cols_bred = list(set.intersection(*(set(df.columns) for df in [df_hcmc_bred, df_cantho_bred])))\n",
    "\n",
    "# df_rloss_joined = pd.concat([df[common_cols_rloss] for df in [df_hcmc_rloss, df_cantho_rloss]], ignore_index=True)\n",
    "# df_bred_joined = pd.concat([df[common_cols_bred] for df in [df_hcmc_bred, df_cantho_bred]], ignore_index=True)\n",
    "\n",
    "\n",
    "# print(df_rloss_joined.shape)\n",
    "# df_rloss_joined.head(3)   # not containing HCMCs shp_sector, 'resilience', 'resilienceLeftAlone',\n",
    "\n",
    "# print(df_bred_joined.shape)\n",
    "# df_bred_joined.head(3)   # not containing HCMCs shp_sector, 'resilience', 'resilienceLeftAlone',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hcmc_rloss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cantho.Target_contentloss_euro.describe()\n",
    "\n",
    "# ## Test if also removing high abs losses imporves Logistic Reg (HCMC: rm 4 highest abs losses)\n",
    "# print(\"Absolute loss higher than 500€: \", len(df_cantho.Target_contentloss_euro>500.0))\n",
    "# df_cantho = df_cantho.loc[ ~(df_cantho.Target_contentloss_euro>500.0), :]\n",
    "# df_cantho.Target_contentloss_euro.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cantho_bred.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution cantho and hcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 10, 100)\n",
    "\n",
    "plt.hist((df_cantho.Target_relative_contentloss_euro), bins, alpha=0.5, label='cantho rloss')\n",
    "plt.hist((df_hcmc_rloss.Target_relative_contentloss_euro), bins, alpha=0.5, label='hcmc rloss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "## --> with removing 4 highest abs loss in HCMC and similar much in CanTho\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 159/313\n",
    "# 50% cantho\n",
    "\n",
    "191/320\n",
    "# 60% hcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_cantho_rloss\n",
    "df_hcmc_rloss.describe()\n",
    "# df_hcmc_rloss.Target_relative_contentloss_euro.value_counts()\n",
    "# df_cantho_bred.Target_businessreduction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for CanTHo\n",
    "df_cantho_rloss.to_excel(f\"../{s.INPATH_DATA}/input_data_contentloss_cantho.xlsx\", index=False)\n",
    "df_cantho_bred.to_excel(f\"../{s.INPATH_DATA}/input_data_businessreduction_cantho.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rloss_joined.insert(0, \"Target_relative_contentloss_euro\", df_rloss_joined.pop(\"Target_relative_contentloss_euro\"))\n",
    "# df_bred_joined.insert(0, \"Target_businessreduction\", df_bred_joined.pop(\"Target_businessreduction\"))\n",
    "\n",
    "# df_rloss_joined.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save to disk\n",
    "\n",
    "# # combined for HCMC and CanTHo\n",
    "# df_rloss_joined.to_excel(\"../input_survey_data/input_data_contentloss_tueb_cantho.xlsx\", index=False)\n",
    "# df_bred_joined.to_excel(\"../input_survey_data/input_data_businessreduction_tueb_cantho.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Relative content loss dataset\")\n",
    "# print(\"Number of candidate predictors \", df_rloss_joined.shape[1])\n",
    "# print(\"Number of cases \", df_rloss_joined.shape[0])\n",
    "\n",
    "# print(\"\\nBusiness reduction dataset\")\n",
    "# print(\"Number of candidate predictors \", df_bred_joined.shape[1])\n",
    "# print(\"Number of cases \", df_bred_joined.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distirbutions HCMC ~ CanTho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Target_relative_contentloss_euro\", \"Target_businessreduction\"]\n",
    "target = target[0]\n",
    "\n",
    "target_plot = [\"Target_relative_contentloss_euro_categ\", \"Target_businessreduction_categ\"]\n",
    "target_plot = target_plot[0]\n",
    "\n",
    "\n",
    "df_hcmc_p = df_hcmc_bothtargets.drop([\n",
    "  \"geometry\", \n",
    "  \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "  \"shp_registered_capital_euro\", \"shp_sector\",\n",
    "  \"overall_problem_house\", \"shp_business_limitation\"\n",
    "  ], axis=1)\n",
    "df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "\n",
    "## add col for hue plotting\n",
    "df_hcmc_p[\"cityname\"] = \"HCMC\"\n",
    "df_cantho_p[\"cityname\"] = \"Can Tho\"\n",
    "\n",
    "## merge df for plotting\n",
    "df_merged_p = pd.concat([df_hcmc_p, df_cantho_p], axis=0).reset_index(drop=True)\n",
    "  \n",
    "if target ==  \"Target_relative_contentloss_euro\":\n",
    "        df_merged_p = df_merged_p.loc[df_merged_p.Target_relative_contentloss_euro > 0.0, : ]\n",
    "\n",
    "#all_input_p[\"bage_categ\"] = [f'{int(i//10*10)} - {int(i//10*10+10)}' for i in all_input_p[\"bage\"]] # apply modulo operator and multiply by 10\n",
    "df_merged_p[target_plot] = [ i//5*5 for i in df_merged_p[target]]  # apply modulo operator and multiply by 10\n",
    "# print(df_merged_p[target_plot].value_counts())\n",
    "\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "bins = 50\n",
    "hue_colors=(\"teal\",\"firebrick\")\n",
    "alpha=0.2\n",
    "color_dict = {\n",
    "        \"HCMC\": to_rgba(hue_colors[0], alpha), # set transparency for each class independently\n",
    "        \"Can Tho\": to_rgba(hue_colors[1], alpha)\n",
    "    }\n",
    "\n",
    "legend = True\n",
    "p = sns.histplot(\n",
    "        df_merged_p,  \n",
    "        x=target_plot,\n",
    "        hue=\"cityname\", stat=\"count\",\n",
    "        multiple=\"dodge\",\n",
    "        bins=range(0, 101, 5), \n",
    "        palette=color_dict, \n",
    "        )\n",
    "p\n",
    "# set a hatch for HCMC, to distinguish bars for color-blind people\n",
    "for hues, hatch in zip(ax.containers, [\"//\", \"\"]): \n",
    "        for hue in hues:\n",
    "                hue.set_hatch(hatch)\n",
    "\n",
    "# degree rcloss:\n",
    "plt.xlabel(\"Relative content loss without zero-loss\", fontsize=12)\n",
    "plt.ylabel(\"Number of cases\", fontsize=12)\n",
    "plt.legend(loc=\"upper right\", labels=[\"Can Tho\", \"HCMC\"])\n",
    "fig.get_figure().savefig(OUTPATH_FIGURES / \"histo_degree_hcmc_cantho.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Target_relative_contentloss_euro\", \"Target_businessreduction\"]\n",
    "target = target[1]\n",
    "\n",
    "target_plot = [\"Target_relative_contentloss_euro_categ\", \"Target_businessreduction_categ\"]\n",
    "target_plot = target_plot[1]\n",
    "\n",
    "\n",
    "df_hcmc_p = df_hcmc_bothtargets.drop([\n",
    "  \"geometry\", \n",
    "  \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "  \"shp_registered_capital_euro\", \"shp_sector\",\n",
    "  \"overall_problem_house\", \"shp_business_limitation\"\n",
    "  ], axis=1)\n",
    "df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "\n",
    "## add col for hue plotting\n",
    "df_hcmc_p[\"cityname\"] = \"HCMC\"\n",
    "df_cantho_p[\"cityname\"] = \"Can Tho\"\n",
    "\n",
    "## merge df for plotting\n",
    "df_merged_p = pd.concat([df_hcmc_p, df_cantho_p], axis=0).reset_index(drop=True)\n",
    "  \n",
    "if target ==  \"Target_relative_contentloss_euro\":\n",
    "        df_merged_p = df_merged_p.loc[df_merged_p.Target_relative_contentloss_euro > 0.0, : ]\n",
    "\n",
    "#all_input_p[\"bage_categ\"] = [f'{int(i//10*10)} - {int(i//10*10+10)}' for i in all_input_p[\"bage\"]] # apply modulo operator and multiply by 10\n",
    "df_merged_p[target_plot] = [ i//5*5 for i in df_merged_p[target]]  # apply modulo operator and multiply by 10\n",
    "# print(df_merged_p[target_plot].value_counts())\n",
    "\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "bins = 50\n",
    "hue_colors=(\"teal\",\"firebrick\")\n",
    "alpha=0.2\n",
    "color_dict = {\n",
    "        \"HCMC\": to_rgba(hue_colors[0], alpha), # set transparency for each class independently\n",
    "        \"Can Tho\": to_rgba(hue_colors[1], alpha)\n",
    "    }\n",
    "\n",
    "legend = True\n",
    "p = sns.histplot(\n",
    "        df_merged_p,  \n",
    "        x=target_plot,\n",
    "        hue=\"cityname\", stat=\"count\",\n",
    "        multiple=\"dodge\",\n",
    "        bins=range(0, 101, 5), \n",
    "        palette=color_dict, \n",
    "        )\n",
    "p\n",
    "# set a hatch for HCMC, to distinguish bars for color-blind people\n",
    "for hues, hatch in zip(ax.containers, [\"//\", \"\"]): \n",
    "        for hue in hues:\n",
    "                hue.set_hatch(hatch)\n",
    "\n",
    "# rbred:\n",
    "plt.xlabel(\"Relative interruption loss\", fontsize=12)\n",
    "plt.ylabel(\"Number of cases\", fontsize=12)\n",
    "plt.legend(loc=\"upper right\", labels=[\"Can Tho\", \"HCMC\"])\n",
    "fig.get_figure().savefig(OUTPATH_FIGURES / \"histo_rbred_hcmc_cantho.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Target_relative_contentloss_euro\", \"Target_businessreduction\"]\n",
    "target = target[0]\n",
    "\n",
    "\n",
    "\n",
    "df_hcmc_p = df_hcmc_bothtargets.drop([\n",
    "  \"geometry\", \n",
    "  \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "  \"shp_registered_capital_euro\", \"shp_sector\",\n",
    "  \"overall_problem_house\", \"shp_business_limitation\"\n",
    "  ], axis=1)\n",
    "df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "\n",
    "df_hcmc_p.loc[df_hcmc_p.Target_relative_contentloss_euro > 0.0, : ] = 1.0 \n",
    "df_cantho_p.loc[df_cantho_p.Target_relative_contentloss_euro > 0.0, : ] = 1.0 \n",
    "\n",
    "# print(df_hcmc_p[target].describe())\n",
    "# print(df_cantho_p[target].describe())\n",
    "\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plt.hist(\n",
    "    (df_hcmc_p[target], df_cantho_p[target]), 2, \n",
    "    rwidth=.2, label=(\"HCMC\",\"Can Tho\"), \n",
    "    alpha=0.8, color=(\"teal\", \"firebrick\"))\n",
    "# set a hatch for HCMC, to distinguish bars for color-blind people\n",
    "for hues, hatch in zip(ax.containers, [\"\", \"//\"]):\n",
    "    for hue in hues:\n",
    "        hue.set_hatch(hatch)\n",
    "\n",
    "labels = ['', '', '', '', '', '', '', '', '']\n",
    "labels[1] = \"                 zero-loss\"\n",
    "labels[6] = \"                 loss\"\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# plt.xlabel(\"Relative loss due to business interruption (rbred)\")\n",
    "plt.xlabel(\"Chance of content loss\", fontsize=12)\n",
    "plt.ylabel(\"Number of cases\", fontsize=12)\n",
    "plt.legend(loc='upper right', labels=[\"HCMC\",\"Can Tho\"])\n",
    "p\n",
    "\n",
    "fig.get_figure().savefig(OUTPATH_FIGURES / \"histo_chance_hcmc_cantho.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "## prepare HCMC and cantho datasets for plotting, drop unneeded vars incl targets\n",
    "df_hcmc_p =  df_hcmc_bothtargets.drop([\n",
    "  \"Target_relative_contentloss_euro\", \"Target_businessreduction\",\n",
    "  \"Target_contentloss_euro\", \n",
    "    \"geometry\", \n",
    "    \"shp_content_value_euro\",\n",
    "    \"shp_registered_capital_euro\", \"shp_sector\",\n",
    "    \"overall_problem_house\", \"shp_business_limitation\"\n",
    "  ], axis=1)\n",
    "df_hcmc_p\n",
    "df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "\n",
    "## add col for hue plotting\n",
    "df_hcmc_p[\"cityname\"] = \"HCMC\"\n",
    "df_cantho_p[\"cityname\"] = \"Can Tho\"\n",
    "\n",
    "## merge df for plotting\n",
    "df_merged_p = pd.concat([df_hcmc_p, df_cantho_p], axis=0).reset_index(drop=True)\n",
    "df_merged_p\n",
    "  \n",
    "## nice feature labels \n",
    "df_merged_p.rename(columns=s.feature_names_plot, inplace=True) \n",
    "\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "\n",
    "hue_colors=(\"teal\",\"firebrick\")\n",
    "alpha=0.3\n",
    "color_dict = {\n",
    "        \"HCMC\": to_rgba(hue_colors[0], alpha), # set transparency for each class independently\n",
    "        \"Can Tho\": to_rgba(hue_colors[1], alpha)\n",
    "    }\n",
    "numcols = 4\n",
    "bins = 30 #100 # np.linspace(0, 10, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1+len(df_merged_p.columns)//numcols, numcols, figsize=(15, 15), constrained_layout=True)\n",
    "\n",
    "## shorten vlaue ranges of few vars for better plotting\n",
    "df_merged_p[\"inundation duration\"][df_merged_p[\"inundation duration\"] > 120] = np.nan   # 5 cases only in hcmc\n",
    "df_merged_p[\"building area\"][df_merged_p[\"building area\"] > 1000] = np.nan   # 3 cases only in cantho\n",
    "df_merged_p[\"mthly. sales\"][df_merged_p[\"mthly. sales\"] > 5000] = np.nan  #  3 cases onl in cantho\n",
    "\n",
    "\n",
    "## plot histos\n",
    "for col, ax in zip(df_merged_p.columns, axes.flat):\n",
    "    p = sns.histplot(\n",
    "        df_merged_p,  \n",
    "        x=col, stat=\"count\",\n",
    "        hue=\"cityname\", hue_order=[\"HCMC\", \"Can Tho\"],\n",
    "        bins=bins, \n",
    "        # edgecolor=\"black\",\n",
    "        palette=color_dict,\n",
    "        legend=False,\n",
    "        # binwidth=.5,\n",
    "        ax=ax).set_ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    #p.legend(fontsize=10, )  # outside plot: bbox_to_anchor= (1.2,1)\n",
    "    #plt.setp(p.get_legend().get_texts(), fontsize=\"12\")  \n",
    "    #plt.setp(p.get_legend().get_title(), fontsize=\"15\")\n",
    "\n",
    "    # set a hatch for HCMC, to distinguish bars for color-blind people\n",
    "    for hues, hatch in zip(ax.containers, [\"//\", \"\"]): \n",
    "        for hue in hues:\n",
    "            hue.set_hatch(hatch)\n",
    "                      \n",
    "# plt.legend(loc=\"upper right\")  # FIXME legend\n",
    "\n",
    "fig.get_figure().savefig(OUTPATH_FIGURES / \"histo_predictors_hcmc_cantho.png\", dpi=300, bbox_inches=\"tight\")\n",
    "# sns.move_legend(ax, bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left overs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Target relative closs\n",
    "\n",
    "# # #t[[\"abs_closs_VND\", \"shp_content_value_VND\"]] = t[[\"abs_closs_VND\", \"shp_content_value_VND\"]].fillna(0, inplace=True)\n",
    "# # df_cantho[\"Target_rcloss\"] = df_cantho[\"abs_closs_VND\"] / df_cantho[\"shp_content_value_VND\"]\n",
    "# # # set all zero-loss cases to 0\n",
    "# # df_cantho = df_cantho.apply(pd.to_numeric)\n",
    "# # df_cantho.Target_rcloss[df_cantho.Target_rcloss.isna()] = 0.0\n",
    "\n",
    "# ## rloss > cv\n",
    "# print(\"Records with relative content loss exceding the content values for businesses:\", sum(df_cantho.Target_rcloss > 1.0) )\n",
    "# # t.Target_rcloss[t.Target_rcloss > 1.0]  = 1.0\n",
    "# # print(all_input_contentloss[all_input_contentloss.Target_relative_contentloss_euro > 0.99 ])\n",
    "# df_cantho[[\"abs_closs_VND\", \"shp_content_value_VND\", \"Target_rcloss\", \"Target_bred\"]].describe()\n",
    "\n",
    "# #df_cantho[\"Target_rcloss\"] = df_cantho[\"abs_closs_VND\"] / df_cantho[\"shp_content_value_VND\"]\n",
    "# df_cantho[\"Target_rcloss\"] = df_cantho[\"abs_closs_VND\"] / df_cantho[\"shp_content_value_VND_self\"]\n",
    "\n",
    "# # set all zero-loss cases to 0\n",
    "# df_cantho = df_cantho.apply(pd.to_numeric)\n",
    "# df_cantho.Target_rcloss[df_cantho.Target_rcloss.isna()] = 0.0\n",
    "\n",
    "# ## rloss > cv\n",
    "# print(\"Records with relative content loss > total content value :\", sum(df_cantho.Target_rcloss > 1.0) )\n",
    "\n",
    "# ## drop these records where rloss > cv\n",
    "# df_cantho = df_cantho.loc[~(df_cantho.Target_rcloss >= 1.0), :]\n",
    "\n",
    "# df_cantho[[\"abs_closs_VND\", \"shp_content_value_VND_self\", \"Target_rcloss\", \"Target_bred\"]].describe()\n",
    "\n",
    "\n",
    "# # Overestimation of CV for small businesses \n",
    "# # --> businesses with overestimated Cv is charcterized by low number of employees\n",
    "\n",
    "# shps_with_overeestimated_cv = df_cantho.loc[df_cantho[\"shp_content_value_VND\"]  <= 2000000.00, :]  # using 25% of busineeses with smallest CV [1.Qunatile]\n",
    "# shps_with_overeestimated_cv.number_employees.value_counts()  \n",
    "# ## --> most of the shops with small cv are indeed very small businesses\n",
    "\n",
    "# # ## drop these records where rloss > cv\n",
    "# # df_cantho = df_cantho.loc[~(df_cantho.Target_rcloss >= 0.5), :]\n",
    "\n",
    "# # df_cantho[[\"abs_closs_VND\", \"shp_content_value_VND_self\", \"Target_rcloss\", \"Target_bred\"]].describe()\n",
    "\n",
    "# # df_cantho.abs_closs_VND.describe()  # max abs loss is 2300 €\n",
    "\n",
    "# # ## explore cases where rloss > cv\n",
    "# # tt = t.loc[t.Target_rcloss > 1.0, :]\n",
    "# # tt.sort_values(\"abs_closs_VND\", ascending=False)\n",
    "\n",
    "\n",
    "# ## get rloss to similar ratio as for HCMC (rloss=0.3)\n",
    "# df_cantho =  df_cantho.loc[~(df_cantho.Target_rcloss >= 1.0), :] \n",
    "# #df_cantho =  df_cantho.loc[~(df_cantho.Target_rcloss >= .5), :]   # removed ~ 15 records with higher loss ratio than 50%\n",
    "# df_cantho[[\"abs_closs_VND\", \"shp_content_value_VND\", \"Target_rcloss\", \"Target_bred\"]].describe()\n",
    "\n",
    "# ## Can Tho\n",
    "# # Abs Closs: median: 1 €, 3.Quantile: 50 €, mean 74 € , max:  2310 €\n",
    "# # rloss : median: 0.0 , mean: 0.05,  max: 0.48\n",
    "# # CV: mean 3340 €, median: 310 € , max: 392.610 € (no inflation corrected)\n",
    "# ## Bred : mean 40%, median 40%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-jOCI_7hf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
