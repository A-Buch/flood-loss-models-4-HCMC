{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data preprocessing for HCMC survey dataset\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"anna.buch@uni-heidelberg.de\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIM:\n",
    "Clean survey dataset and do explore features\n",
    "- unify commata to decimals\n",
    "- clean columns from typos\n",
    "- define correct datatype to the columns\n",
    "- split records by event type (most recent, most severe) based on flood-related variables and flood times\n",
    "- convert monetary variables to price level of 2020\n",
    "- select columns which could be later needed for model development (feature selection, flood loss models)\n",
    "- derive variable for relative content loss based on business values\n",
    "- create feature spaces for relative content loss and for business interruption loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation \n",
    "For reasons of reusability and comparability the workflow of the data preprocessing was kept similar to previous preprocessings of the same questionnaire. This was achieved by dealing with missing or erroneous values in a similar way likewise erroneous values with commas or points in the beginning were converted to decimal numbers beginning with zero. Furthermore some of the variables such as building age are constructed in the same way. And most important identical events are identifed in the same way and the records are splitted by their flood event types in the same way as done in other studies by the same working group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "UTILS_PATH = os.path.join(os.path.abspath(\"\"), \"../\", \"utils\")\n",
    "sys.path.append(UTILS_PATH)\n",
    "import figures as f\n",
    "import preprocessing as pp\n",
    "import feature_selection as fs\n",
    "import settings as s\n",
    "\n",
    "OUTPATH_FIGURES = Path(\"../figures\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw survey data\n",
    "\n",
    "raw_data = pd.read_excel(\"../input_survey_data/raw_data_vietnamese_geolocations_no_dublicates.xlsx\") # HCMC dataset with vietnamese addresses and improved geolocations and building information\n",
    "raw_data.tail(3)                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix erroneous coordinate pair \n",
    "raw_data.GPS = raw_data.geometry.astype(str).replace(\n",
    "    {\"10.722.546,106.62888\":\"10.722546,106.62888\",\n",
    "    \"10797626106701100\":\"10.797626,106.701100\",  # idx 24\n",
    "    \"10722187106.63\":\"10.722187,106.63\"})  # idx 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coords in readable format for gpd\n",
    "raw_data_geometry = raw_data\n",
    "\n",
    "raw_data[\"geometry\"] = raw_data[\"geometry\"].str.findall(r\"(\\d+\\.\\d+)\").astype(str)\n",
    "geom_col = raw_data[\"geometry\"].str.split(\" \", n=1, expand=True)\n",
    "geom_col\n",
    "\n",
    "raw_data = gpd.GeoDataFrame(raw_data,  \n",
    "           geometry=gpd.points_from_xy(\n",
    "               geom_col[0].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "               geom_col[1].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "    ))\n",
    "print(raw_data.shape)\n",
    "\n",
    "\n",
    "## save shp locations to disk, \n",
    "# extract elevation based on shop locations in datapoints_vars_bui.shp via QGIS due to loading size and process with gdal\n",
    "#print(raw_data_geometry.crs)\n",
    "raw_data = raw_data.set_crs(4326) \n",
    "# raw_data_geometry.geometry.to_file(\"../input_survey_data/DEM_LiDAR/datapoints_vars_bui_tuebingen.shp\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visual check of SMEs locations\n",
    "\n",
    "glimpse = raw_data[~ raw_data.is_empty]   # drop emtpy geoms\n",
    "glimpse.geometry.explore()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map shows spatial distribution of business locations from the HCMC survey dataset.\n",
    "It can be seen that a few shops are probably outside the admininstrative area of HCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename columns\n",
    "\n",
    "All variables based on the most recent event are ending with \"_r\", all variables for the most serious event since 2010 are ending with \"_s\".\n",
    "\"hh\" is used as abbreviation for households, single \"b\" for building \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Targets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data\n",
    "\n",
    "# target var for direct cost on content loss [VND]\n",
    "df.insert(0, \"Target_contentloss_mVND_r\", df.pop(\"P1Q5.6.1\")) \n",
    "df.insert(1, \"Target_contentloss_mVND_s\", df.pop(\"P1Q5.6.2\"))\n",
    "\n",
    "# explanatory var: monthly reduction of business [%] \n",
    "df.insert(2, \"Target_businessreduction_r\", df.pop(\"P1Q5.9.1\"))  \n",
    "df.insert(3, \"Target_businessreduction_s\", df.pop(\"P1Q5.9.2\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*possible relevant columns*\n",
    "Including candidate predictors and features to derive certain predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for candidate predictors: With which reason a certain candidate predictor is selected is written after # (more detailed description in master thesis)\n",
    "\n",
    "col_names = {        \"P1Q1\":\"flood_experience\",  \n",
    "                     \"P1Q2.2.1\":\"flood_time_r\", \n",
    "                     \"P1Q2.2.2\":\"flood_time_s\",\n",
    "                     \"P1Q2.3.1\":\"inundation_duration_h_r\",\n",
    "                     \"P1Q2.3.2\":\"inundation_duration_h_s\",\n",
    "                     \"P1Q2.4.1\":\"water_depth_cm_r\",  \n",
    "                     \"P1Q2.4.2\":\"water_depth_cm_s\",    \n",
    "                     \"P1Q2.5.1\":\"contaminations_r\",    \n",
    "                     \"P1Q2.5.2\":\"contaminations_s\",    \n",
    "                     \"P1Q2.6.1\":\"flowvelocity_r\",     \n",
    "                     \"P1Q2.6.2\":\"flowvelocity_s\",  \n",
    "                     \"P1Q2.7.1\":\"flood_type_r\",      \n",
    "                     \"P1Q2.7.2\":\"flood_type_s\",      \n",
    "                     \"P1Q2.9.1\":\"warning_time_h_r\",   \n",
    "                     \"P1Q2.9.2\":\"warning_time_h_s\",   \n",
    "                     \"P1Q2.10.1\":\"emergency_measures_r\",   \n",
    "                     \"P1Q2.10.2\":\"emergency_measures_s\",   \n",
    "                     #\"P1Q2.11.1.1\":\"overall_problem_house_r\", # binary problem / no problem\n",
    "                     #\"P1Q2.11.2.1\":\"overall_problem_house_s\",  \n",
    "                     \"P1Q2.11.1\":\"overall_problem_house_r\", # multi class\n",
    "                     \"P1Q2.11.2\":\"overall_problem_house_s\",  \n",
    "\n",
    "                     \"P1Q3.2.1\":\"damage_level_floor_r\",\n",
    "                     \"P1Q3.2.2\":\"damage_level_floor_s\",\n",
    "                     \"P1Q3.3.1\":\"damage_level_walls_r\",\n",
    "                     \"P1Q3.3.2\":\"damage_level_walls_s\",\n",
    "                     \"P1Q3.4.1\":\"damage_level_foundation_r\",\n",
    "                     \"P1Q3.4.2\":\"damage_level_foundation_s\",\n",
    "                     \"P1Q3.5.1\":\"damage_level_doors_r\",\n",
    "                     \"P1Q3.5.2\":\"damage_level_doors_s\",\n",
    "                     \"P1Q3.6.1\":\"damage_level_roof_r\",\n",
    "                     \"P1Q3.6.2\":\"damage_level_roof_s\",\n",
    "                     \"P1Q3.7.1\":\"damage_level_basement_r\",\n",
    "                     \"P1Q3.7.2\":\"damage_level_basement_s\",\n",
    "                     \"P1Q3.88.1\":\"damage_level_other_r\",\n",
    "                     \"P1Q3.88.2\":\"damage_level_other_s\",\n",
    "                     \n",
    "                     \"P1Q3.88.1.specify\":\"damage_level_specify_r\",\n",
    "                     \"P1Q3.88.2.specify\":\"damage_level_specify_s\",\n",
    "                     \"P1Q3.8.1\":\"repair_costs_building_VND_r\",\n",
    "                     \"P1Q3.8.2\":\"repair_costs_building_VND_s\",\n",
    "                     \"P1Q3.10.1\":\"reason_why_not_repaired_business_r\", \n",
    "                     \"P1Q3.10.2\":\"reason_why_not_repaired_business_s\",\n",
    "                     \"P1Q3.11.1\":\"repair_costs_building_complete_mVND_r\",\n",
    "                     \"P1Q3.11.2\":\"repair_costs_building_complete_mVND_s\",\n",
    "                     \"P1Q5.2.1\":\"shp_damage_level_furniture_r\",\n",
    "                     \"P1Q5.3.1\":\"shp_damage_level_electronics_r\",\n",
    "                     \"P1Q5.4.1\":\"shp_damage_level_equipment_r\",\n",
    "                     \"P1Q5.5.1\":\"shp_damage_level_products_r\",\n",
    "                     \"P1Q5.88.1\":\"shp_damage_level_others_r\",\n",
    "                     \"P1Q5.2.2\":\"shp_damage_level_furniture_s\",\n",
    "                     \"P1Q5.3.2\":\"shp_damage_level_electronics_s\",\n",
    "                     \"P1Q5.4.2\":\"shp_damage_level_equipment_s\",\n",
    "                     \"P1Q5.5.2\":\"shp_damage_level_products_s\",\n",
    "                     \"P1Q5.88.2\":\"shp_damage_level_others_s\",\n",
    "                     \"P1Q5.7.1\":\"shp_closed_d_r\",\n",
    "                     \"P1Q5.7.2\":\"shp_closed_d_s\",\n",
    "                     \"P1Q5.8.1\":\"shp_business_limitation_r\",\n",
    "                     \"P1Q5.8.2\":\"shp_business_limitation_s\", # needed for modelling monetary loss (abs. loss) of business reduction \n",
    "                     \"P2Q1.1.implement\":\"protect_valuables_impl\",\n",
    "                     \"P2Q1.1.spend\":\"protect_valuables_VND_spnd\",\n",
    "                     \"P2Q1.2.implement\":\"water_barriers_impl\",\n",
    "                     \"P2Q1.2.spend\":\"water_barriers_VND_spnd\",\n",
    "                     \"P2Q1.3.implement\":\"pumping_equipment_impl\",\n",
    "                     \"P2Q1.3.spend\":\"pumping_equipment_VND_spnd\",\n",
    "                     \"P2Q1.4.implement\":\"elevation_building_impl\",\n",
    "                     \"P2Q1.4.spend\":\"elevation_building_VND_spnd\",\n",
    "                     \"P2Q1.5.implement\":\"resistant_material_building_impl\",\n",
    "                     \"P2Q1.5.spend\":\"resistant_material_building_VND_spnd\",\n",
    "                     \"P2Q1.6.implement\":\"electricity_higher_impl\",\n",
    "                     \"P2Q1.6.spend\":\"electricity_higher_VND_spnd\",\n",
    "                     \"P2Q1.7.implement\":\"flood_protections_impl\",\n",
    "                     \"P2Q1.7.spend\":\"flood_protections_VND_spnd\",\n",
    "                     \"P2Q2.1.1\":\"elevation_building_year\",\n",
    "                     \"P2Q2.2\":\"elevation_building_height_cm\",\n",
    "                     \"P2Q3.1.1\":\"insurance_building_VND\",\n",
    "                     \"P2Q3.2.1\":\"insurance_business_VND\",\n",
    "                     \n",
    "                     \"P3Q1.1\":\"resilience_city_protection\",\n",
    "                     \"P3Q1.2\":\"resilience_more_future_affected\",\n",
    "                     \"P3Q1.3\":\"resilience_govern_warnings_helpful\",\n",
    "                     \"P3Q1.4\":\"resilience_govern_careing\",\n",
    "                     \"P3Q1.5\":\"resilience_govern_careing_increases\",\n",
    "                     \"P3Q1.6\":\"resilience_left_alone\",\n",
    "                     \"P3Q1.7\":\"resilience_neighbor_management\",\n",
    "                     \"P3Q2.3\":\"perception_who_responsible4protection\",  \n",
    "                     \"P3Q2.4\":\"perception_govern_support_past\",  \n",
    "                     \"P3Q2.5\":\"perception_govern_support_future\",  \n",
    "                     \"P3Q2.6\":\"perception_private_economy_future\",  \n",
    "  \n",
    "                     \"P4Q1.8\":\"hh_education\",\n",
    "                     \"P4Q1.10\":\"hh_monthly_income_cat\",\n",
    "                     \"P4Q2.1\":\"b_movingin\",\n",
    "                     \"P4Q2.2\":\"b_year\", \n",
    "                     \"P4Q2.3\":\"b_area\",\n",
    "                     \"P4Q2.5\":\"b_value_mVND\",\n",
    "                     \"P4Q2.4\":\"lu_cert\",\n",
    "                     \"P4Q3.1\":\"b_material_foundation\", \n",
    "                     \"P4Q3.2\":\"b_material_floor\",\n",
    "                     \"P4Q3.3\":\"b_material_wall\",\n",
    "                     \"P4Q3.4\":\"b_material_roof\",\n",
    "                     \"P4Q3.5\":\"b_material_doors\",\n",
    "                     r\"P4Q3.6$\":\"elevation_rel2surrounding_cat\", \n",
    "                     \"P4Q4.2.1\":\"ren1\",  \n",
    "                     \"P4Q4.2.2\":\"ren2\", \n",
    "                     \"P4Q4.4.1\":\"b_renovation_reasons_r\",   \n",
    "                     \"P4Q4.4.2\":\"b_renovation_reasons_s\", \n",
    "                     \"P4Q4.5.1\":\"b_renovation_cost_mVND_r\", \n",
    "                     \"P4Q4.5.2\":\"b_renovation_cost_mVND_s\",  \n",
    "\n",
    "                     r\"P5Q1.1$\":\"shp_owner\",\n",
    "                     \"P5Q1.3\":\"shp_sector\", \n",
    "                     \"P5Q1.4\":\"shp_employees\",\n",
    "                     \"P5Q1.5\":\"shp_avgmonthly_sale_cat\",\n",
    "                     \"P5Q1.6.1\":\"shp_finance_investments\",\n",
    "                     \"P5Q1.7\":\"shp_registered_capital_mVND\",\n",
    "                     \"P5Q1.9\":\"shp_suppliers_location\",\n",
    "                     \"P5Q1.10\":\"shp_profits_last5years\",\n",
    "                     \"P5Q1.11\":\"shp_risk_tolerance\",\n",
    "                     \"P5Q1.12\":\"shp_monetary_resources4prevention\",\n",
    "                     \n",
    "}\n",
    "\n",
    "for k, v in col_names.items():\n",
    "    df.rename(columns ={ i: re.sub(k, v, i) for i in  df.columns }, inplace=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #.hh_monthly_income_cat.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flood times \n",
    "\n",
    "If the month and the day of the flood time is missing only the year is selected. If only the day of a flood event is unknown it is set to the first day of the respective month.\n",
    "\n",
    "Pandas datetime fills missing information by 01 which would change the original information e.g if only the year is known, datetime would set it to the first January of the year. Due to this reason and many incomplete dates, the flood times were kept as ojects in two possible formats: as month/day/year or as only the year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean flood time by keeping unknown months, convert unknow days to first day of month\n",
    "# a special treatment was done to keep as much time information as possible, due that many days and months are unknown, simple conversions could cause same dates for possibliy different flood events\n",
    "\n",
    "for i, d in enumerate(df.flood_time_r):\n",
    "    if \"99/99\" in str(d):  # extract only year if day and month are unknown\n",
    "        d = d.split(\"/\")[-1]\n",
    "        df.flood_time_r[i] = pd.to_datetime(d, format=\"mixed\").strftime(\"%Y\")\n",
    "    else:\n",
    "      d = str(d).replace(\"/99\",\"/01\")  # replace unknown days\n",
    "      df.flood_time_r[i] = pd.to_datetime(d, format=\"mixed\").strftime(\"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "for i, d in enumerate(df.flood_time_s):\n",
    "    if \"99/99\" in str(d):  # extract only year if day and month are unknown\n",
    "        d = d.split(\"/\")[-1]\n",
    "        df.flood_time_s[i] = pd.to_datetime(d, format=\"mixed\").strftime(\"%Y\")\n",
    "    else:\n",
    "      d = str(d).replace(\"/99\",\"/01\")  # replace unknown days\n",
    "      df.flood_time_s[i] = pd.to_datetime(d, format=\"mixed\").strftime(\"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "## set flood times with unknown years to NAN, as well as for typos\n",
    "df.flood_time_r = df.flood_time_r.replace(\"01/01/1970\", pd.NaT)\n",
    "df.flood_time_s = df.flood_time_s.replace(\"01/01/1970\", pd.NaT)\n",
    "df.flood_time_s = df.flood_time_s.replace(\"08/20/2023\", pd.NaT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"recent events for which days and months are unknown:\", (df.flood_time_r.str.len()==4).sum())\n",
    "print(\"serious events for which days and months are unknown:\", (df.flood_time_s.str.len()==4).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore spatial distribution of certain variables such as flow veloctiy for recent and strongest events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = df\n",
    "# dff[\"geometry\"] = dff[\"geometry\"].str.findall(r\"(\\d+\\.\\d+)\").astype(str)\n",
    "# geom_col = dff[\"geometry\"].str.split(\" \", n=1, expand=True)\n",
    "# geom_col\n",
    "# dff.geometry\n",
    "# dff = gpd.GeoDataFrame(dff,  \n",
    "#           geometry=gpd.points_from_xy(\n",
    "#                geom_col[0].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "#                geom_col[1].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "#           ))\n",
    "\n",
    "dff = gpd.GeoDataFrame(df)\n",
    "dff = dff.set_crs(4326) \n",
    "\n",
    "dff.tail(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glimpse =  dff[~ dff.is_empty] \n",
    "# #glimpse = glimpse[glimpse.flowvelocity]\n",
    "# glimpse.geometry.explore() \n",
    "\n",
    "dff[\"flowvelocity_r\"] = dff[\"flowvelocity_r\"].astype(float)\n",
    "dff[\"flowvelocity_s\"] = dff[\"flowvelocity_s\"].astype(float)\n",
    "\n",
    "\n",
    "## visual check of SMEs locations\n",
    "import folium\n",
    "\n",
    "glimpse = dff[~ dff.is_empty]   # drop emtpy geoms\n",
    "glimpse_geolocations = dff[~ dff.is_empty]   # drop emtpy geoms\n",
    "glimpse_geolocations = glimpse_geolocations[[\"geometry\", \"flowvelocity_r\", \"flowvelocity_s\"]]\n",
    "m = glimpse.geometry.explore(name=\"survey ds\", color=\"red\", k=10)  \n",
    "m = glimpse_geolocations.explode(ignore_index=True).explore(\n",
    "    m=m, \n",
    "    name=\"flowvelocity per event type\",\n",
    "    column=\"flowvelocity_s\", \n",
    "    popup=True, \n",
    "    cmap=\"viridis\"\n",
    ")    \n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m\n",
    "\n",
    "## 0.1 = calm velocity, 0.5 = turbulent velocity\n",
    "## no spatial relationship between veloctiy strength and event type\n",
    "## --> seems like flow velocity is infleunced by rather other flood sources like overwhelmed drainage systems \n",
    "# than by the strenght of the flood event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage Variables\n",
    "\n",
    "Damage variables comprises hydrological variables, emergency measures,  variables about damage levels  of contents and binary information if a building was damaged or not,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caution: compared to the Rscript further variables are included in \"vars_dam\" such as flood time and damae levels of business contents\n",
    "vars_dam = pd.concat([\n",
    "    df.loc[ :, \"flood_time_r\":\"flood_type_r.3\"],\n",
    "    df.loc[ :, \"warning_time_h_r\":\"overall_problem_house_r.9\"],\n",
    "    df.loc[ :, \"shp_damage_level_furniture_r\":\"shp_damage_level_products_r\"],\n",
    "    df.loc[ :, \"flood_time_s\":\"flood_type_s.3\"],\n",
    "    df.loc[ :, \"warning_time_h_s\":\"overall_problem_house_s.9\"],\n",
    "    df.loc[ :, \"shp_damage_level_furniture_s\":\"shp_damage_level_products_s\"],\n",
    "], axis=1)\n",
    "\n",
    "# drop string columns (all cols ending with 88, 99, specify)\n",
    "vars_dam = pp.drop_object_columns(vars_dam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contamination\n",
    "\n",
    "vars_dam[\"contaminations_r.0\"] = vars_dam[\"contaminations_r.0\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_r.1\"] = vars_dam[\"contaminations_r.1\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_r.2\"] = vars_dam[\"contaminations_r.2\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_r.3\"] = vars_dam[\"contaminations_r.3\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_r.4\"] = vars_dam[\"contaminations_r.4\"].replace(np.nan, 0)\n",
    "\n",
    "vars_dam[\"contaminations_s.0\"] = vars_dam[\"contaminations_s.0\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_s.1\"] = vars_dam[\"contaminations_s.1\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_s.2\"] = vars_dam[\"contaminations_s.2\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_s.3\"] = vars_dam[\"contaminations_s.3\"].replace(np.nan, 0)\n",
    "vars_dam[\"contaminations_s.4\"] = vars_dam[\"contaminations_s.4\"].replace(np.nan, 0)\n",
    "\n",
    "# warning time\n",
    "vars_dam.warning_time_h_r = vars_dam.warning_time_h_r.replace(np.nan, 99)\n",
    "vars_dam.warning_time_h_s = vars_dam.warning_time_h_s.replace(np.nan, 99)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing information about contamination type or warning time are set to 0 or 99 respectively.\n",
    "Stronger contamination increases potentially the damage cost,  i.e. oil or petrol in the water and multiple contaminations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical events\n",
    "\n",
    "Same matrix indicates \n",
    "- 0 for a given damage variable when businesses have different values for most recent & the most serious event or different flood times, \n",
    "- 1 when businesses have same value for recent & serious event and identical flood times\n",
    "\n",
    "In a subsequent step the information about same damage variables in combination with same flood times are used to identify identical events. \n",
    "\n",
    "1 = identical damage vars\\\n",
    "0 = different damage vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## columns used to find identical events\n",
    "# vars_dam.columns  #hydro, damage to building + inventory, flood time, emergency measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over cols -> check if each value in col for recent events is identical with value in the other respective col for serious events\n",
    "\n",
    "col_len = len(vars_dam.columns)//2\n",
    "df_same = pd.DataFrame(index=range(len(vars_dam)), columns=range(col_len)) # init binary df - indicating if certain cols are the same\n",
    "\n",
    "for c in range(col_len):\n",
    "    for r in range(len(vars_dam)):\n",
    "        if vars_dam.iloc[r, c] == vars_dam.iloc[r, c + col_len]:  # identical damage variabels or identical flood times\n",
    "            df_same.iloc[r, c] = 1\n",
    "        if vars_dam.iloc[r, c] != vars_dam.iloc[r, c + col_len]:  # different damage variables or flood times\n",
    "            df_same.iloc[r, c] = 0\n",
    "\n",
    "\n",
    "df_same.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create indidcator in one col, showing if events are equal\n",
    "vars_dam[\"same\"] = pd.Series() # init column indicating if both events are the same\n",
    "\n",
    "for r in range(len(vars_dam)):\n",
    "    vars_dam.same[r] = (df_same.iloc[r, :] == 1).all()  # if all damage variables and flood times are identical than set indicator to True\n",
    "#  vars_dam.same : True= identical events, False= different events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cases with different damage variables and/or different flood times :\", vars_dam.same[vars_dam.same==False].count())\n",
    "print(\"Cases with identical damage variables and identical flood times:\", vars_dam.same[vars_dam.same==True].count())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including further variables ( flood time and damage levels of business contents) to the identification which events are identical reduces the number of identified identical events from 118 to 107.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precautionary measures \n",
    "\n",
    "Variables are transformed to binary values [0,1] \n",
    "- 0 when the measure is implemented before the event \n",
    "-  options 1 or 3 for the serious event and options 2 or 3 for the recent event. \n",
    "- Options 1, 2 or 3 if recent is also the serious event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty columns for precautionary measurment implementations\n",
    "prec_measures_impl_colnames_r = []\n",
    "prec_measures_impl_colnames_s = []\n",
    "\n",
    "for c in  df.filter(regex=\"_impl$\", axis=1).columns:\n",
    "    prec_measures_impl_colnames_r.append(c + \"_r\")\n",
    "    prec_measures_impl_colnames_s.append(c + \"_s\")\n",
    "\n",
    "vars_dam[prec_measures_impl_colnames_r] = 0\n",
    "vars_dam[prec_measures_impl_colnames_s] = 0\n",
    "\n",
    "\n",
    "## add flood experience \n",
    "vars_dam[\"flood_experience\"]  = df.flood_experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precautionary measures   \n",
    "## 1 - before serious, 2 - before recent, 3 - before both, 4 - after both, 5 - did not implement\n",
    "pre_vars = df.filter(regex=\"_impl$\", axis=1)\n",
    "\n",
    "\n",
    "## different events \n",
    "## Recent   \n",
    "vars_dam[prec_measures_impl_colnames_r] = pre_vars.replace( {2:1, 3:1}) # set precautionary measures to 1 before event happend\n",
    "## Serious\n",
    "vars_dam[prec_measures_impl_colnames_s] = pre_vars.replace({1:1, 3:1}) # set precautionary measures to 1 before event happend\n",
    "\n",
    "\n",
    "# ## identical events [Options 1, 2 or 3]\n",
    "idx_identical_events = vars_dam.loc[vars_dam.same==True, :].index\n",
    "vars_dam.loc[idx_identical_events, prec_measures_impl_colnames_r] = pre_vars.loc[idx_identical_events,:].replace( {1:1, 2:1, 3:1}).values \n",
    "vars_dam.loc[idx_identical_events, prec_measures_impl_colnames_s] = pre_vars.loc[idx_identical_events,:].replace( {1:1, 2:1, 3:1}).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dam.loc[idx_identical_events, \"flood_protections_impl_r\"].value_counts()\n",
    "# \tprotect_valuables_impl_s\twater_barriers_impl_s\tpumping_equipment_impl_s\televation_building_impl_s\tresistant_material_building_impl_s\telectricity_higher_impl_s\tflood_protections_impl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars_dam[\"flood_protections_impl_s\"].value_counts() # 2\n",
    "vars_dam[prec_measures_impl_colnames_s].hist(bins=5, figsize=(38, 30))\n",
    "\n",
    "# pumping_equipment_impl_s\televation_building_impl_s\tresistant_material_building_impl_s\telectricity_higher_impl_s\tflood_protections_impl_r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socio-economic variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4Q1_cols_list = [r\"Target_contentloss_*\", r\"Target_businessreduction_*\", \n",
    "                        r\"^hh_monthly_income_cat\",  # = e.g.  \"hh_monthly_income_cat\"\n",
    "                        r\"^shp_*\",  # = e.g \"shp_closed_d*\", r\"shp_sector$\", \"shp_owner\",\n",
    "                    ]\n",
    "\n",
    "pattern_part4Q1_cols = re.compile(\"|\".join(part4Q1_cols_list))\n",
    "vars_soc = df.filter(regex=pattern_part4Q1_cols, axis=1)\n",
    "\n",
    "## keep sectors\n",
    "# shp_sector\n",
    "\n",
    "## data cleaning\n",
    "vars_soc = pp.drop_object_columns(vars_soc)\n",
    "vars_soc = pp.drop_typos(vars_soc).astype(\"Float64\")\n",
    "\n",
    "#For building variables - during serious and recent events - building age; how long has the \n",
    "#householder lived in the location. LU certificate, building cost.\n",
    "vars_bui = df.loc[:,[\"lu_cert\",\"b_value_mVND\"]]\n",
    "vars_bui[\"b_area\"] = pp.drop_typos(df.b_area)\n",
    "vars_bui[\"b_area\"]  = pd.to_numeric(vars_bui[\"b_area\"])\n",
    "flood_year_r = pd.to_datetime(df[\"flood_time_r\"], format=\"mixed\").dt.strftime(\"%Y\")\n",
    "flood_year_s = pd.to_datetime(df[\"flood_time_s\"], format=\"mixed\").dt.strftime(\"%Y\")\n",
    " \n",
    "vars_bui[\"flood_year_r\"] = flood_year_r\n",
    "vars_bui[\"flood_year_s\"] = flood_year_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_soc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df[\"flood_time_r\"], format=\"mixed\")#.dt.strftime(\"%Y\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perception variables\n",
    "The individual perception influences if meaurues for flood protection and prevention are implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3Q1Q2_cols_list = [r\"resilience_*\", r\"^perception_*\"] \n",
    "\n",
    "pattern_part3Q1Q2_cols = re.compile(\"|\".join(part3Q1Q2_cols_list))\n",
    "vars_perception = df.filter(regex=pattern_part3Q1Q2_cols, axis=1)\n",
    "vars_perception = pp.drop_object_columns(vars_perception)\n",
    "\n",
    "\n",
    "## order ranks\n",
    "# 1: Maintained  -> 2\n",
    "# 2: Reduced  -> 1\n",
    "# 3: Increased -> 3\n",
    "vars_perception[\"perception_govern_support_past\"] = vars_perception[\"perception_govern_support_past\"].replace({2:1, 1:2})\n",
    "vars_perception[\"perception_govern_support_future\"] = vars_perception[\"perception_govern_support_future\"].replace({2:1, 1:2})\n",
    "\n",
    "# 1: Richer (e.g. for preparing and repairing your house) --> 3\n",
    "# 2: Poorer --> 1\n",
    "# 3: Same --> 2\n",
    "vars_perception[\"perception_private_economy_future\"] = vars_perception[\"perception_private_economy_future\"].replace({2:1, 3:2, 1:3})\n",
    "\n",
    "## replace 88 = others and typos\n",
    "vars_perception = vars_perception.replace({88: np.nan, 98: np.nan})\n",
    "\n",
    "vars_soc = pd.concat([vars_soc, vars_perception], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.b_movingin = df.b_movingin.replace(99, np.nan)  # P4Q2.1  \n",
    "df.b_year = df.b_year.replace(99, np.nan)  # P4Q2.2: when was the house constructed [year]?\n",
    "# flood_year_r : year of (recent) flood event [year]\n",
    "\n",
    "## extract cases where building construction or moving in of the households was after the flood event\n",
    "vars_bui[\"occ_yrs_r\"] = flood_year_r.astype(\"Int64\") - df.b_movingin.astype(\"Int64\")  \n",
    "vars_bui[\"occ_yrs_s\"] = flood_year_s.astype(\"Int64\") - df.b_movingin.astype(\"Int64\")\n",
    "\n",
    "## NOTE [\"bage\"] is relative: its the b.age at the time of flood event\n",
    "vars_bui[\"bage_r\"] = flood_year_r.astype(\"Int64\") - df.b_year.astype(\"Int64\")  # building age at time of flood event\n",
    "vars_bui[\"bage_s\"] = flood_year_s.astype(\"Int64\") - df.b_year.astype(\"Int64\")\n",
    "\n",
    "\n",
    "print((vars_bui.occ_yrs_r < 0.0).sum(), \"cases in which moving is after recent flood events\")\n",
    "print((vars_bui.occ_yrs_s < 0.0).sum(), \"cases in which moving is after serious flood events\")\n",
    "print((vars_bui.bage_r < 0.0).sum(), \"cases in which building was constructed after the recent flood event\")\n",
    "print((vars_bui.bage_s < 0.0).sum(), \"cases in which building was constructed after the serious flood event\")\n",
    "\n",
    "#For physical damage, if the householder did not live \n",
    "#in this house during the reported flood events, we remove those records from further analysis.\n",
    "#Valid = 1; not valid = 0\n",
    "vars_bui[\"valid_r\"] = 1\n",
    "vars_bui[\"valid_s\"] = 1\n",
    "\n",
    "vars_bui.loc[vars_bui.occ_yrs_r < 0.0, \"valid_r\"] = 0  # set all cases which moved in or where building was constructed after event to zero\n",
    "vars_bui.loc[vars_bui.occ_yrs_s < 0.0, \"valid_s\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add further building related variables likewise about elevation\n",
    "\n",
    "#df.filter(regex=\"elevatio\", axis=1)#.value_counts()\n",
    "vars_bui[[\"elevation_building_height_cm\", \"elevation_rel2surrounding_cat\", \"geometry\"]] = df[[\"elevation_building_height_cm\", \"elevation_rel2surrounding_cat\", \"geometry\"]]\n",
    "vars_bui = gpd.GeoDataFrame(vars_bui)\n",
    "\n",
    "##  elevation height relatve to surrounding \n",
    "## 1 - same level, 2- house floor is lower, 3 - house floor is higher \n",
    "## for case 1 and 2 -> 0, case 3 --> 1\n",
    "vars_bui[\"elevation_rel2surrounding_cat\"] = df.elevation_rel2surrounding_cat.replace({1:0, 2:0, 3:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add number of floors and building type\n",
    "vars_bui[\"address\"]  = df[\"housenumber_street\"] \n",
    "vars_bui[\"floors\"]  = df[\"Nr_Floors\"] \n",
    "vars_bui[\"buildingtype_moon\"]  = df[\"BT_Moon\"] \n",
    "vars_bui[\"buildingtype_moon\"]  = vars_bui[\"buildingtype_moon\"].replace({\"Shop-Traditional\":0, \"Shop-New\":1, \"Doppel/Rowhouse\":2, \"Rudimental\":3, \"Villa-Individual\":4, \"other\":5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocations\n",
    "Add geolocations from which a few are improved by Tuebing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui[\"geometry\"] = df[\"geometry\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building age at time of flooding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui.bage_r[vars_bui.bage_r < 0] = np.nan\n",
    "vars_bui.bage_s[vars_bui.bage_s < 0] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui[[\"bage_r\", \"bage_s\"]].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content and building value\n",
    "\n",
    "Building damage variables\n",
    "\n",
    "Absolute and relative building loss are not calculated due that they seem to be less important for content loss modelling. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui[\"b_value_mVND\"] = df[\"b_value_mVND\"].astype(\"Int64\")  # = P4Q2.5: building value if sell or rebuild completely\n",
    "vars_bui.b_value_mVND[vars_bui.b_value_mVND == 99.0] = np.nan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content value\n",
    "\n",
    "Derive content value (cv) from building value (bv)\n",
    "\n",
    "Registered capital is based on bv + cv (if interviewee is owner)  \n",
    "US HAZUS uses bv = 100% cv (trade, serivces), bv =150% cv (light+heavy industry, food and metall) \n",
    "\n",
    "similar work regarding cv: Chinh 2015, Paprotny2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## due that only a few buildings have now floor info, the missing floor values are filled by the constant of 2 floors per building\n",
    "print(vars_bui.floors.describe() )\n",
    "\n",
    "vars_bui[\"floors\"] = vars_bui[\"floors\"].replace({np.nan:2, 0:2}).astype(int) # replace buildings with missing or unreasonable height\n",
    "#print(vars_bui.floors.describe() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_soc.shp_registered_capital_mVND[vars_soc.shp_registered_capital_mVND == 99.0] = np.nan\n",
    "\n",
    "## total bv an bv of commercial part in mVND in 2020\n",
    "vars_bui[\"shp_building_value_mVND\"] = vars_bui.b_value_mVND / vars_bui.floors\n",
    "vars_bui[\"shp_building_value_mVND\"] = vars_bui[\"shp_building_value_mVND\"].astype(\"Int64\")  # Int64 = keep nan\n",
    "print(\"Number of businesses for which registered capital is less than value for commercial building part:\", (vars_soc.shp_registered_capital_mVND < vars_bui.shp_building_value_mVND).sum())\n",
    "#print(\"No building value information: \", vars_bui.shp_building_value_mVND.isna().sum())\n",
    "#print(\"No registered capital information: \", vars_soc.shp_registered_capital_mVND.isna().sum())\n",
    "\n",
    "# ## quick check number of business owner\n",
    "print(vars_soc.shp_owner.value_counts())  # 1- owner , 2- manager 3 - both, 88 - other\n",
    "\n",
    "\n",
    "vars_bui[\"shp_building_value_mVND\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due that registered capital can not be used to derive content losses, another apporach is utilitized. \n",
    "Business content values are estimated based on the value of the commercial used part of the building (building value / floor number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative approach: \n",
    "\n",
    "## Assigne 1/4 of shp_building_value as = shp_content_value\n",
    "vars_bui[\"shp_content_value_mVND\"] = vars_bui.b_value_mVND  / vars_bui.floors.astype(int) * 0.25 # shop cv as the ration between bv and number of floors\n",
    "vars_bui[\"shp_content_value_mVND\"] = vars_bui[\"shp_content_value_mVND\"].astype(\"Int64\") \n",
    "vars_bui[\"shp_content_value_mVND\"].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Test: is building area a better source to derive content vaule than floor number? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui[\"shp_content_value_mVND\"] = vars_bui.b_value_mVND / vars_bui.floors.astype(int)  * 0.25 # shop cv as the ration between bv and building footprint\n",
    "vars_bui[\"shp_content_value_mVND\"] = vars_bui[\"shp_content_value_mVND\"].astype(\"Int64\") \n",
    "\n",
    "vars_bui[\"shp_content_value_mVND\"] = vars_bui[\"shp_content_value_mVND\"]\n",
    "\n",
    "vars_bui[\"shp_content_value_mVND\"].describe()\n",
    "## idea: capital investment * sqrm  = cv \n",
    "\n",
    "# HCMC: median in euro (no inflation correction): 13 471 €\n",
    "\n",
    "# ## with divide by floor number\n",
    "# # count         216.0\n",
    "# # mean     496.319444\n",
    "# # std      601.118519\n",
    "# # min            30.0\n",
    "# # 25%           200.0\n",
    "# # 50%           350.0\n",
    "# # 75%           500.0\n",
    "# # max          5500.0\n",
    "\n",
    "## HCMC new cv (no inflation corrected): median : 3300 €, mean: 4750 € , 1Quantile_ 1916 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui.buildingtype_moon.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial variable\n",
    "*Currently not used due to the high discrepancy between geolocation from the survey and the actual locations of the shops, derived by the comaprison of HCMC reported geolocations with gmaps and OSM.*\n",
    "\n",
    "<!-- Add information about terrain height above sea level as a predictor. This is a common approach for geograpihc related issues to improve the predictive power of multi-variate models by adding spatial information. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem__lufi_pts = gpd.read_file(\"../input_survey_data/DEM_LiDAR/HCMC_Lidar_2020_DEM_4326_LuFi_points.shp\")  # shop locations including elevation height based on original and interpolated DEM\n",
    "\n",
    "# dem__lufi_pts = gpd.GeoDataFrame(dem__lufi_pts,  \n",
    "#             geometry=gpd.points_from_xy( \n",
    "#                      dem__lufi_pts[\"geometry\"].x,  # lon\n",
    "#                      dem__lufi_pts[\"geometry\"].y, #lat\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# ## crs of 4326 is in degree unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## assigne DEM point locations to shp location of dataframe, silght differences in coordinates due to conversion\n",
    "# vars_bui = gpd.sjoin_nearest(vars_bui, dem__lufi_pts, how=\"left\")\n",
    "# vars_bui = vars_bui.set_geometry(\"geometry\")\n",
    "# vars_bui = vars_bui.drop([\"index_right\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## replace locations with missing elevation by interpolated DEM (all locations with missing elevation height have value 0.0 )\n",
    "# #print(vars_bui[vars_bui.HCMC_Lidar==0.0])\n",
    "# vars_bui.HCMC_Lidar = np.where(vars_bui.HCMC_Lidar==0.0, vars_bui.LuFIDEMInt, vars_bui.HCMC_Lidar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars_bui = vars_bui.rename(columns={\"HCMC_Lidar\":\"elevation_m\"})\n",
    "# vars_bui = vars_bui.drop(\"LuFIDEMInt\", axis =1)\n",
    "# vars_bui.insert(len(vars_bui.columns)-2, \"elevation_m\", vars_bui.pop(\"elevation_m\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline: direct and indiect flood damage [€] for all flood events \n",
    "\n",
    "x axis= years\n",
    "y = reported damage to inventory and calc. loss due to business reduction \n",
    "\n",
    "dont seperate by flood events (count severe and recent ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare severe and most recent floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all vars ending with _r or _s into on df for plotting\n",
    "\n",
    "\n",
    "df_recent = pd.concat(\n",
    "    [vars_bui[\"bage_r\"],\n",
    "        vars_dam[[\"inundation_duration_h_r\", \"water_depth_cm_r\",  \"flowvelocity_r\", \n",
    "                \"contaminations_r.0\", \"contaminations_r.1\",\t\"contaminations_r.2\", \"contaminations_r.3\", \"contaminations_r.4\",\n",
    "                \"emergency_measures_r.1\", \"emergency_measures_r.2\", \"emergency_measures_r.3\", \"emergency_measures_r.4\", \"emergency_measures_r.7\", \"emergency_measures_r.8\" ]],\n",
    "        vars_soc[[\"Target_contentloss_mVND_r\", \"Target_businessreduction_r\"]]], \n",
    "    axis=1)\n",
    "\n",
    "df_severe = pd.concat(\n",
    "    [vars_bui[\"bage_s\"],\n",
    "        vars_dam[[\"inundation_duration_h_s\", \"water_depth_cm_s\",  \"flowvelocity_s\", \n",
    "                \"contaminations_s.0\", \"contaminations_s.1\",\t\"contaminations_s.2\", \"contaminations_s.3\", \"contaminations_s.4\",\n",
    "                \"emergency_measures_s.1\", \"emergency_measures_s.2\", \"emergency_measures_s.3\", \"emergency_measures_s.4\", \"emergency_measures_s.7\", \"emergency_measures_s.8\" ]],\n",
    "        vars_soc[[\"Target_contentloss_mVND_s\", \"Target_businessreduction_s\"]]], \n",
    "    axis=1)\n",
    "\n",
    "\n",
    "## remove endings\n",
    "df_recent.columns = df_recent.columns.str.replace(\"_r\", \"\")\n",
    "df_recent.columns = df_recent.columns.str.replace(\"_1$\", \"\", regex=True)\n",
    "\n",
    "df_severe.columns = df_severe.columns.str.replace(\"_s\", \"\")\n",
    "# sev.columns = sev.columns.str.replace(\"2$\", \"\", regex=True) \n",
    "df_severe.columns = df_severe.columns.str.replace(\"_2$\", \"\", regex=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "\n",
    "# df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "## add col for hue plotting\n",
    "df_severe[\"flood_event\"] = \"severe\"\n",
    "df_recent[\"flood_event\"] = \"recent\"\n",
    "\n",
    "# merge df for plotting\n",
    "df_merged_p = pd.concat([df_severe, df_recent], axis=0).reset_index(drop=True)\n",
    "df_merged_p = df_merged_p.dropna()\n",
    "\n",
    "## fix miss spellings and NAN handling by changing data type\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\",2\", \"2\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\",5\", \"5\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\"1,5\", \"1\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\"\", np.nan)\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p[\"inundation_duration_h\"].astype(float)\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p[\"inundation_duration_h\"].astype(\"int64\")\n",
    "\n",
    "df_merged_p[\"bage\"] = df_merged_p[\"bage\"].astype(\"int64\")\n",
    "df_merged_p[\"Target_contentloss_mVND\"] = df_merged_p[\"Target_contentloss_mVND\"].astype(\"int64\")\n",
    "df_merged_p[\"Target_businessreduction\"] = df_merged_p[\"Target_businessreduction\"].astype(\"int64\")\n",
    "\n",
    "# df_merged_p.info()  # check that datatypes are plotable \n",
    "print(df_merged_p.shape)\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "hue_colors=(\"teal\", \"firebrick\")\n",
    "alpha=0.3\n",
    "color_dict = {\"severe\": to_rgba(hue_colors[0], alpha), # set transparency for each class independently\n",
    "        \"recent\": to_rgba(hue_colors[1], alpha)}\n",
    "numcols = 4\n",
    "legend = True\n",
    "bins = 100 # np.linspace(0, 10, 100)\n",
    "\n",
    "\n",
    "# ## shorten vlaue ranges of few vars for better plotting\n",
    "df_merged_p.Target_contentloss_mVND[df_merged_p[\"Target_contentloss_mVND\"] > 60] = np.nan   # 9 cases (3 cases recent + 6 serious) \n",
    "\n",
    "\n",
    "## plot\n",
    "fig, axes = plt.subplots( 1 + len(df_merged_p.columns)//numcols, numcols, figsize=(15, 15), constrained_layout=True)\n",
    "# bage inundation_duration_h\n",
    "l = ['bage', 'water_depth_cm', 'flowvelocity',\n",
    "       'contaminations.0', 'contaminations.1', 'contaminations.2',\n",
    "       'contaminations.3', 'contaminations.4', 'emergency_measures.1',\n",
    "       'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4',\n",
    "       'emergency_measures.7', 'emergency_measures.8',\n",
    "       'Target_contentloss_mVND', 'Target_businessreduction']\n",
    "## plot histos\n",
    "for col, ax in zip(df_merged_p[l].columns, axes.flat):\n",
    "    # print(col)\n",
    "    p = sns.histplot(\n",
    "        df_merged_p,  \n",
    "        x=col, \n",
    "        hue=\"flood_event\", stat=\"count\",\n",
    "        bins=bins,\n",
    "        multiple='stack',\n",
    "        palette=color_dict, \n",
    "        legend=legend,\n",
    "        # binwidth=.5,\n",
    "        ax=ax).set_ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    # if legend is True:\n",
    "    #     plt.setp(p.get_legend().get_texts(), fontsize=\"12\")  \n",
    "    #     plt.setp(p.get_legend().get_title(), fontsize=\"15\")\n",
    "\n",
    "#fig.get_figure().savefig(\"../figures/histo_severe_recent.png\", dpi=300, bbox_inches=\"tight\")\n",
    "p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select predictors and merge identical events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui = vars_bui.drop([\"occ_yrs_r\", \"occ_yrs_s\", \"valid_r\", \"valid_s\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dam[\"id\"] = range(len(df))\n",
    "\n",
    "vars_dam = pp.drop_typos(vars_dam)\n",
    "\n",
    "## select cases with recent events and non specific cols\n",
    "data_ip1 = pd.concat(\n",
    "  [vars_dam.loc[:, \"flood_time_r\":\"overall_problem_house_r.9\"],  # flood vars, damage vars\n",
    "    vars_dam.loc[:,\"same\":\"flood_protections_impl_r\"],  #  same, precaution measures, \n",
    "    vars_dam.loc[:,[\"flood_experience\"]], \n",
    "    #vars_dam.loc[:, [\"rloss_1\", \"bloss_1\"]], \n",
    " ], axis=1\n",
    ")\n",
    "\n",
    "print(f\"Identical events are: {(vars_dam.same==True).sum()}\")\n",
    "\n",
    "## drop unique endings to obtain identical column names in both dfs\n",
    "data_ip1.columns = data_ip1.columns.str.replace(\"_r\", \"\")\n",
    "data_ip1.columns = data_ip1.columns.str.replace(\"_1\", \"\") \n",
    "\n",
    "\n",
    "\n",
    "## if not identical event select serious events\n",
    "for i in range(len(data_ip1.loc[:,:])):  \n",
    "  if (data_ip1.same[i]==False):   #  events are different, select also serious event\n",
    "    sev = pd.concat(\n",
    "        [vars_dam.loc[i, \"flood_time_s\" : \"overall_problem_house_s.9\"],  # flood-vars, damage-vars\n",
    "         vars_dam.loc[i, [\"same\"]],\n",
    "         vars_dam.loc[i, \"protect_valuables_impl_s\": \"flood_experience\"],  # precaution measures, flood experience\n",
    "         #vars_dam.loc[i, [\"rloss_2\", \"bloss_2\"]]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## convert to 1-row df, drop unique endings to merge both dfs by cols names\n",
    "    sev = pd.DataFrame(sev).T\n",
    "    sev.columns =sev.columns.str.replace(\"_s\", \"\") \n",
    "    sev.columns =sev.columns.str.replace(\"_2\", \"\") \n",
    "    sev[1:] = sev[1:].apply(pd.to_numeric)  # exclude datetime-column: flood_time\n",
    "\n",
    "    ## append events which are not identical to a recent event\n",
    "    data_ip1 = pd.concat([data_ip1, sev], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bui[\"id\"] = range(len(df))\n",
    "\n",
    "## select cases with recent events and non specific cols\n",
    "data_ip2 = pd.concat(\n",
    "  [vars_bui.loc[:, [\"flood_year_r\", \"elevation_building_height_cm\",\t\"elevation_rel2surrounding_cat\"]],\n",
    "    vars_bui.loc[:, [\"bage_r\", \"b_area\", \"geometry\"]],\n",
    "    vars_bui.loc[:, [\"floors\", \"buildingtype_moon\"]], \n",
    "    vars_bui.loc[:, \"shp_building_value_mVND\": \"id\"],\n",
    "    ], axis=1\n",
    ")\n",
    "data_ip2.columns = data_ip2.columns.str.replace(\"1\",\"\")\n",
    "data_ip2.columns = data_ip2.columns.str.replace(\"_r$\", \"\", regex=True) \n",
    "\n",
    "## if not identical event select serious events\n",
    "for i in range(len(data_ip2.loc[:,:])): \n",
    "  if (vars_dam.same[i]==False):\n",
    "    sev = pd.concat(\n",
    "        [vars_bui.loc[i, [\"flood_year_s\", \"elevation_building_height_cm\",\t\"elevation_rel2surrounding_cat\"]],\n",
    "            vars_bui.loc[i, [\"bage_s\", \"b_area\", \"geometry\"]], \n",
    "            vars_bui.loc[i, [\"floors\", \"buildingtype_moon\"]], \n",
    "            vars_bui.loc[i, \"shp_building_value_mVND\": \"id\"]\n",
    "        ]\n",
    "      )\n",
    "    ## convert to 1-row df, drop unique endings to merge both dfs by cols names\n",
    "    sev = pd.DataFrame(sev).T\n",
    "    sev.columns = sev.columns.str.replace(\"_s\", \"\")\n",
    "    sev.columns = sev.columns.str.replace(\"2$\", \"\", regex=True) \n",
    "\n",
    "    ## append events which are not identical to a recent event\n",
    "    data_ip2 = pd.concat([data_ip2, sev], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_soc[\"id\"] = range(len(df))\n",
    "\n",
    "## select cases with recent events and non specific cols\n",
    "data_ip3 = pd.concat(\n",
    "  [vars_soc.loc[:, [\"Target_contentloss_mVND_r\", \"Target_businessreduction_r\", \"shp_business_limitation_r\"]],#[:,:4],\n",
    "    vars_soc.loc[:, \"hh_monthly_income_cat\":] #  monthly_income, firm specific variables eg. ownership, monthly sale.., resilience and perception vars\n",
    "      ], axis=1\n",
    ")\n",
    "\n",
    "#sev_list = []\n",
    "data_ip3.columns = data_ip3.columns.str.replace(\"_r$\", \"\", regex=True) \n",
    "\n",
    "## if not identical event select serious events\n",
    "for i in range(len(data_ip3.loc[:,:])): \n",
    "  if (vars_dam.same[i]==False):  \n",
    "    sev = pd.concat(\n",
    "        [vars_soc.loc[i, [\"Target_contentloss_mVND_s\", \"Target_businessreduction_s\", \"shp_business_limitation_s\"]],\n",
    "            vars_soc.loc[i, \"hh_monthly_income_cat\":]\n",
    "        ], \n",
    "      )\n",
    "    ## convert to 1-row df, drop unique endings to merge both dfs by cols names\n",
    "    sev = pd.DataFrame(sev).T\n",
    "    sev.columns = sev.columns.str.replace(\"_s$\", \"\", regex=True) \n",
    "\n",
    "    #sev_list.append(sev)\n",
    "    ## append not identical events to end of df\n",
    "    data_ip3 = pd.concat([data_ip3, sev], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Business sectors per microbusiness\n",
    "\n",
    "\n",
    "## combine business sectors into two or three main casses to see if they are importnat for rloss quantification\n",
    "vars_soc.shp_sector.value_counts()\n",
    "\n",
    "vars_soc[\"shp_sector\"] = vars_soc[\"shp_sector\"].replace(\n",
    "    {\n",
    "        11.0 : 1,     # 1. Shop/Retailer\n",
    "        12.0 : 1, \n",
    "        13.0 : 1, \n",
    "        14.0 : 1, \n",
    "        15.0 : 1, \n",
    "        16.0 : 1, \n",
    "        17.0 : 1, \n",
    "        11.0 : 1, \n",
    "        11.0 : 1, \n",
    "        11.0 : 1, \n",
    "        21.0 : 2,     # 2. Services \n",
    "        22.0 : 2, \n",
    "        23.0 : 2, \n",
    "        24.0 : 2,\n",
    "        31.0 : 3,     # 3. Production\n",
    "        32.0 : 3, \n",
    "        33.0 : 3, \n",
    "        34.0 : 3, \n",
    "        35.0 : 3, \n",
    "        88.0 : np.nan,\n",
    "    }\n",
    ")\n",
    "vars_soc.shp_sector.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## check how many microbuinesses reported one [False] or two [True] flood events\n",
    "mb_events = pd.concat([vars_dam, vars_soc], axis=1)\n",
    "mb_events.loc[(mb_events.shp_employees < 10.0), :].same.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add geometries for creation of Maps (e.g. FP,TN,TP, FN map for classification and BN modelling)\n",
    "data_ip4 = df.geometry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all important variabels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merged cases\n",
    "all_input = pd.concat(\n",
    "        [data_ip1.loc[:, data_ip1.columns != \"id\"], \n",
    "        data_ip2.loc[:, data_ip2.columns != \"id\"],\n",
    "        data_ip3,\n",
    "        data_ip4\n",
    "        ], axis=1\n",
    ")\n",
    "all_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of cases is larger than from Rscript, due that col \"same\" incoporates flood times and more varibles are used.\n",
    "\n",
    " 2*145 + 1 * 107 = 397 cases [count different events twice, identical events once]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check ratio of  many event reported one or two flood events\n",
    "all_input.loc[~(all_input.shp_employees >= 10.0), :].same.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microbusinesses\n",
    "Use only microbusineess for flood loss estimation and identification of drivers, due that small and medium shops have a too different structure in the processes that caused content losses and business reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(df.loc[df.shp_employees < 10.0, :]))\n",
    "df = df.loc[~(df.shp_employees >= 10.0), :]\n",
    "print(df.shape)\n",
    "df.shp_employees.describe()\n",
    "\n",
    "df.shp_employees.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature reduction and transformation\n",
    "\n",
    "\n",
    "Simple approaches to reduce Multicollinearity:\n",
    "- zero variance features\n",
    "- Remove some of the highly correlated independent variables - Not APPLIED yet in subsequent part of the script - their influence in FS models is tested first and than they will be removed here and a combined indicator for resilience features will be tested on model performance\n",
    "- Linearly combine the independent variables, such as adding them together - APPLIED in subsequent part of the script\n",
    "- Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model.\n",
    "- apply regularized linear models such as elastic net - APPLIED in Feature selection section\n",
    "Some features might be less important for model performance, they are left in feature space so far.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minor cleaning \n",
    "\n",
    "Fix commas and 99, empty columns, spatial drop\n",
    "\n",
    "Empty cells or the interviewee could not answer the question (“I don’t know”) were all set to missing number (not a number: NaN) and for dates to missing date (not a date: NaT). Nulls were left unchanged, ie. for the target variables for which it is assumed that the interviewee had no direct or indirect content losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[\"shp_employees\"] = all_input[\"shp_employees\"].astype(\"Int64\")\n",
    "all_input[\"shp_employees\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix data types\n",
    "all_input.flood_time = pd.to_datetime(all_input.flood_time, format=\"mixed\")\n",
    "\n",
    "\n",
    "## fix obj columns - convert to numeric by keeping nan values\n",
    "all_input_obj = all_input.select_dtypes(include=object)  # all obj cols\n",
    "all_input_obj =all_input_obj.drop(\"geometry\", axis=1)\n",
    "\n",
    "for c in all_input_obj.columns:\n",
    "    all_input[c]  =  pp.drop_typos(all_input_obj[c]).apply(pd.to_numeric)  # convert to int or float,  handles NAN\n",
    "\n",
    "all_input = all_input.replace(99, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance and collinearity of features before Feature Reduction\n",
    "\n",
    "Remove features with zero or near zero variance or merge them into indicators. Do this in respect to the amount of missing data stored in the features. Reduce colinearity between similar features by mergin them into indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop of columns which were only needed to develop the datasets\n",
    "\n",
    "all_input = all_input.drop(\n",
    "    [\n",
    "        \"flood_time\", \"flood_year\", \"id\", \"same\", #\"geometry\",  # keep geoms for results to plot in MAPs\n",
    "    ],  \n",
    "    axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move targets to beginning\n",
    "all_input.insert(0, \"Target_contentloss_mVND\", all_input.pop(\"Target_contentloss_mVND\"))\n",
    "all_input.insert(1, \"Target_businessreduction\", all_input.pop(\"Target_businessreduction\"))\n",
    "\n",
    "# plt.figure(figsize=(18,18))\n",
    "# f.plot_spearman_rank(all_input.drop(\"geometry\", axis=1), min_periods=50, signif=True, psig=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.percentage_of_nan(all_input_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_scaled = MinMaxScaler().fit_transform(\n",
    "    all_input.drop(\n",
    "        [\"Target_contentloss_mVND\", \"Target_businessreduction\", \"geometry\"], \n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "X_scaled = pd.DataFrame(\n",
    "    X_scaled,\n",
    "    columns=all_input.drop([\"Target_contentloss_mVND\", \"Target_businessreduction\", \"geometry\"], axis=1).columns\n",
    ")\n",
    "\n",
    "\n",
    "df_variance = X_scaled.var()\n",
    "df_variance = np.round(df_variance, 5)\n",
    "print(pd.DataFrame({\"variance\":df_variance}).sort_values(\"variance\").head(25))\n",
    "\n",
    "var = []\n",
    "for i in range(0, len(df_variance)):\n",
    "    if df_variance[i] != 0.00000:\n",
    "            var.append(X_scaled.columns[i])\n",
    "df_variance = pd.DataFrame({\"variance\":df_variance}).sort_values(\"variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.percentage_of_nan(all_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop frther columns which were only needed to develop the datasets, are qualitative features or are empty\n",
    "\n",
    "all_input = all_input.drop(\n",
    "    [\n",
    "        \"warning_time_h\", # less importnat for a flood-proven society and contains many missing values \n",
    "        \"shp_suppliers_location.1\", \"shp_suppliers_location.2\", \"shp_suppliers_location.3\",   #  qualitative variables not usable for Bayesian Network\n",
    "        \"shp_suppliers_location.4\", \"shp_suppliers_location.5\", \"shp_suppliers_location.6\",\n",
    "        #\"shp_sector\",  # qualitative variable not usable in NPBN for continous variables\n",
    "        \"shp_owner\",   # qualitative variable not usable in NPBN for continous variables\n",
    "        \"perception_who_responsible4protection.Rank7\",   # empty features + qualitative feature\n",
    "        \"perception_who_responsible4protection.Rank6\", \"perception_who_responsible4protection.Rank5\", # empty features + qualitative feautre\n",
    "        \"perception_who_responsible4protection.Rank4\", \"perception_who_responsible4protection.Rank3\",\n",
    "        \"perception_who_responsible4protection.Rank2\", \"perception_who_responsible4protection.Rank1\",\n",
    "        \"buildingtype_moon\",  #  rather qualitative variables not usable for Bayesian Network\n",
    "        \"shp_finance_investments\", # qualitative variable\n",
    "        \"shp_building_value_mVND\", # not seen as potential predictor, used to derive content value\n",
    "        \"overall_problem_house.1\",  # only inidcating if a problem on the building occured or not -> its infomration is captured by similar features\n",
    "        \"shp_risk_tolerance\",       # remove to remove collinearity, it\"s imfornation is potenitally caputred in applied/not applied precautioin and emergency meausres\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the variance of the features, collineraties and the amount of missing values er feature following decisions are done:\n",
    "\n",
    "**Merge**\n",
    "- merge contamination to one or two indicators (if two indicators: one for light and one for heavy contamination)\n",
    "- merge emergency measures to ratio of implemented / potentially implemented\n",
    "- merge resilience predictors or remove them, due that no monotonic relation could be seen to both target variables\n",
    "- merge perception predictors if possible\n",
    "- combine damages on the building into a single predictor (\"overall_problem_house\"). stronger damage on walls/floor -> can influence flood-losses on shop content . Cracks, mold, stronger damage on walls/floor  --> the length of business reduction.\n",
    "It is assumed that stronger building damages influence content loss and business reduction much more than minor building damages. Minor damages might not at all influence the resposne variables \n",
    "\n",
    "**Remove**\n",
    "\n",
    "--> remove all features for perception_who_repsonsible4protection due that the only remaining Rank is binary and wouldnt be usable in the final Bayesian network version which relys on continous data. Moreover the Ranks are qualitative\n",
    "\n",
    "--> remove warning_time_h : assumed that it is less important predictor for a flood-proven society\n",
    "\n",
    "--> remove shp_supplier_locations due that they are qualitative e.g. the distance to a supplier outside the city can be shorter than to another supplier located inside the city but in a far away district\n",
    "\n",
    "**Keep**\n",
    "\n",
    "Keep following features with low variation due that they are seen in other studies as important predictors. \n",
    "Likewise socio-economic features and predictors describing the shop characteristics seemst to be potential important flood-loss influencing variables \n",
    "- b_area, \n",
    "- shp_registered_capital_mVND, \n",
    "- shp_employees, \n",
    "- inundation_duration_h, \n",
    "- shp_content_value_mVND \n",
    "\n",
    "\n",
    "Keep all resilience related features. they are potentially captured by implemented precaution and emergency measures, but this relation can be tested in the Bayesian network if needed [ \"resilience_city_protection\", \"resilience_left_alone\",\n",
    "      \"resilience_neighbor_management\", \"resilience_more_future_affected\",\"resilience_govern_careing\", \"resilience_govern_careing_increases\",\n",
    "]\n",
    "\n",
    "*Some features might be less important for model performance, they are left in feature space so far*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "f.plot_spearman_rank(all_input.drop(\"geometry\", axis=1), min_periods=50, signif=True, psig=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "- VIF >10 : high correlation with other features\n",
    "- VIF <5: medium correlation\n",
    "\n",
    "**Mulitcollinear features are only explored -> the results from the exploration are implemented in chapter \"Indicators\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first reduce multicollineary in predictor set by normalization\n",
    "## Our goal is to have less multicollineary in our model training\n",
    "## Note normalization already reduces the avarge VIF score remarkably \n",
    "\n",
    "X = all_input.drop([\"Target_contentloss_mVND\", \"Target_businessreduction\", \"geometry\"], axis=1)\n",
    "## reset index due that some samples are removed\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "d = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(d, columns=X.columns)\n",
    "X_scaled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Percentage of missing values per feature [%]\\n\", round(X_scaled.isna().mean().sort_values(ascending=False)[:20]  * 100), 2) \n",
    "pp.percentage_of_nan(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vif = pd.DataFrame()\n",
    "df_vif[\"X_features\"] =  X_scaled.columns\n",
    "\n",
    "## fill features with the most of missing values with median, only for VIF calculation\n",
    "for feature in [\"shp_registered_capital_mVND\", \"elevation_building_height_cm\",  \n",
    "                \"shp_content_value_mVND\", \"resilience_govern_careing_increases\",\n",
    "                \"perception_govern_support_future\", \"perception_govern_support_past\"]:\n",
    "    X_scaled[f\"{feature}\"] = X_scaled[f\"{feature}\"].replace(np.nan, np.nanmedian(X_scaled[f\"{feature}\"]))\n",
    "\n",
    "\n",
    "X_scaled_drop_nan = X_scaled.dropna()\n",
    "X_scaled_drop_nan = X_scaled_drop_nan.reset_index(drop=True)\n",
    "\n",
    "print(X_scaled.shape)\n",
    "print(X_scaled_drop_nan.shape)\n",
    "\n",
    "df_vif = fs.vif_score(X_scaled_drop_nan)\n",
    "print(df_vif.sort_values(\"vif_scores\", ascending=False).head(15)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_drop_nan = X_scaled_drop_nan.drop([\n",
    "      \"shp_profits_last5years\", \"shp_monetary_resources4prevention\",# information can be captured in e.g. precuationary measures, e.g. increased profits --> purchased more precautionry meausres\n",
    "      \"emergency_measures.5\", # saving elderly and kids -> not important for commercial sector\n",
    "      \"emergency_measures.6\", # pumping eq captured in Precautionary measures, \n",
    "      \"emergency_measures.9\", # action/no action\n",
    "      #\"elevation_building_height_cm\",  # keep it due its small multicollinearity, still its information is included in elevation_impl \n",
    "      \"elevation_rel2surrounding_cat\",  # remove due that its information is covered in other elevation features\n",
    "      #\"geometry\",\n",
    "      ], axis=1\n",
    ")\n",
    "X_scaled_drop_nan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average degree of Multicollinearity is quite high with a score of around 10.\n",
    "\n",
    "As already seen in the previous Spearman Rank Order matrics, some features should be merged into a single indicator or alternatively should be removed from feature space if they are not important.\n",
    "- contaminiations into one or two indicators \n",
    "- overall_problem_house features: merge into indicator\n",
    "-  perception features: merge into indicator\n",
    "-  resilience features: merge into indicator\n",
    "\n",
    "\n",
    "Other strong collinear features are left unchanged .e.g precuationary measures (*_impl) in the dataset due that they seem to be important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge the aforementioned feautres and check the VIF scores again*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_drop_nan[\"contaminations\"] = None\n",
    "X_scaled_drop_nan[\"overall_problem_house\"] = None\n",
    "X_scaled_drop_nan[\"emergency_measures\"] = None\n",
    "\n",
    "\n",
    "# rank contamination according to their occurence and damage potential: 0:no contamination, 1:light contamination and 2:heavy contamination\n",
    "## light contamination\n",
    "X_scaled_drop_nan.contaminations = np.where( \n",
    "    (X_scaled_drop_nan[\"contaminations.1\"]==1) | \n",
    "    (X_scaled_drop_nan[\"contaminations.4\"]==1) ,\n",
    "    1, 0\n",
    ") \n",
    "## heavy contamination\n",
    "idx_heavy_contamination = np.where(\n",
    "    (X_scaled_drop_nan[\"contaminations.2\"]==1) |\n",
    "    (X_scaled_drop_nan[\"contaminations.3\"]==1)\n",
    ")\n",
    "X_scaled_drop_nan.contaminations[idx_heavy_contamination[0].tolist()] = 2\n",
    "\n",
    "\n",
    "## overall problem building\n",
    "X_scaled_drop_nan.overall_problem_house = np.where( (X_scaled_drop_nan[\"overall_problem_house.2\"]==1), 1, 0) \n",
    "\n",
    "idx = np.where(X_scaled_drop_nan[\"overall_problem_house.3\"]==1)\n",
    "X_scaled_drop_nan.overall_problem_house[idx[0].tolist()] = 2\n",
    "\n",
    "idx = np.where(X_scaled_drop_nan[\"overall_problem_house.4\"]==1)\n",
    "X_scaled_drop_nan.overall_problem_house[idx[0].tolist()] = 3\n",
    "\n",
    "idx = np.where(X_scaled_drop_nan[\"overall_problem_house.5\"]==1)\n",
    "X_scaled_drop_nan.overall_problem_house[idx[0].tolist()] = 4\n",
    "\n",
    "idx = np.where(X_scaled_drop_nan[\"overall_problem_house.6\"]==1)\n",
    "X_scaled_drop_nan.overall_problem_house[idx[0].tolist()] = 5\n",
    "\n",
    "idx = np.where(\n",
    "    (X_scaled_drop_nan[\"overall_problem_house.7\"]==1) | \n",
    "    (X_scaled_drop_nan[\"overall_problem_house.8\"]==1) |\n",
    "    (X_scaled_drop_nan[\"overall_problem_house.9\"]==1)\n",
    ")\n",
    "X_scaled_drop_nan.overall_problem_house[idx[0].tolist()] = 6\n",
    "\n",
    "\n",
    "## emergency easures - merged binray feautres into on multiclass inidcator \n",
    "pattern = [r\"emergency_measures.?\"] \n",
    "pattern_cols = re.compile(\"|\".join(pattern))\n",
    "df_emergency = X_scaled_drop_nan.filter(regex=pattern_cols, axis=1)\n",
    "## create indicator as ratio between implemented and potentially implemented emergency measures\n",
    "X_scaled_drop_nan[\"emergency_measures\"] = df_emergency.eq(1).sum(axis=1) #/ len(df_emergency.columns)\n",
    "\n",
    "\n",
    "\n",
    "X_scaled_drop_nan = X_scaled_drop_nan.drop([\n",
    "            \"contaminations.0\", \"contaminations.1\", \"contaminations.4\", \n",
    "            \"contaminations.2\", \"contaminations.3\",\n",
    "            \"overall_problem_house.2\", \"overall_problem_house.3\", \"overall_problem_house.4\",\n",
    "            \"overall_problem_house.5\", \"overall_problem_house.6\", \"overall_problem_house.7\", \"overall_problem_house.8\",\n",
    "            \"overall_problem_house.9\",\n",
    "            \"emergency_measures.1\", \"emergency_measures.2\", \"emergency_measures.3\", \"emergency_measures.4\",\n",
    "            \"emergency_measures.7\", \"emergency_measures.8\",\n",
    "            ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vif = fs.vif_score(X_scaled_drop_nan)\n",
    "print(df_vif.sort_values(\"vif_scores\", ascending=False).head(15)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The again applied VIF score shows that merging contaminations, overall problem of the house and emergency measures into single indicators  reduces the average VIF score slightly under 10.\n",
    "\n",
    "--> perception and resilience featues should be combined as inidcators or be removed \n",
    "\n",
    "*Be aware* : By merging feautres into indicators, information can be lost or remarkable characteristics of one feature are less expressive in a combined indicator than compared to nth single features\n",
    "\n",
    "*Be aware* : removing features with high collinearity from the dataset changes the VIF score of the remaining feautres, so only removing feautres is not a good solution to reduce multicollinearity\n",
    "\n",
    "- VIF >10 : high correlation with other features\n",
    "- VIF <5: medium correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "f.plot_spearman_rank(X_scaled_drop_nan, min_periods=50, signif=True, psig=0.05)\n",
    "X_scaled_drop_nan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between the candidate predictors could be removed remarkably, espcially the strongest correlations were removed.\n",
    "Still features describing shop characteristics such as number of employees, registered capital etc, show a higher multicollinearity in this subgroup. Similar subgroups with higher collinearites are the feautres for resilience and perception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop itendtified variables\n",
    "all_input = all_input.drop(\n",
    "    [\n",
    "     \"shp_profits_last5years\", \"shp_monetary_resources4prevention\",# information can be captured in e.g. precuationary measures, e.g. increased profits --> purchased more precautionry meausres\n",
    "      \"emergency_measures.5\", # saving elderly and kids -> not important for commercial sector\n",
    "      \"emergency_measures.6\", # pumping eq captured in Precautionary measures, \n",
    "      \"emergency_measures.9\", # action/no action\n",
    "      #\"elevation_building_height_cm\",  # keep it due its small multicollinearity, still its information is included in elevation_impl \n",
    "      \"elevation_building_height_cm\", \"elevation_rel2surrounding_cat\",  # remove due that its information is covered in other elevation features\n",
    "    ], axis=1\n",
    ")\n",
    "all_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Find out if perception and resilience features could be merged into indicators* \n",
    "Maybe the loadings from the PCA can give some ideas how these features could be merged \\\\\n",
    "Due on variables which are continuous or ordinal scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define set for target and explanatory variables\n",
    "# df_perception_resilience = all_input[list(all_input.filter(regex=f\"^resilience*|^percept*\"))]  # only explanatory vars\n",
    "df_perception_resilience = all_input[list(all_input.filter(regex=f\"_impl|\"))]  # only explanatory vars\n",
    "# df_perception_resilience.columns\n",
    "\n",
    "# df_perception_resilience.resilience_govern_careing_increases = df_perception_resilience.resilience_govern_careing_increases.replace(\n",
    "#     {1:5, 2:4, 3:3, 4:2, 5:1}\n",
    "# )\n",
    "\n",
    "df_perception_resilience = df_perception_resilience.dropna()\n",
    "df_perception_resilience_X = df_perception_resilience # .drop(\"Target_contentloss_mVND\", axis=1)^b\n",
    "# df_perception_resilience_Y = df_perception_resilience[\"Target_contentloss_mVND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perception_resilience_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [r\"^resilience\"] \n",
    "# pattern = [r\"_impl$\"] \n",
    "pattern_cols = re.compile(\"|\".join(pattern))\n",
    "df_precautionary = all_input.filter(regex=pattern_cols, axis=1)\n",
    "\n",
    "df_perception_resilience_X = df_precautionary\n",
    "df_perception_resilience_X.tail(2)\n",
    "\n",
    "# fill NA  or drop it for PCA\n",
    "# df_perception_resilience_X = df_perception_resilience_X.apply(lambda x: x.fillna(x.median()),axis=0)\n",
    "df_perception_resilience_X = df_perception_resilience_X.dropna()\n",
    "df_perception_resilience_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize data \n",
    "X = MinMaxScaler().fit_transform(df_perception_resilience_X)\n",
    "X = pd.DataFrame(X, columns=df_perception_resilience_X.columns)\n",
    "print(X.shape, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Start by using only 2 principal components, then explore 3 principal components and 4 principal components.\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "pca = PCA(n_components=3)\n",
    "# pca = PCA(n_components=4)\n",
    "\n",
    "PC = pca.fit_transform(X)\n",
    "# principalDF = pd.DataFrame(data=PC, columns=[\"pc1\",\"pc2\"])\n",
    "principalDF = pd.DataFrame(data=PC, columns=[\"pc1\",\"pc2\", \"pc3\"])\n",
    "# principalDF = pd.DataFrame(data=PC, columns=[\"pc1\",\"pc2\", \"pc3\", \"pc4\"])\n",
    "df_pca = principalDF\n",
    "# df_pca = pd.concat([principalDF, df_perception_resilience_Y], axis = 1)\n",
    "# df_pca.head(10)\n",
    "#X.columns.tolist()\n",
    "#components #=components[9:21]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explained variance\n",
    "explained variance tells us how much information (variance) can be attributed to each of the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalues, eigenvectors = np.linalg.eig(np.corrcoef(X.T))\n",
    "\n",
    "# tot = sum(eigenvalues)\n",
    "# var_exp = [(i / tot)*100 for i in sorted(eigenvalues, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "# cum_var_exp  --> eigenvaules and cum. proportions are later examined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings with varimax rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings without rotation\n",
    "PC_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "components = df_perception_resilience_X.columns.tolist()  # use X before standarized\n",
    "\n",
    "# loadingdf = pd.DataFrame(PC_loadings, columns=(\"PC1\",\"PC2\"))\n",
    "loadingdf = pd.DataFrame(PC_loadings, columns=(\"PC1\",\"PC2\", \"PC3\"))\n",
    "# loadingdf = pd.DataFrame(PC_loadings, columns=(\"PC1\",\"PC2\", \"PC3\", \"PC4\"))\n",
    "loadingdf[\"variable\"] = components\n",
    "loadingdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Varimax rotation for better interpretation of variables**\n",
    "By the rotation or adjustment the variance shared among the variables should be maximized\n",
    "\n",
    "Code snippets were modified based on: https://scikit-learn.org/stable/auto_examples/decomposition/plot_varimax_fa.html (and https://www.kaggle.com/code/gianinamariapetrascu/pca-varimax-rotation#var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit factor analyzer with principal components and varimax rotation\n",
    "n_factors = 3\n",
    "fa = FactorAnalyzer(rotation=\"varimax\", n_factors=n_factors, method=\"principal\")\n",
    "fa.fit(X)\n",
    "\n",
    "# get the rotated factor pattern\n",
    "loadings2 = pd.DataFrame(\n",
    "    fa.loadings_, \n",
    "    index=df_perception_resilience_X.columns,\n",
    "    columns=[f\"Factor{i+1}\" for i in range(n_factors)]\n",
    ")\n",
    "rotated_factor_pattern = loadings2[abs(loadings2) >= 0.2].dropna(how=\"all\")  # drop loadings with less explanatory power\n",
    "# # rotated_factor_pattern = loadings2[abs(loadings2) >= 0.4].dropna(how=\"all\")  # drop loadings with less explanatory power\n",
    "\n",
    "rotated_factor_pattern.reset_index()\n",
    "#loadings2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.percentage_of_nan(df_precautionary)# resilience_govern_warnings_helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Summarize Varimax rotation\n",
    "*Interpretation of factors*\n",
    "\n",
    "By applying varimax rotation it is easier which features can be combined with each other into one indicator.\n",
    "- One indicator for resilinece could be created by nearly all resilience features except \"resilinece_left_alone\" and eventually \"resilience_govern_warnings_helpful\"\n",
    "- One indicator for \"perception\" may created with \"perception_govern_support_past\" and \"perception_govern_support_future\", but maybe this would be a quite weak variable due that similarities between both variables are weak\n",
    "\n",
    "\n",
    "TODO interprete eigenvectors of first feautres -> am meiste nausagekraft== highest eigenvector?\n",
    "\n",
    "\n",
    "PCA produces Linear combination of the features with the highest variance (first PCA component) and so on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors and eigenvalues\n",
    "Code snippets were taken from: https://www.kaggle.com/code/gianinamariapetrascu/pca-varimax-rotation#var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc Eigenvector and eigenvalues\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(np.corrcoef(X.T))\n",
    "\n",
    "# sort the eigenvalues and eigenvectors in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "\n",
    "# convert to dataframes\n",
    "eigenvalues_df = pd.DataFrame({\"Eigenvalue\": eigenvalues}, index=df_perception_resilience_X.columns)\n",
    "eigenvalues_df[\"Proportion\"] = eigenvalues_df[\"Eigenvalue\"] / eigenvalues_df[\"Eigenvalue\"].sum()\n",
    "eigenvalues_df[\"Cumulative Proportion\"] = eigenvalues_df[\"Proportion\"].cumsum()\n",
    "\n",
    "display(eigenvalues_df.style.format({\"Eigenvalue\": \"{:.4f}\", \"Proportion\": \"{:.4f}\", \"Cumulative Proportion\": \"{:.4f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donwsides of PCA:\n",
    "- PCA only considers linear relationships and PCA as well as SVD-based methods or univariate screening methods (t-test, correlation, etc.), dont account the potential multivariate nature of the data structure (e.g., higher order interaction between variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore correlations before creation of indicators and removal of furhter variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[\"emergency_measures.8\"].value_counts()\n",
    "all_input[\"flood_protections_impl\"].value_counts()\n",
    "\n",
    "# all_input[\"protect_valuables_impl\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE check ob shop who did em1 and em4 also impl prec_1  -> rm \"protect_valuables_impl\"-eher from cheap precaution or from emergency.4 and emerg.1 - wo less PCA or mehr NAN\n",
    "all_input.describe()#[[\"emergency_measures.1\", \"emergency_measures.4\", \"protect_valuables_impl\"]].loc[all_input.protect_valuables_impl <=1,:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.boxplot(all_input.)\n",
    "# # print(all_input.precautionary_measures_lowcost[all_input_p.shp_sector == 1.].describe() )\n",
    "\n",
    "# all_input.boxplot(column=\"precautionary_measures_lowcost\", by=[\"shp_sector\"])\n",
    "# # all_input.boxplot(column=\"shp_avgmonthly_sale_mVND\", by=[\"shp_sector\"])\n",
    "# print(all_input.shp_sector.value_counts())\n",
    "\n",
    "# ## higher sale rates in shops with Electricity equipment, in porduction sites: metal products have higher sales (but sample sizes are for all bus.types  too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the most common business tyopes  across the sectors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[\"shp_sector\"].value_counts()  # 1x shops, 2x: services, 3x: manufactoring\n",
    "\n",
    "## shops\n",
    "# food/beverage [152], other type of shops [116]\n",
    "\n",
    "## services\n",
    "## hair salon [25], repairing [21]\n",
    "\n",
    "## manufactoring\n",
    "## gamrnets [10], paper printing [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How owns buildings with larger floor areas - are these manufacturing firms?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input.loc[all_input[\"b_area\"] > 0, : ][\"shp_sector\"].value_counts()\n",
    "all_input.loc[all_input[\"b_area\"] > 100, : ][\"shp_sector\"].value_counts()\n",
    "\n",
    "## -> no, many busiensses in buildigns with larger floor areas are shops/retailers, only 5 from 26 manufcaturing firms are in larger buidligns (>100sqm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators and transform categorical variables\n",
    "Based on the above findings from VIF and spearman correlation plot certain features from the original feature space will be merged into indicators:\n",
    "\n",
    "The later applied Bayesian Network uses continous variables, therfore categorical variables are transformed to continous scales if possible.\n",
    "Ordered categories can be easily transformed when they are in quantitative values, while binary or qualitative variables can not be used in the Bayesian Network\n",
    "\n",
    "\n",
    "#### Variables to modify:\n",
    "\n",
    "binary vars: --> remove or modify\n",
    "- contaminaitions : modify to indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform categorical (quantitative) vairables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flow velocity: categories 1:calm - 5:torrential\n",
    "## Paprotny 2021:  As for flow velocity, the respondents assessed is based on a qualitative scale, \n",
    "# providing a value from 1 to 6, with half-points possible (Thieken et al. 2005). \n",
    "# A value of 0.1 m/s was assigned to each full step of this qualitative scale. \n",
    "\n",
    "\n",
    "all_input.flowvelocity.describe()\n",
    "\n",
    "all_input.flowvelocity = all_input.flowvelocity.replace(\n",
    "    {\n",
    "        1:0.1, \n",
    "        2:0.2, \n",
    "        3:0.3, \n",
    "        4:0.4, \n",
    "        5:0.5\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flood experience: mean of ranges\n",
    "all_input.flood_experience.describe()\n",
    "\n",
    "all_input.flood_experience = all_input.flood_experience.replace(\n",
    "    {\n",
    "      1:3,     # less than once a year , freq: 1\n",
    "      2:8,     # about once a year, freq: 14\n",
    "      3:16,      # twice a year, freq: 37\n",
    "      4:36,      # 4 times a year, freq: 58\n",
    "      5:76,      # 8 times a year, freq: 98\n",
    "      6:151     # 15 times a year - used range 100-200, freq: 112 (more than 10 times a year, 100 times since 2010)\n",
    "    }\n",
    ")\n",
    "all_input.flood_experience.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avg monthly sale (million VND/month)\n",
    "all_input.shp_avgmonthly_sale_cat.value_counts()\n",
    "\n",
    "all_input[\"shp_avgmonthly_sale_mVND\"] = None\n",
    "all_input[\"shp_avgmonthly_sale_mVND\"] = all_input.shp_avgmonthly_sale_cat.replace(\n",
    "    {\n",
    "      1:2.5,     # < 5m , freq: 126\n",
    "      2:7.5,     # 5m – 10m , freq: 129\n",
    "      3:20,      # 10m – 30m , freq: 54\n",
    "      4:40,      # 30m – 50m , freq: 7\n",
    "      5:75,      # 50m – 100m , freq: 8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avg monthly income\n",
    "#print(all_input.hh_monthly_income_cat.value_counts())\n",
    "\n",
    "all_input[\"hh_monthly_income_mVND\"] = None\n",
    "all_input.hh_monthly_income_mVND = all_input.hh_monthly_income_cat.replace(\n",
    "    {\n",
    "      1:0.5,     # < 1m , freq: 4\n",
    "      2:3,     # 1m – 5m , freq: 68\n",
    "      3:8,     # 5m – 10m , freq: 153\n",
    "      4:15,    # 10m – 20m , freq: 111\n",
    "      5:25,    # 20m – 30m , freq: 26\n",
    "      6:40,    # 30m – 50m , freq: 4\n",
    "      7:65,    # 50m – 80m , freq: 4\n",
    "      8:90,    # 80m – 100m , freq: 3\n",
    "    }\n",
    ")\n",
    "\n",
    "all_input.drop([\"hh_monthly_income_cat\", \"shp_avgmonthly_sale_cat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test if business secotrs are important predictive variablesduring feature extraction**\n",
    "--> No, they have minor or moderate importance for degree of loss and bred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine business sectors into two or three main casses to see if they are importnat for rloss quantification\n",
    "all_input.shp_sector.value_counts()\n",
    "\n",
    "all_input[\"shp_sector\"] = all_input[\"shp_sector\"].replace(\n",
    "    {\n",
    "        11.0 : 1,     # 1. Shop/Retailer\n",
    "        12.0 : 1, \n",
    "        13.0 : 1, \n",
    "        14.0 : 1, \n",
    "        15.0 : 1, \n",
    "        16.0 : 1, \n",
    "        17.0 : 1, \n",
    "        11.0 : 1, \n",
    "        11.0 : 1, \n",
    "        11.0 : 1, \n",
    "        21.0 : 2,     # 2. Services \n",
    "        22.0 : 2, \n",
    "        23.0 : 2, \n",
    "        24.0 : 2,\n",
    "        31.0 : 3,     # 3. Production\n",
    "        32.0 : 3, \n",
    "        33.0 : 3, \n",
    "        34.0 : 3, \n",
    "        35.0 : 3, \n",
    "        88.0 : np.nan,\n",
    "    }\n",
    ")\n",
    "all_input.shp_sector.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicators\n",
    "indictator for severla multiple choice answers e..g emergency and precautionary measures\n",
    "\n",
    "as a ratio between b measures implemented prior to the flood (nI) divided by the nb measures potentailly could have implemented (nP)\n",
    "(https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020WR027649)\n",
    "\n",
    "\\\n",
    "\\\n",
    "Maybe for socio-economic status indicator:\n",
    "\n",
    "according to Plapp 2003, applied by Thieken et al 2005\n",
    "indicator of warning information, indicator of emergency measures, perception of efficiency of private precaution, building quality, building/content value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_input\n",
    "df.columns\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emergency measures as indicator : range: 0:no measures - 6: applied all measures \n",
    "# em.1 = save documents\n",
    "# em.2 = move vehicles\n",
    "# em.3 = move furnuture\n",
    "# em.4 = relocate products (dry-proofing)\n",
    "# em.7 = sand bags or other temporary+small scale protection\n",
    "# em.8 = sealing doors or windows\n",
    "\n",
    "pattern = [r\"emergency_measures.?\"] \n",
    "pattern_cols = re.compile(\"|\".join(pattern))\n",
    "df_emergency = df.filter(regex=pattern_cols, axis=1)\n",
    "\n",
    "## create indicator as ratio between implemented and potentially implemented emergency measures\n",
    "df[\"emergency_measures\"] = None\n",
    "df[\"emergency_measures\"] = df_emergency.eq(1).sum(axis=1) / len(df_emergency.columns)\n",
    "\n",
    "# keep only indicator in final df\n",
    "df.drop(\n",
    "    df_emergency.filter(\n",
    "        regex=r\"^(?:.+\\d)$\"\n",
    "        ).columns, \n",
    "    axis=1, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergency.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## precautionary measures as indicator\n",
    "# ## --> test first as seperate feautres to keep as much information as possible\n",
    "\n",
    "pattern = [r\"_impl$\"] \n",
    "pattern_cols = re.compile(\"|\".join(pattern))\n",
    "df_precautionary = df.filter(regex=pattern_cols, axis=1)\n",
    "\n",
    "## prec 1 : dry-proofing valuables (elelctronics, products)\n",
    "## prec 2 : acquire water barriers\n",
    "## prec 3 : pumping eq.\n",
    "## prec 4 : builidng elelvation\n",
    "## prec 5 : water-resistant builidng material\n",
    "## prec 6 : electrity control system at higher level\n",
    "## prec 7 : install flood protection (sealed basement, doors..)\n",
    "\n",
    "\n",
    "\n",
    "df_precautionary = df_precautionary.replace(\n",
    "    {\n",
    "        1:1,    # impl before \n",
    "        2:0,    # impl after event (but before recent )\n",
    "        4:0,    # impl after \n",
    "        5:0     # not at all  \n",
    "    }\n",
    ")\n",
    "# ## devide into expensive and low cost precautionary measures due that expensive meausres seems to be better in flood-loss-reduction\n",
    "## df_precautionary_expensive: comprises all meausres for which the building has to be fundamentally changes (eg. during major renovation) \n",
    "df_precautionary_expensive = df_precautionary[[\"elevation_building_impl\", \"resistant_material_building_impl\"]]#, \"flood_protections_impl\"]]\n",
    "df_precautionary_low = df_precautionary[[\"protect_valuables_impl\", \"water_barriers_impl\", \"electricity_higher_impl\", \"pumping_equipment_impl\"]]\n",
    "# NOTE \"protect_valuables_impl\" use dry-proofing (need acquisition before flood) vs emegency measure: save valuables shortly before flood\n",
    "# NOTE \"flood_protections_impl\" - drop due that partly incoporated in emergency meausres (EM: 8)\n",
    "# add water_barriers_impl + electrity_higher_impl to cheap meausres (due that building has not be fundamentally be changed)\n",
    "# NOTE prec. waterbariers (permanently installed)  ~ em: temporay like sand bags (not permantently installed)\n",
    "\n",
    "# create indicator as ratio between implemented and potentially implemented measures (in total 7 measures exist)\n",
    "# range: 0.0: no measure wa implemented before the flood event - 1.0: all potential measures were implemented before the reported flood\n",
    "# --> splitting in low and expensive precaution measures reduces the importance of the feautres for the models remarkable\n",
    "df[\"precautionary_measures_lowcost\"] = None\n",
    "df[\"precautionary_measures_lowcost\"] = df_precautionary_low.sum(axis=1) / len(df_precautionary_low.columns)\n",
    "df[\"precautionary_measures_expensive\"] = None\n",
    "df[\"precautionary_measures_expensive\"] = df_precautionary_expensive.sum(axis=1) / len(df_precautionary_expensive.columns)\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df.drop(\n",
    "    df_precautionary.columns, \n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resilience as indicator:\n",
    "\n",
    "## 1 : strong disagree - 5 : strong agree\n",
    "## currently would leave at it is and see if its included in final Feature space\n",
    "## TEST: as merged indcator and as seperate features\n",
    "## o\tOrderer rank okay as long its  quantitative\n",
    "\n",
    "## select based on findings from PCA\n",
    "df_resilience = df[[\"resilience_city_protection\", \"resilience_govern_careing\",\"resilience_neighbor_management\", \"resilience_govern_warnings_helpful\"]]\n",
    "print(df_resilience.columns)\n",
    "\n",
    "df[\"resilience\"] = None\n",
    "df[\"resilience\"] = df_resilience.sum(axis=1) / len(df_resilience.columns)\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df.drop(\n",
    "    df.filter(like=\"resilience_\").columns,\n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df.columns)\n",
    "# ## 1: disagree, 5: agree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Target_contentloss_mVND\", \"Target_businessreduction\", \"resilience\"]].corr(method=\"spearman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop perception variables\n",
    "df.drop(\n",
    "    df.filter(like=\"perception_\").columns, \n",
    "    axis=1, inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contaminations\n",
    "## rank contamination according to their occurence and damage potential: 0:no contamination, 1:light contamination and 2:heavy contamination\n",
    "\n",
    "df[\"contaminations\"] = None\n",
    "df_contaminations = df.filter(like=\"contaminations\", axis=1)\n",
    "\n",
    "\n",
    "## light contamination\n",
    "df.contaminations = np.where( \n",
    "    (df_contaminations[\"contaminations.1\"]==1) | \n",
    "    (df_contaminations[\"contaminations.4\"]==1) ,\n",
    "    1, 0\n",
    ") \n",
    "## heavy contamination\n",
    "idx_heavy_contamination = np.where(\n",
    "    (df_contaminations[\"contaminations.2\"]==1) |\n",
    "    (df_contaminations[\"contaminations.3\"]==1)\n",
    ")\n",
    "df.contaminations[idx_heavy_contamination[0].tolist()] = 2\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df.drop(\n",
    "    df_contaminations.filter(like=\"contaminations.\").columns, \n",
    "    axis=1, inplace=True\n",
    ")\n",
    "#print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## overall_problem_house\n",
    "\n",
    "# 0 if no damage occured, (# 1 if building was damaged )\n",
    "# 2.\tmoisture penetration of walls and ceilings              freq: 207\n",
    "# 3.\tmold and contamination;                                 freq: 175\n",
    "# 4.\tsmall cracks in walls, doors, windows                   freq: 86\n",
    "# 5.\tlarger cracks in walls, doors, windows                  freq: 31\n",
    "# 6.\tsubsidence of the building                              freq: 76\n",
    "# 7.\treplacement of building components necessary            freq: 19\n",
    "# 8.\tcollapse of load-bearing walls, replacement necessary   freq: 6\n",
    "# 9.\tcollapse of large building parts, demolition necessary  freq: 3\n",
    "\n",
    "# --> test overall_problem_house as multiclass \n",
    "df[\"overall_problem_house\"] = None\n",
    "\n",
    "pattern = [r\"overall_problem_house.*(?<!1)$\"] # get all columns but not overall_problem_house_r.1: no problem/ problem \n",
    "pattern_cols = re.compile(\"|\".join(pattern))\n",
    "df_problem_house = df.filter(regex=pattern_cols, axis=1)\n",
    "\n",
    "## create indicator\n",
    "df.overall_problem_house = np.where( (df_problem_house[\"overall_problem_house.2\"]==1), 1, 0) \n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house.3\"]==1)\n",
    "df.overall_problem_house[idx[0].tolist()] = 2\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house.4\"]==1)\n",
    "df.overall_problem_house[idx[0].tolist()] = 3\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house.5\"]==1)\n",
    "df.overall_problem_house[idx[0].tolist()] = 4\n",
    "\n",
    "idx = np.where(df_problem_house[\"overall_problem_house.6\"]==1)\n",
    "df.overall_problem_house[idx[0].tolist()] = 5\n",
    "\n",
    "idx = np.where(\n",
    "    (df_problem_house[\"overall_problem_house.7\"]==1) | \n",
    "    (df_problem_house[\"overall_problem_house.8\"]==1) |\n",
    "    (df_problem_house[\"overall_problem_house.9\"]==1)\n",
    ")\n",
    "df.overall_problem_house[idx[0].tolist()] = 6\n",
    "\n",
    "\n",
    "\n",
    "## keep only indicator in final df\n",
    "df.drop(\n",
    "    df_problem_house.filter(like=\"overall_problem_house.\").columns, \n",
    "    axis=1, inplace=True\n",
    ")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"precautionary_measures_lowcost\", \"precautionary_measures_expensive\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "all_input = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.boxplot(column=\"precautionary_measures_lowcost\", by=\"shp_sector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO boxplot cheap prec grouped by seco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[\"shp_sector\"].value_counts()  # 1x shops, 2x: services, 3x: manufactoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variance of features after Feature Reduction\n",
    "Remove features with zero or near zero variance or merge them into indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop of further variables whose information is captured in other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## keep only indicator in final df\n",
    "# all_input.drop(\n",
    "#     [\"overall_problem_house\", \"shp_registered_capital_mVND\"],\n",
    "#     axis=1, inplace=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler().fit_transform(\n",
    "    all_input.drop(\n",
    "        [\"Target_contentloss_mVND\", \"Target_businessreduction\", \"geometry\"], \n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "X_scaled = pd.DataFrame(\n",
    "    X_scaled,\n",
    "    columns=all_input.drop([\"Target_contentloss_mVND\", \"Target_businessreduction\", \"geometry\"], axis=1).columns\n",
    ")\n",
    "\n",
    "\n",
    "df_variance = X_scaled.var()\n",
    "df_variance = np.round(df_variance, 5)\n",
    "#print(pd.DataFrame({\"variance\":df_variance}).sort_values(\"variance\").head(25))\n",
    "\n",
    "var = []\n",
    "for i in range(0, len(df_variance)):\n",
    "    if df_variance[i] != 0.00000:\n",
    "            var.append(X_scaled.columns[i])\n",
    "print(var)\n",
    "#df_variance = pd.DataFrame({\"variance\":df_variance}).sort_values(\"variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,13))\n",
    "f.plot_spearman_rank(all_input.drop(\"geometry\", axis=1), min_periods=50, signif=True, psig=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.hh_monthly_income_mVND.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vif = fs.vif_score(all_input.drop(\"geometry\", axis=1).dropna())\n",
    "print(df_vif.sort_values(\"vif_scores\", ascending=False)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify monetary values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*list of all monetary vars*\n",
    "\n",
    "- b_value_mVND\t- price level for 2020 (year when survey was done)\n",
    "- shp_building_value_mVND\t- price level for 2020\n",
    "- shp_content_value_mVND\t- price level for 2020\n",
    "- Target_contentloss_mVND\t- price levels based on flood time\n",
    "- shp_registered_capital_mVND  - price level for 2020\n",
    "- hh_monthly_income_mVND     - continous [value ranges in mVND], # price level for 2020\n",
    "- shp_avgmonthly_sale_mVND   - continous [value ranges in mVND], # price level for 2020\n",
    "\n",
    "All other vars are inflation corrected based on flood time or when survey was done \n",
    "cpi_2020 = 168.8    ,  2020 = year when the survey was done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for very small registered capital\n",
    "vars_money = all_input.filter(regex=\"_mVND\", axis=1)\n",
    "\n",
    "\n",
    "## covnert all columns with million VND --> VND\n",
    "\n",
    "vars_money = np.where( (vars_money.values != np.nan),\n",
    "            vars_money.values * 1000000, # convert to VND\n",
    "            vars_money.values)\n",
    "\n",
    "## rename columns\n",
    "new_cols = all_input.filter(regex=\"_mVND\", axis=1).columns.str.replace(\"_mVND\", \"_VND\")\n",
    "vars_money = pd.DataFrame(vars_money, columns=new_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conversion of VND to euro (or US$)*\n",
    "\n",
    "Based on JRC, p.8 and Paprotny2018, eg.p245\n",
    "The reported damage values have been converted to Euro using the the exchange rate for the year 2020 (mean annual value)\n",
    "\n",
    "*Source:* \n",
    "- www.oanda.com/currency/historical-rates\n",
    "-  www.ecb.europa.eu/stats/exchange/eurofxref/html/eurofxref-graph-idr.en.html\n",
    "- https://jp.tradingeconomics.com/vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_price_index_year_of_issue = {\n",
    "    2011: 121.26,\n",
    "    2012: 134.51,\n",
    "    2013: 140.91,\n",
    "    2014: 146.07,\n",
    "    2015: 145.80,\n",
    "    2016: 147.41,\n",
    "    2017: 153.44,\n",
    "    2018: 158.65,\n",
    "    2019: 161.49,\n",
    "    2020: 163.58\n",
    "}\n",
    "# GDP deflator source : https://jp.tradingeconomics.com/vietnam/gdp-deflator\n",
    "\n",
    "\n",
    "gdp_price_index_2020 = 163.58 \n",
    "gdp_price_index_year = data_ip2[\"flood_year\"].astype(\"Int64\").map(gdp_price_index_year_of_issue)  # series of cpi for each year of flood event\n",
    "\n",
    "exchange_rate = 1 / 27155  # ~ 3.68e-05  dong-> euro in 2020 \n",
    "## (based on https://www.oanda.com/currency-converter/de/?from=VND&to=EUR&amount=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inflation correction via GDP-deflator*\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{align*}\n",
    "\n",
    "&uninflated_{2020} = losses_y * exchangerate_{2020} \\\\\n",
    "&inflationrate = uninflated_{2020} * pindex_{2020} / pindex_y\\\\\n",
    "\n",
    "\\end{align*}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- losses_y : losses in VND for year y\n",
    "- uninflated_{2020} : uninflated losses in euro for 2020\n",
    "- exchangerate_{2020} : exchang erate for VND to euro in year 20202 \n",
    "- pindex_{2020} : price index from GDP-deflator for 2020 \n",
    "- pindex_y : price index from GDP-deflator in year y\n",
    "\n",
    "Given that inflation is the percentage change in the overall price of an item in an economy, we can use the GDP deflator to calculate the inflation rate since its a measure of the price level.\n",
    "\n",
    "\n",
    "*Further sources*\n",
    "Paprotny 2018: also used country-level GDP deflators for adjusting nomnal to real losses in 2011 prices , p153, p244\n",
    "Sairam et al. 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GDP-deflator\n",
    "\n",
    "vars_money[\"Target_contentloss_VND_gdp\"] = None\n",
    "\n",
    "##  only direct losses needs inflation correction in respect to flood time\n",
    "for r in range(len(vars_money.Target_contentloss_VND)):\n",
    "    ## exchange rate: convert VND in certain year to € in the same year\n",
    "    uninflated_losses = (vars_money.Target_contentloss_VND[r] * exchange_rate) # get uninflated losses in euros for year 2020\n",
    "    ## price index from GDP-deflator\n",
    "    vars_money[\"Target_contentloss_VND_gdp\"][r] = round(uninflated_losses * gdp_price_index_2020 / gdp_price_index_year[r], 1)\n",
    "\n",
    "\n",
    "\n",
    "# ##  for all other monetary continous vars: only need exchange conversion\n",
    "for c in vars_money.drop([\"Target_contentloss_VND_gdp\",\"Target_contentloss_VND\"], axis=1).columns:\n",
    "    vars_money[c] = vars_money[c].apply(pd.to_numeric)\n",
    "    for r in range(len(vars_money[c])):\n",
    "        ## convert VND_2020 to €_2020\n",
    "        vars_money[c][r] = round((vars_money[c][r] * exchange_rate), 1)#.astype(int)\n",
    "\n",
    "\n",
    "## rename columns\n",
    "new_cols = vars_money.filter(regex=\"_VND\", axis=1).columns.str.replace(\"_VND\", \"_euro\")\n",
    "vars_money.columns = new_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_money.Target_contentloss_euro_gdp = vars_money.Target_contentloss_euro_gdp.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all_input with unified and inflated currencies\n",
    "all_input.drop(all_input.filter(regex=\"_mVND\", axis=1).columns, axis=1, inplace=True) \n",
    "all_input = pd.concat([all_input, vars_money], axis=1)\n",
    "all_input.drop(\"Target_contentloss_euro\", axis=1, inplace=True)\n",
    "all_input.rename(columns={\"Target_contentloss_euro_gdp\" : \"Target_contentloss_euro\"}, inplace=True)\n",
    "\n",
    "all_input.filter(regex=\"euro\", axis=1).columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if inundation duration of 600 hours is a spellign mistake or reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.loc[all_input.inundation_duration_h > 300, :]  ## 600h == ~13 days of inundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.Target_contentloss_euro.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.Target_businessreduction.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Content loss\n",
    "Content loss dataset: create new Target for relative content loss\n",
    "Derive relative content loss as a ratio between the absoulte loss and the estimated content value of each business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[[\"Target_contentloss_euro\", \"shp_content_value_euro\"]].corr(method=\"spearman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.plot.scatter(\"shp_content_value_euro\", \"Target_contentloss_euro\")\n",
    "plt.xlim(0, 10000)\n",
    "plt.ylim(0, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.Target_contentloss_euro.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[all_input.shp_employees> 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[\"Target_contentloss_euro\"] = all_input[\"Target_contentloss_euro\"].apply(pd.to_numeric)\n",
    "# all_input = all_input.loc[ ~(all_input.Target_contentloss_euro > 10000), :]\n",
    "all_input.Target_contentloss_euro.describe()\n",
    "# --> 4 cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc fraction of content loss on total content value inside business\n",
    "\n",
    "all_input[[\"Target_contentloss_euro\", \"shp_content_value_euro\"]] = all_input[[\"Target_contentloss_euro\", \"shp_content_value_euro\"]].apply(pd.to_numeric)\n",
    "all_input[\"Target_relative_contentloss_euro\"] =  all_input[\"Target_contentloss_euro\"] / all_input[\"shp_content_value_euro\"]\n",
    "\n",
    "# make rcloss range comparable with Bred range\n",
    "all_input[\"Target_relative_contentloss_euro\"] = all_input[\"Target_relative_contentloss_euro\"] * 100  \n",
    "\n",
    "print(all_input[\"Target_relative_contentloss_euro\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[[\"Target_contentloss_euro\", \"Target_relative_contentloss_euro\", \"shp_content_value_euro\", \"Target_businessreduction\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases for which content loss exceeds estimated inventory value\n",
    "Set relative losses which excede content value to 1, indicating relativ losses as high as the entire content value of the business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_input.loc[(all_input.Target_contentloss_euro > 10000), :]),  \"buinesses with absolute loss higher than 10.000€\")\n",
    "print(\"For all these cases high content damages occured and the losses exceed the estimated value of the inventory\")\n",
    "\n",
    "\n",
    "# all_input.loc[(all_input.Target_contentloss_euro > 10000), :] # four shops with valueable euqipemnts\n",
    "all_input.Target_relative_contentloss_euro[all_input.Target_relative_contentloss_euro > 100.0] # 7 shops wehre closs > content value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Records with relative content loss exceeding the content values for businesses:\", sum(all_input.Target_relative_contentloss_euro >100.0) )\n",
    "\n",
    "all_input.Target_relative_contentloss_euro[all_input.Target_relative_contentloss_euro > 100.0]  = np.nan #1.0\n",
    "all_input.Target_relative_contentloss_euro.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare CanTho and HCMC key statistics\n",
    "verify if CanTho and HCMC have similar socio-economic condtions, ahve similar flood-conditions etc\n",
    "Verify or improve CV calcalculatoin for HCMC, by CV values from Can THo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input[\"Target_contentloss_euro\"] = all_input[\"Target_contentloss_euro\"].apply(pd.to_numeric)\n",
    "all_input[[\"Target_contentloss_euro\", \"shp_content_value_euro\", \"Target_relative_contentloss_euro\", \"Target_businessreduction\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.water_depth_cm.describe()  \n",
    "## water depth HCMC\n",
    "# medina: 0.30m , mean: 0.33m,  max 1.5m \n",
    "\n",
    "## water depth CanTho\n",
    "# median: 0.20cm, mean: 0.25m, max: 0.80m\n",
    "\n",
    "## --> similar water depth in HCMC and Can THo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## socio economic status - similar between HCMC and Cantho ?\n",
    "all_input[[\"hh_monthly_income_euro\", \"shp_employees\", \"shp_avgmonthly_sale_euro\"]].describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent[\"flood_event\"].value_counts()\n",
    "\n",
    "df_merged_p[df_merged_p[\"Target_contentloss_mVND\"] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "\n",
    "# df_cantho_p = df_cantho[df_hcmc_p.columns]\n",
    "\n",
    "## add col for hue plotting\n",
    "df_severe[\"flood_event\"] = \"severe\"\n",
    "df_recent[\"flood_event\"] = \"recent\"\n",
    "\n",
    "# merge df for plotting\n",
    "df_merged_p = pd.concat([df_severe, df_recent], axis=0).reset_index(drop=True)\n",
    "df_merged_p = df_merged_p.dropna()\n",
    "\n",
    "## fix miss spellings and NAN handling by changing data type\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\",2\", \"2\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\",5\", \"5\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\"1,5\", \"1\")\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p.inundation_duration_h.replace(\"\", np.nan)\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p[\"inundation_duration_h\"].astype(float)\n",
    "df_merged_p[\"inundation_duration_h\"] = df_merged_p[\"inundation_duration_h\"].astype(\"int64\")\n",
    "\n",
    "df_merged_p[\"bage\"] = df_merged_p[\"bage\"].astype(\"int64\")\n",
    "df_merged_p[\"Target_contentloss_mVND\"] = df_merged_p[\"Target_contentloss_mVND\"].astype(\"int64\")\n",
    "df_merged_p[\"Target_businessreduction\"] = df_merged_p[\"Target_businessreduction\"].astype(\"int64\")\n",
    "\n",
    "# df_merged_p.info()  # check that datatypes are plotable \n",
    "print(df_merged_p.shape)\n",
    "\n",
    "## style settings\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "hue_colors=(\"teal\", \"firebrick\")\n",
    "alpha=0.3\n",
    "color_dict = {\"severe\": to_rgba(hue_colors[0], alpha), # set transparency for each class independently\n",
    "        \"recent\": to_rgba(hue_colors[1], alpha)}\n",
    "numcols = 4\n",
    "legend = True\n",
    "bins = 100 # np.linspace(0, 10, 100)\n",
    "\n",
    "\n",
    "# ## shorten vlaue ranges of few vars for better plotting\n",
    "df_merged_p.Target_contentloss_mVND[df_merged_p[\"Target_contentloss_mVND\"] > 60] = np.nan   # 9 cases (3 cases recent + 6 serious) \n",
    "\n",
    "\n",
    "## plot\n",
    "fig, axes = plt.subplots( 1 + len(df_merged_p.columns)//numcols, numcols, figsize=(15, 15), constrained_layout=True)\n",
    "# bage inundation_duration_h\n",
    "l = ['bage', 'water_depth_cm', 'flowvelocity',\n",
    "       'contaminations.0', 'contaminations.1', 'contaminations.2',\n",
    "       'contaminations.3', 'contaminations.4', 'emergency_measures.1',\n",
    "       'emergency_measures.2', 'emergency_measures.3', 'emergency_measures.4',\n",
    "       'emergency_measures.7', 'emergency_measures.8',\n",
    "       'Target_contentloss_mVND', 'Target_businessreduction']\n",
    "## plot histos\n",
    "for col, ax in zip(df_merged_p[l].columns, axes.flat):\n",
    "    # print(col)\n",
    "    p = sns.histplot(\n",
    "        df_merged_p,  \n",
    "        x=col, \n",
    "        hue=\"flood_event\", stat=\"count\",\n",
    "        bins=bins,\n",
    "        multiple='stack',\n",
    "        palette=color_dict, \n",
    "        legend=legend,\n",
    "        # binwidth=.5,\n",
    "        ax=ax).set_ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    # if legend is True:\n",
    "    #     plt.setp(p.get_legend().get_texts(), fontsize=\"12\")  \n",
    "    #     plt.setp(p.get_legend().get_title(), fontsize=\"15\")\n",
    "\n",
    "#fig.get_figure().savefig(\"../figures/histo_severe_recent.png\", dpi=300, bbox_inches=\"tight\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of cases is larger than from Rscript, due that col \"same\" incoporates flood times and more varibles are used.\n",
    "\n",
    " 2*145 + 1 * 107 = 397 cases [count different events twice, identical events once]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore correlations \n",
    "Visualize correlations between candidate predictors and targets incl rloss, abs loss, business reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bage ~ rcloss\n",
    "## -> builidings built before 80s have much lower closses than newer buis built in late 80s-2010\n",
    "\n",
    "\n",
    "# column = \"Target_businessreduction\"\n",
    "column = \"Target_relative_contentloss_euro\"\n",
    "group = \"bage_categ\"\n",
    "# scatterpoints = \"shp_sector\"\n",
    "scatterpoints = \"bage_categ\"\n",
    "\n",
    "\n",
    "all_input_p = all_input#.copy()\n",
    "all_input_p = all_input_p.loc[ ~all_input_p[\"bage\"].isna(), :]\n",
    "all_input_p = all_input_p.loc[all_input_p[column] > 0.0, :]\n",
    "\n",
    "\n",
    "## group businesses by the decade of their construction\n",
    "all_input_p[\"bage_categ\"] = [f\"{int(i//10*10)} - {int(i//10*10+10)}\" for i in all_input_p[\"bage\"]] # apply modulo operator and multiply by 10\n",
    "# all_input_p[\"bage_categ\"] = [ i//10*10+1 for i in all_input_p[\"bage\"]]  # apply modulo operator and multiply by 10\n",
    "\n",
    "grouped = all_input_p.groupby(group)\n",
    "categories = np.unique(all_input_p[scatterpoints])\n",
    "colors = np.linspace(0, 1, len(categories))\n",
    "colordict = dict(zip(categories, colors))  \n",
    "all_input_p[scatterpoints] = all_input_p[scatterpoints].apply(lambda x: colordict[x])\n",
    "\n",
    "names, vals, xs, colrs = [], [] ,[], []\n",
    "\n",
    "for i, (name, subdf) in enumerate(grouped):\n",
    "    names.append(name)\n",
    "    vals.append(subdf[column].tolist())\n",
    "    xs.append(np.random.normal(i+1, 0.04, subdf.shape[0]))\n",
    "    colrs.append(subdf[scatterpoints].tolist())\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6),  constrained_layout=True)\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\" : False})# {\"grid.linestyle\": \"\"})\n",
    "\n",
    "# p = ax.violinplot(\n",
    "#     vals, #labels=names,\n",
    "#     showextrema=False,\n",
    "#     showmedians=True,\n",
    "#     vert=False,)\n",
    "\n",
    "p = ax.boxplot(\n",
    "    vals, labels=names,\n",
    "    # showfliers=True,\n",
    "    showfliers=True,\n",
    "    patch_artist=True,\n",
    "    # showmeans=True,\n",
    "    # showmedians=False,\n",
    "    vert=False,\n",
    "    flierprops={\"marker\":\"x\", \"markersize\":5},\n",
    "    boxprops={\"facecolor\":\"steelblue\", \"alpha\":0.4},\n",
    "    whiskerprops={\"color\":\"k\", \"alpha\":0.4},\n",
    "    capprops={\"color\":\"k\", \"alpha\":0.4},\n",
    "    )\n",
    "for patch, color in zip(p[\"boxes\"], colors):\n",
    "        patch.set_facecolor(\"steelblue\",)\n",
    "        patch.set_edgecolor(\"black\")\n",
    "\n",
    "\n",
    "for x, val, colr in zip(xs, vals, colrs):\n",
    "    print(len(colr))\n",
    "\n",
    "\n",
    "# plt.rcParams.update({\"font.size\": 15})\n",
    "plt.rc(\"axes\", titlesize=12)     # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=12)    # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=12)    # fontsize of the tick labels\n",
    "\n",
    "# plt.xlabel(f\"Rbred [%]\")\n",
    "plt.xlabel(f\"Degree of rcloss [%]\")\n",
    "plt.ylabel(f\"Building age at the time of flooding [years]\")\n",
    "plt.title(\"Builidng age in relation to degree of rcloss\")\n",
    "# plt.title(\"Builidng age in relation to degree of business reudction\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(OUTPATH_FIGURES / \"boxplot_bage_degree.png\", dpi=300, bbox_inches=\"tight\") \n",
    "# plt.savefig(OUTPATH_FIGURES / \"boxplot_bage_degree_of_bred.png\", dpi=300, bbox_inches=\"tight\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Employee ~ target ~ sector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DONE which shops have highest sale rates, are these services or specialized businesses ?\n",
    "# all_input.boxplot(column=\"shp_avgmonthly_sale_euro\", by=[\"shp_sector\"])\n",
    "\n",
    "# # 1: shop/retialer [289], 2: service [65], 3: production [n=25]\n",
    "# all_input.shp_sector.value_counts()\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "all_input_p = all_input#.copy()\n",
    "\n",
    "## select only shops with content damages - only those show this concrete pattern (high sale rates - less precaution) using all cases (inl zeroloss : no pattern visible) \n",
    "# all_input_p = all_input_p.loc[all_input_p[\"Target_contentloss_euro\"] > 0.0, :]\n",
    "all_input_p = all_input_p.loc[all_input_p[\"Target_relative_contentloss_euro\"] > 0.0, :]\n",
    "# all_input_p = all_input_p.loc[all_input_p[\"Target_businessreduction\"] > 0.0, :]\n",
    "\n",
    "all_input_p = all_input_p.loc[ ~all_input_p[\"shp_sector\"].isna(), :]\n",
    "\n",
    "print(all_input_p.shp_sector.value_counts())\n",
    "\n",
    "# def plot_boxplot_scatterplot(x_column=group, y_column=column, scatterpoints=):\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "group = \"shp_employees\"\n",
    "# group = \"shp_sector\"\n",
    "group = \"shp_avgmonthly_sale_euro\"\n",
    "# column = \"precautionary_measures_expensive\"\n",
    "column = \"precautionary_measures_lowcost\"\n",
    "# column = \"Target_businessreduction\"\n",
    "# column = \"Target_relative_contentloss_euro\"\n",
    "# column = \"Target_contentloss_euro\"\n",
    "# column = \"shp_content_value_euro\"\n",
    "# column = \"emergency_measures\"\n",
    "scatterpoints=\"shp_sector\"\n",
    "\n",
    "grouped = all_input_p.groupby(group)\n",
    "categories = np.unique(all_input_p[scatterpoints])\n",
    "colors = np.linspace(0, 1, len(categories))\n",
    "colordict = dict(zip(categories, colors))  \n",
    "all_input_p[scatterpoints] = all_input_p[scatterpoints].apply(lambda x: colordict[x])\n",
    "\n",
    "\n",
    "names, vals, xs, colrs = [], [] ,[], []\n",
    "\n",
    "for i, (name, subdf) in enumerate(grouped):\n",
    "    names.append(name)\n",
    "    vals.append(subdf[column].tolist())\n",
    "    xs.append(np.random.normal(i+1, 0.04, subdf.shape[0]))\n",
    "    colrs.append(subdf[scatterpoints].tolist())\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "p = ax.boxplot(\n",
    "    vals, labels=names,\n",
    "    # width=.5,\n",
    "    showfliers=False,\n",
    "    patch_artist=True,\n",
    "    #boxprops={\"facecolor\":(1,0,0,.2), \"edgecolor\":\"k\"}\n",
    "    boxprops={\"facecolor\":\"steelblue\", \"alpha\":0.4},\n",
    "    whiskerprops={\"color\":\"k\", \"alpha\":0.4},\n",
    "    capprops={\"color\":\"k\", \"alpha\":0.4},\n",
    "    )\n",
    "for patch, color in zip(p[\"boxes\"], colors):\n",
    "        # patch.set_facecolor(\"steelblue\",)\n",
    "        patch.set_edgecolor(\"black\")\n",
    "\n",
    "ngroup = len(vals)\n",
    "# clevels = np.linspace(0., 1., ngroup)\n",
    "\n",
    "for x, val, colr in zip(xs, vals, colrs):\n",
    "    print(len(colr))\n",
    "    sns.scatterplot(\n",
    "        x=x, y=val, \n",
    "        hue=colr, edgecolors=colr,\n",
    "        marker=\"o\", s=20, lw=0,\n",
    "        legend=False, palette=[\"green\", \"blue\", \"red\"]\n",
    "    )\n",
    "plt.xlabel(f\"{group}\")\n",
    "# plt.ylim(0, 10000)\n",
    "# plt.xlabel(\"monhtly sale rates [€]\")\n",
    "plt.ylabel(f\"{column}\")\n",
    "plt.title(\"Number of employees in relation to business reduction and sector type\")\n",
    "\n",
    "# plt.ylabel(\"fraction of implemented non-structural measures\")\n",
    "#plt.ylabel(\"fraction of implemented emergency measures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check ratio of non struc between the 3 sectors for the lowest sales group\n",
    "all_input_p = all_input#.copy()\n",
    "all_input_p = all_input_p.loc[all_input_p[group] < 300.0, :]  # n = 126 businesses\n",
    "\n",
    "all_input_p = all_input_p.loc[all_input_p[\"Target_relative_contentloss_euro\"] > 0.0, :]\n",
    "# print(\"on avg. implemented non-struc. meausres for  the 3 sectors in the group of the lowest sales:\\n\" , \n",
    "all_input_p.groupby(\"shp_sector\")[\"precautionary_measures_lowcost\"].mean(), \"number: \", all_input_p.groupby(\"shp_sector\")[\"precautionary_measures_lowcost\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = all_input_p.groupby(\"shp_employees\")[\"shp_sector\"].apply(list)\n",
    "# tt = list(all_input.groupby(\"shp_employees\")[\"shp_sector\"].apply(list))\n",
    "# 1 empy = 14 services to 44 shps\n",
    "# 2 empy = 1 services to 41 shps\n",
    "# 3 empy = 1 services to 6 shps and 2 production sites\n",
    "# 4 empy = 4 services to 3 shps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all busiensses which experiecned damage on cotnent:\n",
    "-- > the highest losses are found for shops and retailors, however also some businesses in this secotr had very small damage ratios \n",
    "    so the avg. loss ratio for shops and retailors is similar high as for the other sector types \n",
    "--> highest abs closs have shops with higher sale rates, but also here outliers\n",
    "--> businesses with higher sales also experienced more severe damages on their inventory (only damage cases)\n",
    "    , while for bred no clear pattern also due that sample sizes for businesses with higher sales are quite small\n",
    "    but it highest bus. reductions (excl zero-reduction) are only experienced by retailors and shops- giving a hint that shps an retialors are more vulnerable to floods than services and productions sites (still uncertainty due to quite different sample sizes 156 - 37 - 8) \n",
    "    - smae als o for degree of loss: highest degree of loss are experienced by retials and shops\n",
    "--> most businesses with one or two employees are usually shops or retailers    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_p.loc[all_input_p.shp_sector == .0, :].describe()  # check for services - mean of cheap precuation (0.3625+-0.329)\n",
    " \n",
    "print(all_input_p.shp_avgmonthly_sale_euro[all_input_p.shp_sector == .0].describe() )\n",
    "print(all_input_p.shp_avgmonthly_sale_euro[all_input_p.shp_sector == .5].describe() )\n",
    "print(all_input_p.shp_avgmonthly_sale_euro[all_input_p.shp_sector == 1.].describe() )\n",
    "\n",
    "# -->shops with low sale rates tend to impl more emegerncy meausres than serivces or production sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WD ~ FV ~ degree of rcloss and bred**\n",
    "--> there is no clear connection between fv -> target or wd --> targets \n",
    "--> only wd and fv are usually correlated (as seen already in spearman rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_input_p.flowvelocity.value_counts())\n",
    "\n",
    "all_input_p = all_input#.copy()\n",
    "\n",
    "target = [\"Target_relative_contentloss_euro\", \"Target_businessreduction\"][1]\n",
    "## select only shops with content damages - only those show this concrete pattern (high sale rates - less precaution) using all cases (inl zeroloss : no pattern visible) \n",
    "# all_input_p = all_input_p.loc[all_input_p[\"Target_contentloss_euro\"] > 0.0, :]\n",
    "all_input_p = all_input_p.loc[all_input_p[target] > 0.0, :]\n",
    "\n",
    "all_input_p[\"wd_categ\"] = [ i//10*10+1 for i in all_input_p[\"water_depth_cm\"]]  # apply modulo operator and multiply by 10\n",
    "all_input_p.boxplot(column=target, by=[\"wd_categ\"])\n",
    "# all_input_p.boxplot(column=target, by=[\"flowvelocity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.relplot(\n",
    "        data=all_input_p, x=\"water_depth_cm\", y=\"flowvelocity\", \n",
    "        #hue=\"origin\", \n",
    "        #size=\"Target_businessreduction\",\n",
    "        size=\"Target_relative_contentloss_euro\",\n",
    "        sizes=(40, 400), alpha=.5, palette=\"muted\",\n",
    "        height=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.columns\n",
    "all_input.insert(1, \"Target_contentloss_euro\", all_input.pop(\"Target_contentloss_euro\"))\n",
    "all_input.insert(2, \"Target_relative_contentloss_euro\", all_input.pop(\"Target_relative_contentloss_euro\"))\n",
    "\n",
    "all_input_degree = all_input.drop([\n",
    "    \"flood_type.1\", \"flood_type.2\", \"flood_type.3\", \n",
    "    \"geometry\",\"shp_sector\", \"overall_problem_house\", \"floors\",\n",
    "    \"Target_contentloss_euro\", \"Target_businessreduction\"\n",
    "    ], axis=1)\n",
    "all_input_degree = all_input_degree.loc[all_input_degree[\"Target_relative_contentloss_euro\"] > 0.0 , :]\n",
    "\n",
    "# f.plot_correlations(all_input.drop(\"geometry\", axis=1) , outfile=f\"../figures/combi_scatter_correlation.png\")\n",
    "# f.plot_correlations(all_input_degree , outfile=f\"../figures/combi_scatter_correlation_degree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unneeded columns and features sort by parent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input = all_input.drop([\n",
    "    # \"floors\",  # remove in outliers removal for larger buildings\n",
    "    \"flood_type.1\", \"flood_type.2\",\"flood_type.3\",\n",
    "    #\"Target_contentloss_euro\" ## keep for BN modelling to compare abs losses\n",
    "    ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_input.insert(0, \"Target_relative_contentloss_euro\", all_input.pop(\"Target_relative_contentloss_euro\"))  \n",
    "all_input.insert(5, \"contaminations\", all_input.pop(\"contaminations\"))  \n",
    "all_input.insert(6, \"overall_problem_house\", all_input.pop(\"overall_problem_house\"))  \n",
    "all_input.insert(17, \"shp_employees\", all_input.pop(\"shp_employees\"))  \n",
    "all_input.insert(16, \"hh_monthly_income_euro\", all_input.pop(\"hh_monthly_income_euro\"))  \n",
    "# all_input.insert(18, \"shp_sector\", all_input.pop(\"shp_sector\"))  \n",
    "\n",
    "all_input.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inundation duration**\n",
    "One case exists with inundation duration of 600 hours, which is in repsect to the experiecned business reduction of 10% durign the flood month and a degree of loss of 5% probably not reasonable, all other maximum durations are 170 or 240 hours long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Case for which inundation duration is more than 300 hours\\n\")\n",
    "all_input.loc[all_input.inundation_duration_h >150]\n",
    "all_input.loc[all_input.inundation_duration_h > 500, \"inundation_duration_h\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shops in large buildings**\n",
    "\n",
    "Remove busineses which are located in buidligns with more than 3 floors, due that approximation is getting less exact due to small sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_input.shape)\n",
    "print(f\"Remove {all_input[all_input['floors'] > 3.0].shape[0]} records which have shops located in larger buildings\")\n",
    "all_input = all_input[ ~(all_input[\"floors\"] > 3.0)]\n",
    "\n",
    "## drop floors from final feature space\n",
    "all_input.drop(\"floors\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input[[\"inundation_duration_h\", \"b_area\", \"hh_monthly_income_euro\", \n",
    "                            \"shp_registered_capital_euro\", \"shp_avgmonthly_sale_euro\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.plot_correlations(all_input , outfile=f\"../figures/combi_scatter_correlations_before_removal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xplore outliers**\n",
    "Visualize outliers which represent unusal but true cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_input.loc[all_input[\"inundation_duration_h\"] < 48,:].shape[0], \"cases with less than 2 days inundation\")\n",
    "print(all_input.loc[all_input[\"inundation_duration_h\"] > 48,:].shape[0], \"cases with more than 2 days inundation\")\n",
    "all_input[\"inundation_duration_h\"].hist(bins=150, alpha=1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> inundation duration has few cases of very long flood duration, the majority is by less than 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of variation\n",
    "access relative volatility between both targets and for predictors\n",
    "Maybe it explians why some predcitors are seen as more important as others in feautre-extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coef. of variation\n",
    "\n",
    "#def calc_coef_of_vairation(x):\n",
    "cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100 \n",
    "#    return cv\n",
    "\n",
    "#calculate CV for each column in data frame\n",
    "print(all_input.drop(\"geometry\", axis=1).apply(cv).sort_values())\n",
    "# calc_coef_of_vairation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_input.drop(\"geometry\", axis=1).apply(cv).sort_values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single datasets for each target variable\n",
    "Select candiate predictors in respect to the response variables: Split dataset into two parts, one with candidate predictors for content losses and one dataset with candidate predictors for business reduction. This means non relevant candidate predictors fo the respective response are moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.drop([\"overall_problem_house\",  \"geometry\",\n",
    "    \"shp_sector\",\n",
    "    \"shp_registered_capital_euro\"], axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_degree = all_input.loc[all_input.Target_relative_contentloss_euro > 0.0, :]\n",
    "# all_input_bred = all_input.loc[all_input.Target_relative_contentloss_euro > 0.0, :]\n",
    "\n",
    "all_input_plot = all_input_degree\n",
    "\n",
    "all_input_plot = all_input_plot.drop(\n",
    "    [#\"Target_businessreduction\", \n",
    "        \"overall_problem_house\",  \"geometry\",\n",
    "        \"shp_sector\", \n",
    "        \"shp_registered_capital_euro\"], axis=1)\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 100, 100)\n",
    "\n",
    "plt.hist(all_input.Target_relative_contentloss_euro*100, bins, alpha=0.7, label=\"rcloss\")\n",
    "plt.hist(all_input.Target_businessreduction, bins, alpha=0.7, label=\"rbred\")\n",
    "plt.hist(all_input_degree.Target_relative_contentloss_euro*100, bins, alpha=0.7, label=\"degree of rcloss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# all_input.Target_relative_contentloss_euro.describe()\n",
    "\n",
    "## --> only 25% of rcloss have losses higher than 2% of their inventory value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "f.plot_spearman_rank(all_input_plot, min_periods=50, signif=False, psig=0.05)\n",
    "\n",
    "## --> degree of loss (rcloss without zerolosses)  has stronger correlations than bred (incl zero cases) (NOTE shp_contentvalue is not used in rcloss ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot spearate correlation plots with feature spaces for degree of rcloss and bred\n",
    "\n",
    "Degree of loss (rcloss without zerolosses)  has stronger correlations than bred (incl zero cases)  ? (NOTE shp_contentvalue is not used in rcloss ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_degree = all_input.loc[all_input.Target_relative_contentloss_euro > 0.0, :]\n",
    "all_input_chance = all_input.copy(deep=True)  # remove link between both instances\n",
    "all_input_chance.loc[all_input_chance[\"Target_relative_contentloss_euro\"] > 0, \"Target_relative_contentloss_euro\"] = 1\n",
    "all_input_bred = all_input\n",
    "\n",
    "\n",
    "all_input_chance = all_input_chance.drop(\n",
    "    [\"Target_businessreduction\", \"shp_business_limitation\",\n",
    "     \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "        \"overall_problem_house\",  \"geometry\",\n",
    "    \"shp_sector\", \"shp_registered_capital_euro\"], axis=1)\n",
    "\n",
    "all_input_degree = all_input_degree.drop(\n",
    "    [\"Target_businessreduction\", \"shp_business_limitation\",\n",
    "     \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "        \"overall_problem_house\",  \"geometry\",\n",
    "    \"shp_sector\", \"shp_registered_capital_euro\"], axis=1)\n",
    "\n",
    "all_input_bred = all_input_bred.drop(\n",
    "    [\"Target_relative_contentloss_euro\", \"shp_business_limitation\",\n",
    "     \"Target_contentloss_euro\", \"shp_content_value_euro\",\n",
    "        \"overall_problem_house\",  \"geometry\",\n",
    "    \"shp_sector\", \"shp_registered_capital_euro\"], axis=1)\n",
    "\n",
    "\n",
    "all_input_degree.rename(columns=s.feature_names_plot, inplace=True)\n",
    "# all_input_degree.rename(columns={\"rcloss\":\"degree of rcloss\"}, inplace=True)\n",
    "all_input_bred.rename(columns=s.feature_names_plot, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, mean_absolute_error as mae\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "# from matplotlib.cbook import boxplot_stats  \n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "\n",
    "# import evaluation_utils as eu\n",
    "import settings as s\n",
    "import feature_selection as fs\n",
    "\n",
    "\n",
    "def plot_spearman_rank(df_corr, min_periods=100, signif=True, psig=0.05, target=None):\n",
    "    \"\"\"\n",
    "    ## Code snippet modified: https://stackoverflow.com/questions/69900363/colour-statistically-non-significant-values-in-seaborn-heatmap-with-a-different\n",
    "\n",
    "    df_corr (dataframe): dataframe with variables to plot\n",
    "    min_periods (int): Minimum number of observations required per pair of columns to have a valid result\n",
    "    signif (boolean): should non significant be masked\n",
    "    psig (float): signifcance level\n",
    "    return: Figure for Spearman Correlation \n",
    "    \"\"\" \n",
    "    ## get the p value for spearman coefficient, subtract 1 on the diagonal\n",
    "    pvals = df_corr.corr(\n",
    "        method=lambda x, y: stats.spearmanr(x, y)[1], min_periods=min_periods\n",
    "        ) - np.eye(*df_corr.corr(method=\"spearman\", min_periods=min_periods).shape)  # np.eye(): diagonal= ones, elsewere=zeros\n",
    "\n",
    "    #  main plot\n",
    "    sns.heatmap(\n",
    "        df_corr.corr(method=\"spearman\", min_periods=min_periods), \n",
    "        annot=True, square=True, \n",
    "        center=0, cmap=\"RdBu\", \n",
    "        fmt=\".2f\", zorder=1,\n",
    "        annot_kws={'size': 10})\n",
    "    plt.title(f\"Spearman's rank correlation: {target}\", fontsize=16)\n",
    "\n",
    "    # signifcance mask\n",
    "    if signif:\n",
    "        ## add another heatmap with colouring the non-significant cells\n",
    "        sns.heatmap(df_corr.corr(method=\"spearman\", min_periods=min_periods)[pvals>=psig], \n",
    "            annot=False, square=True, cbar=False,\n",
    "            ## add-ons\n",
    "            cmap=sns.color_palette(\"Greys\", n_colors=1, desat=1),  \n",
    "            zorder = 2) # put the map above the heatmap\n",
    "        ## add a label for the colour\n",
    "        colors = [sns.color_palette(\"Greys\", n_colors=1, desat=1)[0]]\n",
    "        texts = [f\"not significant (at {psig})\"]\n",
    "        patches = [ mpatches.Patch(color=colors[i], label=\"{:s}\".format(texts[i]) ) for i in range(len(texts)) ]\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(.85, 1.05), loc='center')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_degree.rename(columns={\"rcloss\":\"degree of rcloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "# all_input_degree.rename(columns={\"rcloss\":\"degree of rcloss\"}, inplace=True)\n",
    "plot_spearman_rank(all_input_degree, min_periods=50, signif=False, psig=0.05, target=\"degree of rcloss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "f.plot_spearman_rank(all_input_bred, min_periods=50, signif=False, psig=0.05, target=\"rbred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**correlations with binary chance of rcloss and predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "corr_list = [] \n",
    "all_input_chance = all_input_chance.dropna()\n",
    "\n",
    "#calculate biserial correlation\n",
    "for column in all_input_chance:\n",
    "    x = all_input_chance[column]\n",
    "    corr = stats.pointbiserialr(list(x), list(all_input_chance.Target_relative_contentloss_euro))\n",
    "    corr_list.append(corr[0])\n",
    "print(corr_list)\n",
    "\n",
    "# 2: 16 (wd), 4:18 (fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop unneded columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input = all_input.select_dtypes(exclude=[\"geometry\"])\n",
    "all_input.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_contentloss = all_input\n",
    "all_input_contentloss = all_input_contentloss.drop(\n",
    "    [\n",
    "    \"Target_businessreduction\",\n",
    "    #\"shp_content_value_euro\",  # keep inside to calc obs and predicted abs loss returned from BN\n",
    "    \"overall_problem_house\",  # no logical impact on closs, but might impact bred\n",
    "    # \"shp_sector\",\n",
    "    \"shp_registered_capital_euro\", # same - circular \n",
    " \n",
    "    ]\n",
    "    , axis=1)\n",
    "print(all_input_contentloss.shape)\n",
    "all_input_contentloss.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_business_reduction = all_input\n",
    "all_input_business_reduction = all_input_business_reduction.drop(\n",
    "    [\n",
    "    \"Target_relative_contentloss_euro\",\n",
    "      \"Target_contentloss_euro\", \n",
    "      \"overall_problem_house\",  #rm due that it is circular / captured by other variables\n",
    "      \"shp_registered_capital_euro\", # same - circular, corr with monhtly_sale\n",
    "      # \"shp_sector\",\n",
    "      #\"hh_monthly_income_euro\", \"shp_content_value_euro\",  # corr with monhtly_sale\n",
    "    ],\n",
    "    axis=1)\n",
    "\n",
    "print(all_input_business_reduction.shape)\n",
    "all_input_business_reduction.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests of feature space modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test feature importance of business sector via one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_contentloss = pd.get_dummies(all_input_contentloss, columns=[\"shp_sector\"]) # 3 sector types\n",
    "# all_input_contentloss.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"geometry\"][1].is_\n",
    "# \" \".join([is_digit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df = gpd.GeoDataFrame(df,  \n",
    "#             geometry=gpd.points_from_xy( \n",
    "#                     df[\"geometry\"].str.split(\",\").str[1], # lon\n",
    "#                     df[\"geometry\"].str.split(\",\").str[0],  # lat\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geolocations from the survey of the two largest businesses was compared with their actual location done by an internet search. It could be seen that the geolocations mentioned in the survey at least for the two largest businesses ahave potentially a high uncertainty range which is at least for the two examined businesses more than 200 m (beeline). The usage of spatial variables would be highly imprecise. Furthermore deriving the content value from building height is similar imprecise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality assessment: check OSM building information\n",
    "Check if enough commercial/business-related building information exist in OSM to use it for extrapolation of content loss \n",
    "test for certain streets / small AIO with many shophouses exists from Tübing ds\n",
    "- verify OSM information with geolocation of shophouses from Tübing ds \n",
    "- if enough valid information exists get OSM information from 2020\n",
    "- if enough valid information adapt attributes by shop sectors from HCMC dataset\n",
    "- if enough valid information get boundaries of AOI  :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import requests\n",
    "# # URL = \"https://api.ohsome.org/v1/elements/count\"\n",
    "# # data = {\"bboxes\": \"106.593238,10.6971085,106.7740687,10.8401006\",  \"format\": \"json\", \n",
    "# #         \"time\": \"2023-09-01\", \n",
    "# #         # buildings total end 2023 : ~ 64 800 \n",
    "# #         #\"filter\": \"building=* and builing!=no and shop=*\"}  # ~153 results \n",
    "# #         # \"filter\": \"shop=* or building=commercial or building=industrial or building=kiosk or building=retail or amenity=bar or amenity=cafe or amenity=fast_food or amenity=restaurant or landuse=retail or landuse=commercial\"} # 6424 reuslts\n",
    "# #         # \"filter\": \"shop=* or building=commercial or building=industrial or building=kiosk or building=retail or amenity=bar or amenity=cafe or amenity=fast_food or amenity=restaurant\"} # 6288.0 reuslts\n",
    "# #         \"filter\": \"building=commercial or building=kiosk or building=retail or amenity=bar or amenity=cafe or  amenity=fast_food or amenity=restaurant\"} # 2925.0 reuslts\n",
    "# # # and type:way\"}\n",
    "# # response = requests.post(URL, data=data)\n",
    "# # print(response.json())\n",
    "\n",
    "# # import requests\n",
    "# # #from ohsome import OhsomeClient\n",
    "# # URL = \"https://api.ohsome.org/v1/elements/count\"\n",
    "# # data = {\"bboxes\": \"106.593238,10.6971085,106.7740687,10.8401006\", \"format\": \"json\", \"filter\": \"building=* and builing!=no and building:levels=* or level=* or building:level=* or stories=* or levels=* or building:part:levels=*\"}\n",
    "# # response = requests.post(URL, data=data)\n",
    "# # print(response.json())\n",
    "# # ## --> building tagged in HCMC 64495.0, from those have around 2746 information about stories --> ~ <5% stories information\n",
    "\n",
    "# import requests\n",
    "# URL = \"https://api.ohsome.org/v1/elements/geometry\"\n",
    "# #URL = \"https://api.ohsome.org/v1/elements/length/ratio\"\n",
    "#         #\"filter\": \"highway=living_street and type:way\", \"filter2\": \"highway=living_street and oneway=yes and type:way\"}\n",
    "# data = {\"bboxes\": \"106.593238,10.6971085,106.7740687,10.8401006\", \"properties\": \"tags,metadata\",\n",
    "#         \"time\": \"2023-09-01\", \n",
    "#         \"filter\": \"building=commercial or building=kiosk or building=retail or amenity=bar or amenity=cafe or  amenity=fast_food or amenity=restaurant\"} \n",
    "#         # TODO include shop: (florist, supermarket, clothes, bag, houseware, books, seafood, beauty, mobile_phone, military_surplus, \n",
    "#         # wedding, convenience, yes, electronics , tailor, jewelry, funeral_directors ...) # = yes e.g. Spectacles shop, Clothing\n",
    "# response = requests.post(URL, data=data)\n",
    "# print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from ohsome import OhsomeClient\n",
    "# import requests\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "# # sys.path.append( os.getcwd()+ \"/..\")\n",
    "# from modules.download import download_features, build_ohsome_filters\n",
    "# from modules.utils import load_config, init_logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_layers = \"../modules/config_tags_hcmc.json\"\n",
    "# config_tags = load_config(config_layers)\n",
    "\n",
    "# client = OhsomeClient()\n",
    "# timestamp=None\n",
    "# out_dir = \".\"\n",
    "# output_dir = \"../modules\"\n",
    "# aoi = \"106.503238,10.6071085,106.7740687,10.8401006\" #t.to_frame(name=\"geometry\")\n",
    "\n",
    "# # for layer in config_tags[\"get_all_info\"]:\n",
    "# # for layer in config_tags[\"get_commerce_info\"]:\n",
    "# for layer in config_tags[\"get_residential_info\"]:\n",
    "# # for layer in config_tags[\"get_buildingheight_info\"]:\n",
    "#     ohsome_filter_str = build_ohsome_filters(layer)\n",
    "#     print(ohsome_filter_str)\n",
    "#     response = client.elements.geometry.post(\n",
    "#         bboxes=aoi,\n",
    "#         # bboxes=\"106.593238,10.6971085,106.7740687,10.8401006\",\n",
    "#         #bpolys=aoi, \n",
    "#         time=timestamp, filter=ohsome_filter_str, properties=\"metadata,tags\"\n",
    "#     )\n",
    "#     response.to_json(os.path.join(output_dir, f\"{layer[\"name\"]}.geojson\"))  \n",
    "#     # output_dir = os.path.join(out_dir, \"test\")\n",
    "# os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get spatial distirbution of certain variables\n",
    "E.g. examine spatial variation of reported flowvelocity, water depth, business size, floor number etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data[\"geometry\"].str.findall(r\"(\\d+\\.\\d+)\")\n",
    "\n",
    "df_geom = all_input\n",
    "\n",
    "df_geom[\"geometry\"] = pd.Series(raw_data[\"geometry\"])#.astype(str).str.findall(r\"(\\d+\\.\\d+)\")\n",
    "df_geom = gpd.GeoDataFrame(df_geom)\n",
    "\n",
    "print(df_geom.shape)\n",
    "df_geom.tail(2)\n",
    "# df_geom.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom = df_geom[~ df_geom.is_empty] # drop empty geoms\n",
    "print(df_geom.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_contentloss.bage.hist(bins=100)\n",
    "all_input_business_reduction.bage.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # glimpse =  df[~ df.is_empty] \n",
    "# # #glimpse = glimpse[glimpse.flowvelocity]\n",
    "# # glimpse.geometry.explore() \n",
    "\n",
    "\n",
    "# ## preparation for folium plot\n",
    "# ## drop timestamp columns due not usable in folium plot\n",
    "# df = df_geom[[\"flood_experience\", \"inundation_duration_h\", \"water_depth_cm\", \"flowvelocity\", \"shp_content_value_mVND\", \n",
    "#          \"Target_contentloss_mVND\", \"Target_businessreduction\", \"bage\",\n",
    "#          \"geometry\"\n",
    "#          ]]\n",
    "# ## imporve plot visualization\n",
    "# # df = df.loc[df.Target_contentloss_mVND < 10, :]\n",
    "# df = df.loc[df.Target_contentloss_mVND < 500, :]\n",
    "# df = df.loc[df.inundation_duration_h < 500, :]\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# ## visual check of SMEs locations\n",
    "# import folium\n",
    "\n",
    "# glimpse = df[~ df.is_empty]   # drop emtpy geoms\n",
    "# glimpse_geolocations = df[~ df.is_empty]   # drop emtpy geoms\n",
    "# m = glimpse.geometry.explore(name=\"survey ds\", color=\"red\", k=10)  \n",
    "# m = glimpse_geolocations.explode(ignore_index=True).explore(\n",
    "#     m=m, \n",
    "#     column=\"Target_businessreduction\",\n",
    "#     # column=\"flood_experience\",\n",
    "#     #name=\"flowvelocity\",\n",
    "#     #column=\"flowvelocity\", \n",
    "#     popup=True, \n",
    "#     #tooltip=\"Nr_Floors\", \n",
    "#     cmap=\"viridis\",\n",
    "#     #cmap=\"Reds\"\n",
    "# )    # \n",
    "\n",
    "# folium.LayerControl().add_to(m)\n",
    "# m\n",
    "\n",
    "# ############### spatial distirbutions explored   ##################\n",
    "# ## 0.1 = calm velocity, 0.5 = turbulent velocity\n",
    "# ## no spatial relationship between veloctiy strength and distance to canal or waterway visible \n",
    "# ## --> seems like flow velocity is infleunced by further flood sources like overwhelmed drainage systems\n",
    "\n",
    "# ## abs loss\n",
    "# #     column=\"Target_contentloss_mVND\",\n",
    "# ## only slight spatial agglomeration of damage-case\n",
    "\n",
    "# ### CV\n",
    "# # ## only slight spatial agglomeration\n",
    " \n",
    "#  ## water depth\n",
    "#  ## cluster(s) of higher wd might exists e.g. South-east next to highway\n",
    "\n",
    "# ## inundation duration\n",
    "# ## clusters of longer (multiple clusters) and shorter (one cluster) inundation durations exist\n",
    "\n",
    "# ## flood experience\n",
    "# ## cluster at least in south-east and one in the center exists, rest is quite mixed\n",
    "\n",
    "# ## bage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**employees by sector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO are employess or b_area and sector correlated with cv \n",
    "\n",
    "# from sklearn import linear_model\n",
    "\n",
    "# X = all_input[[\"shp_sector\", \"shp_employees\"]]  # \"b_area\"\n",
    "# y = all_input[\"shp_content_value_euro\"]\n",
    "# regr = linear_model.LinearRegression()\n",
    "# regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(all_input.shp_sector.value_counts())  # number o businesses per sector\n",
    "\n",
    "\n",
    "tt = list(all_input.groupby(\"shp_sector\")[\"shp_employees\"].apply(list))\n",
    "print(Counter(tt[0]))  # count shps grouped by employee number in sector 1\n",
    "print(Counter(tt[1]))  # count shps grouped by employee number in sector 2\n",
    "print(Counter(tt[2]))  # count shps grouped by employee number in sector 3\n",
    "\n",
    "# print(pd.Series(tt[0]).describe())\n",
    "# print(pd.Series(tt[1]).describe())\n",
    "# print(pd.Series(tt[2]).describe())\n",
    "\n",
    "# # ## --> all sectors have usually 1 or 2 employees\n",
    "# all_input.shp_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## does cv correlates with setor type ?\n",
    "\n",
    "# print(all_input.shp_sector.value_counts())\n",
    "# tt = list(all_input.groupby(\"shp_sector\")[\"shp_content_value_euro\"].apply(list))\n",
    "\n",
    "# # print(Counter(tt[0]))  # count shps grouped by employee number in sector 1\n",
    "# # print(Counter(tt[1]))  # count shps grouped by employee number in sector 2\n",
    "# # print(Counter(tt[2]))  # count shps grouped by employee number in sector 3\n",
    "\n",
    "# print(pd.Series(tt[0]).describe())\n",
    "# print(pd.Series(tt[1]).describe())\n",
    "# print(pd.Series(tt[2]).describe())\n",
    "\n",
    "# ## --> secotr 3 haas usaully the most expensive content value (<- it is production sector), \n",
    "# ##    however sample size is quite small with 26 flood-cases from production sector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Spatial Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libpysal as lps\n",
    "# import pysal as ps\n",
    "# import esda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_geom = all_input_incl_geometry\n",
    "\n",
    "# df_geom[\"geometry\"] = raw_data[\"geometry\"].str.findall(r\"(\\d+\\.\\d+)\").astype(str)\n",
    "# geom_col = df_geom[\"geometry\"].str.split(\" \", n=1, expand=True)\n",
    "# geom_col\n",
    "\n",
    "# df_geom.geometry\n",
    "# df_geom = gpd.GeoDataFrame(df_geom,  \n",
    "#            geometry=gpd.points_from_xy(\n",
    "#                geom_col[0].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "#                geom_col[1].str.extract(r\"(\\d+\\.\\d+)\")[0],\n",
    "#     ))\n",
    "# df_geom = df_geom.set_crs(4326) \n",
    "\n",
    "# # dff = df_geom\n",
    "# # dff = gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]))\n",
    "\n",
    "# # target = \"Target_businessreduction\"\n",
    "# # target = \"rcloss\" #\n",
    "# target = \"Target_contentloss_mVND\"\n",
    "# #target = \"shp_content_value_mVND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use survey data points for weights creation - using KNN or DistanceBand neighborhodd definition\n",
    "\n",
    "# dff = df_geom\n",
    "# dff[target] = dff[target].apply(pd.to_numeric)\n",
    "# # dff[[\"shp_content_value_mVND\", \"Target_contentloss_mVND\"]] = dff[[\"shp_content_value_mVND\", \"Target_contentloss_mVND\"]].apply(pd.to_numeric)\n",
    "# # dff[\"rcloss\"] = dff.shp_content_value_mVND / dff.Target_contentloss_mVND\n",
    "# dff = dff[dff[target].notna()]\n",
    "# # dff = dff.loc[dff[target] <=1.00, :]\n",
    "# dff = dff.loc[dff[target] >0.00, :]\n",
    "# #dff[target].fillna((dff[target].median()), inplace=True)\n",
    "# dff =  dff[~ dff.is_empty] # NOTE: mandatory for spatial analysis drop missing geolocations\n",
    "# print(dff[target].describe())\n",
    "\n",
    "# # wq =  lps.weights.DistanceBand.from_dataframe(dff, threshold=0.6, geom_col=\"geometry\")  # def -1.0\n",
    "# # wq =  lps.weights.DistanceBand.from_dataframe(dff, threshold=0.06, alpha=-1.0, binary=False)  # def -1.0\n",
    "# # wq =  lps.weights.Queen.from_dataframe(dff)\n",
    "# # wq =  lps.weights.DistanceBand.from_dataframe(dff, threshold=01.2,binary=False)  \n",
    "# # wq =  lps.weights.Queen.from_dataframe(dff)\n",
    "# wq =  lps.weights.DistanceBand.from_dataframe(dff, threshold=0.6, binary=False)  #  inverse distance weighting\n",
    "\n",
    "# # wq.transform = \"r\"  # continous case , \"b\" binary case\n",
    "\n",
    "# ## use voronoi polygons for weights creation - using Queens neighborhodd defnition\n",
    "# # wq = lps.weights.contiguity.Queen.from_dataframe(df_geom)\n",
    "# #.lib.weights.Queen.from_dataframe(df)\n",
    "\n",
    "# wq.transform = \"r\"\n",
    "\n",
    "# print(f\"Median of {target}\", dff[target].median())# Target_businessreduction\n",
    "\n",
    "# ## Moran’s I is a test for global autocorrelation for a continuous attribute:\n",
    "# #np.random.seed(12345)\n",
    "# mi = esda.moran.Moran(dff[target], wq)\n",
    "\n",
    "# sns.kdeplot(mi.sim, shade=True)\n",
    "# plt.vlines(mi.I, 0, 5, color=\"r\")\n",
    "# plt.vlines(mi.EI, 0,5)\n",
    "# plt.xlabel(\"Moran\"s I\")\n",
    "\n",
    "# print(\"Moran\"s I\", mi.I)  # = red == observed value\n",
    "\n",
    "# print(\"p-value:\", mi.p_sim)  # significant <=0.05\n",
    "# print(\"z-score :\")  #  statement with a given level of confidence about whether or not we reject or accept the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local Morans I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.plot_correlations(\n",
    "#     all_input.drop(\n",
    "#         [\"overall_problem_house\", \"geometry\", \"shp_sector\" , \n",
    "#             \"hh_monthly_income_euro\", \"shp_content_value_euro\",\t\"shp_registered_capital_euro\"],\n",
    "#             axis=1), \n",
    "#      impute_na=True\n",
    "# )\n",
    "# all_input_contentloss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vif = fs.vif_score(all_input.drop(\"\"\"geometry\", axis=1).dropna())\n",
    "# print(df_vif.sort_values(\"vif_scores\", ascending=False)  )\n",
    "all_input_business_reduction.loc[all_input_business_reduction.Target_businessreduction==0.0, :].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_input_contentloss.drop(\"geometry\", axis=1).apply(cv).sort_values())\n",
    "print(all_input_business_reduction.drop(\"geometry\", axis=1).apply(cv).sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_contentloss.loc[all_input_contentloss.Target_relative_contentloss_euro > 0.0, :].describe()\n",
    "# Target_contentloss_euro, shp_content_value_euro\n",
    "all_input_contentloss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move targets to beginning\n",
    "all_input_contentloss.insert(0, \"Target_relative_contentloss_euro\", all_input_contentloss.pop(\"Target_relative_contentloss_euro\"))\n",
    "all_input_business_reduction.insert(0, \"Target_businessreduction\", all_input_business_reduction.pop(\"Target_businessreduction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## drop cases with missing loss information \n",
    "print(f\"Dropping {all_input_contentloss.Target_relative_contentloss_euro.isna().sum()}\")\n",
    "all_input_contentloss = all_input_contentloss[ ~all_input_contentloss[\"Target_relative_contentloss_euro\"].isna()]\n",
    "print(all_input_contentloss.shape)\n",
    "\n",
    "## drop cases with missing reduction information \n",
    "print(f\"Dropping {all_input_business_reduction.Target_businessreduction.isna().sum()}\")\n",
    "all_input_business_reduction = all_input_business_reduction[ ~all_input_business_reduction[\"Target_businessreduction\"].isna()]\n",
    "print(all_input_business_reduction.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_business_reduction.info()\n",
    "# pp.percentage_of_nan(all_input_business_reduction)  # cv has 14% ming NA -maybe remove therefore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "f.plot_spearman_rank(all_input_business_reduction.drop(\"geometry\", axis=1), min_periods=50, signif=True, psig=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check VIP score for final set of potential predictors\n",
    "\n",
    "df_vif = all_input_business_reduction.drop(\n",
    "    [\"geometry\", \"Target_businessreduction\"],\n",
    "    axis=1).dropna()\n",
    "\n",
    "# df_vif = all_input_contentloss.drop(\n",
    "#     [\"geometry\", \"Target_relative_contentloss_euro\", \"Target_contentloss_euro\", \"shp_content_value_euro\"],\n",
    "#     axis=1).dropna()\n",
    "df_vif = fs.vif_score(df_vif)\n",
    "\n",
    "print(df_vif.sort_values(\"vif_scores\", ascending=False).head(15)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_input_contentloss.loc[all_input_contentloss[\"Target_relative_contentloss_euro\"]>0.00,:].shape[0], \"damage cases\")\n",
    "print(all_input_contentloss.loc[all_input_contentloss[\"Target_relative_contentloss_euro\"]==0.00,:].shape[0], \"zero damage cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_contentloss[ (all_input_contentloss.bage<50) & (all_input_contentloss.bage > 20)].Target_relative_contentloss_euro.describe()\n",
    "# # 75%       0.0316 # buis odler than 50 years (10 cases)\n",
    "# all_input_contentloss.bage.hist(bins=100)\n",
    "\n",
    "f.plot_spearman_rank(all_input_business_reduction.drop(\"geometry\", axis=1), signif=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to disk\n",
    "all_input_contentloss.to_excel(\"../input_survey_data/input_data_contentloss_tueb.xlsx\", index=False)\n",
    "all_input_business_reduction.to_excel(\"../input_survey_data/input_data_businessreduction_tueb.xlsx\", index=False)\n",
    "\n",
    "\n",
    "##  save all_input for comparison with cantho distrbutions\n",
    "all_input.to_excel(\"../input_survey_data/input_data_bothtargets_tueb.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relative content loss dataset\")\n",
    "print(\"Number of candidate predictors \", all_input_contentloss.shape[1])\n",
    "print(\"Number of cases \", all_input_contentloss.shape[0])\n",
    "\n",
    "print(\"\\nBusiness reduction dataset\")\n",
    "print(\"Number of candidate predictors \", all_input_business_reduction.shape[1])\n",
    "print(\"Number of cases \", all_input_business_reduction.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_contentloss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input.Target_relative_contentloss_euro.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export distribution statistics \n",
    "\n",
    "tbl = all_input.describe().T\n",
    "tbl.to_excel(\"../input_survey_data/input_data_business_distrib_tueb.xlsx\", index=True)\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare rlosses of older buildings with newer buidlings\n",
    "Theory that older buildings have less losses due that they are located in historical areas of the city\n",
    "--> problem only 10 buildins are older than 50 year accoridng to interviewee reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check performance with income for rcloss, and income + cv for bred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_contentloss.columns # s = all_input_contentloss.drop(\"Target_contentloss_euro\", axis=1)\n",
    "# all_input_contentloss = all_input_contentloss.drop([\"shp_content_value_euro\"], axis=1)\n",
    "\n",
    "# all_input_business_reduction = all_input_business_reduction.drop(\"Target_relative_contentloss_euro\", axis=1)\n",
    "# #all_input_business_reduction = all_input_business_reduction.drop([\"shp_sector\" ,\"flood_type.1\", \"flood_type.2\",\"flood_type.3\"], axis=1)\n",
    "\n",
    "all_input_contentloss.hh_monthly_income_euro.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_business_reduction.columns\n",
    "#all_input_business_reduction.precautionary_measures.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check freq of pluvial/fluvial and riverine flood types\n",
    "# all_input[\"hh_monthly_income\"].describe()  # recent\n",
    "\n",
    "# ## records total : 393\n",
    "# # 1 : tidal 306 records \n",
    "# # 2 : pluvial 286 records --> prolonged or high-intensity can led to overwhelmed drainage systems / waterways (Leitpold 2021)\n",
    "# # 3 : riverine 38 records \n",
    "\n",
    "# ## many compound floodings are reported - in line with Leitpold 2021 for manufactoring firms (two or more flood sources simultainously or in subsequently in short time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_contentloss.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
