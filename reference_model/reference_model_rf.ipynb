{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-10-2023 01:14:13 - __reference_model_rcloss__ - INFO - (312, 15)\n",
      "12-10-2023 01:14:13 - __reference_model_rcloss__ - INFO - \n",
      "############ Applying RandomForestRegressor on Target_relative_contentloss_euro ############\n",
      " \n",
      "12-10-2023 01:14:13 - __reference_model_rcloss__ - INFO - Removing 0 records from entire dataset due that these values are nan in target variable\n",
      "12-10-2023 01:14:13 - __reference_model_rcloss__ - INFO - Finally use 294 records for feature extraction, from those are 180 cases with zero-loss or zero-reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping records with missing values\n",
      "value ranges of features:        Target_relative_contentloss_euro  inundation_duration_h   \n",
      "count                        294.000000             294.000000  \\\n",
      "mean                           4.653852               9.764286   \n",
      "std                           13.409001              26.058117   \n",
      "min                            0.000000               0.200000   \n",
      "25%                            0.000000               2.000000   \n",
      "50%                            0.000000               3.000000   \n",
      "75%                            1.960361               6.000000   \n",
      "max                           91.672324             240.000000   \n",
      "\n",
      "       water_depth_cm  flowvelocity  contaminations  flood_experience   \n",
      "count      294.000000    294.000000      294.000000        294.000000  \\\n",
      "mean        31.979592      0.311565        0.969388         83.510204   \n",
      "std         24.436299      0.126392        0.407793         52.945766   \n",
      "min          1.000000      0.100000        0.000000          3.000000   \n",
      "25%         10.000000      0.200000        1.000000         36.000000   \n",
      "50%         30.000000      0.300000        1.000000         76.000000   \n",
      "75%         50.000000      0.400000        1.000000        151.000000   \n",
      "max        150.000000      0.500000        2.000000        151.000000   \n",
      "\n",
      "             bage      b_area  emergency_measures   \n",
      "count  294.000000  294.000000          294.000000  \\\n",
      "mean    20.142857   76.348639            0.418934   \n",
      "std     14.147203   51.247237            0.265127   \n",
      "min      0.000000   12.000000            0.000000   \n",
      "25%     12.000000   42.250000            0.166667   \n",
      "50%     18.000000   66.500000            0.500000   \n",
      "75%     24.000000   97.500000            0.666667   \n",
      "max    100.000000  400.000000            1.000000   \n",
      "\n",
      "       precautionary_measures_lowcost  precautionary_measures_expensive   \n",
      "count                      294.000000                        294.000000  \\\n",
      "mean                         0.374150                          0.165533   \n",
      "std                          0.299137                          0.196187   \n",
      "min                          0.000000                          0.000000   \n",
      "25%                          0.250000                          0.000000   \n",
      "50%                          0.250000                          0.000000   \n",
      "75%                          0.500000                          0.333333   \n",
      "max                          1.000000                          1.000000   \n",
      "\n",
      "       shp_employees  resilience  shp_avgmonthly_sale_euro  \n",
      "count     294.000000  294.000000                294.000000  \n",
      "mean        1.806122    2.882086                360.129252  \n",
      "std         1.117574    1.112758                468.425556  \n",
      "min         1.000000    0.333333                 92.100000  \n",
      "25%         1.000000    2.000000                 92.100000  \n",
      "50%         2.000000    3.000000                276.200000  \n",
      "75%         2.000000    4.000000                276.200000  \n",
      "max         7.000000    5.000000               2761.900000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-10-2023 01:14:28 - __reference_model_rcloss__ - INFO - Performance of best estimators on outer test-sets:\n",
      "12-10-2023 01:14:28 - __reference_model_rcloss__ - INFO - Params of best model: {'model__random_state': 42, 'model__n_estimators': 100, 'model__criterion': 'squared_error'}\n",
      "12-10-2023 01:14:28 - __reference_model_rcloss__ - INFO - Performance of best model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor:  {'model__random_state': 42, 'model__n_estimators': 100, 'model__criterion': 'squared_error'}\n",
      "RandomForestRegressor:  {'model__random_state': 42, 'model__n_estimators': 100, 'model__criterion': 'squared_error'}\n",
      "RandomForestRegressor:  {'model__random_state': 42, 'model__n_estimators': 100, 'model__criterion': 'squared_error'}\n",
      "test_MAE 7.089206299583531\n",
      "test_RMSE 14.353581755301047\n",
      "test_MBE -0.04842822396011695\n",
      "test_SMAPE 79.35602009863008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-10-2023 01:14:30 - __reference_model_rcloss__ - INFO - \n",
      "Select features based on permutation feature importance\n",
      "12-10-2023 01:14:30 - __reference_model_rcloss__ - INFO - 5 most important features: ['bage', 'shp_avgmonthly_sale_euro', 'water_depth_cm', 'flowvelocity', 'precautionary_measures_lowcost']\n",
      "12-10-2023 01:14:30 - __reference_model_rcloss__ - INFO - \n",
      "Training and evaluation of RandomForestRegressor took 0.28753259999999997 minutes\n",
      "\n",
      "12-10-2023 01:14:30 - __reference_model_rcloss__ - INFO - Creating boxplots for range of performane scores from outer folds of nested cross-validation\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Reference model\"\"\"\n",
    "\n",
    "__author__ = \"Anna Buch, Heidelberg University\"\n",
    "__email__ = \"a.buch@stud.uni-heidelberg.de\"\n",
    "\n",
    "# Reference model to compare with BN results\n",
    "\n",
    "\"\"\"\n",
    "NOTE: \n",
    "- reference mode to predict rcloss and bred \n",
    "- use reference model to verify BN performances for HCMC ds (- maybe not needed), HCMC_OBM ds and CanTHo ds\n",
    "- for rcloss: train with zero-loss cases , aim is to compae applied approach (chance-of-loss + degree-of-loss + BN)\n",
    "- always basic set of hyperparameters is used (no tuning),\n",
    "- evaluate with 10 fold cross-valdiation\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import utils.feature_selection as fs\n",
    "import utils.training as t\n",
    "import utils.evaluation as e\n",
    "import utils.evaluation_metrics as em\n",
    "import utils.figures as f\n",
    "import utils.settings as s\n",
    "import utils.pipelines as p\n",
    "import utils.preprocessing as pp\n",
    "\n",
    "p.main()  # create/update model settings\n",
    "#s.init()\n",
    "seed = s.seed\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "import contextlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#### Load R packages to process Conditional Random Forest in python\n",
    "# *NOTE 1: all needed R packages have to be previously loaded in R*\n",
    "# *NOTE 2: Make sure that caret package version >= 6.0-81, otherwise caret.train() throws an error*\n",
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "from rpy2.robjects.packages import importr, data\n",
    "\n",
    "# get basic R packages\n",
    "utils = importr('utils')\n",
    "base = importr('base')\n",
    "dplyr = importr('dplyr')\n",
    "stats_r = importr(\"stats\")  # rename due to similar python package\n",
    "\n",
    "# pandas.DataFrames to R dataframes \n",
    "pandas2ri.activate()\n",
    "\n",
    "# print r df in html\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "\n",
    "# get libraries for CRF processing, ctree_controls etc\n",
    "party = importr('party')        # Random Forest with Conditional Inference Trees (Conditional Random Forest)\n",
    "permimp = importr('permimp')  # conditional permutation feature importance\n",
    "caret = importr('caret') # package version needs to be higher than  >=  6.0-90\n",
    "nestedcv = importr('nestedcv')\n",
    "tdr = importr(\"tdr\")\n",
    "\n",
    "\n",
    "\n",
    "targets = [\"Target_relative_contentloss_euro\", \"Target_businessreduction\"]\n",
    "target = targets[0]\n",
    "\n",
    "\n",
    "# Get logger  # test: init application\n",
    "main_logger = f\"__reference_model_rcloss__\"\n",
    "logger = s.init_logger(main_logger)\n",
    "\n",
    "## settings for cv\n",
    "kfolds_and_repeats = 3,1 #10, 5 # 3, 1  # <k-folds, repeats> for nested cv\n",
    "inner_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=kfolds_and_repeats[1], random_state=seed)\n",
    "# outer_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=kfolds_and_repeats[1], random_state=seed)\n",
    "outer_cv = RepeatedKFold(n_splits=kfolds_and_repeats[0], n_repeats=1, random_state=seed) # make same as for R nestedcv.train()\n",
    "\n",
    "aoi = \"hcmc\"\n",
    "# aoi = \"hcmc_obm\"\n",
    "# aoi = \"cantho\"\n",
    "\n",
    "## TODO make base outdir ./reference_model_results/degree_of_loss\n",
    "##  out_dir = os.path.join(base_dir, \"path\")\n",
    "\n",
    "## save models and their evaluation in following folders:\n",
    "Path(f\"../reference_model_results/{aoi}/models_trained/degree_of_loss/nested_cv_models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../reference_model_results/{aoi}/models_trained/degree_of_loss/final_models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../reference_model_results/{aoi}/models_evaluation/degree_of_loss\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"../reference_model_results/{aoi}/selected_features/degree_of_loss\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "## load cnadidate predictors and target\n",
    "df_candidates = pd.read_excel(\"../input_survey_data/input_data_contentloss_tueb.xlsx\")\n",
    "# df_candidates = pd.read_excel(\"../input_survey_data/input_data_businessreduction_tueb.xlsx\")\n",
    "\n",
    "\n",
    "df_candidates.drop(\"geometry.1\", axis=1, inplace=True)\n",
    "\n",
    "if target == \"Target_relative_contentloss_euro\":\n",
    "    df_candidates[target] = df_candidates[target] * 100  # make target range more comparable with Bred, TODO move to data_clenaing.ipynb\n",
    "\n",
    "with contextlib.suppress(Exception):\n",
    "    if target == \"Target_relative_contentloss_euro\":\n",
    "        df_candidates.drop(\"hh_monthly_income_euro\", axis=1, inplace=True) \n",
    "        df_candidates.drop(\"shp_registered_capital_euro\", axis=1, inplace=True) # drop due to high collinearit with income and sale, and highest number of missing values\n",
    "\n",
    "with contextlib.suppress(Exception):\n",
    "    if target == \"Target_businessreduction\":\n",
    "        df_candidates.drop(\"hh_monthly_income_euro\", axis=1, inplace=True) \n",
    "        df_candidates.drop(\"shp_content_value_euro\", axis=1, inplace=True) \n",
    "\n",
    "logger.info(df_candidates.shape)\n",
    "\n",
    "## Evaluation metrics \n",
    "score_metrics = {\n",
    "    \"MAE\": make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    \"RMSE\": make_scorer(em.root_mean_squared_error, greater_is_better=False),\n",
    "    \"MBE\": make_scorer(em.mean_bias_error, greater_is_better=False),\n",
    "    # \"R2\": \"r2\",\n",
    "    \"SMAPE\": make_scorer(em.symmetric_mean_absolute_percentage_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## empty variables to store model outputs\n",
    "eval_sets = {}\n",
    "models_trained = {}\n",
    "final_models_trained = {}\n",
    "models_coef = {}\n",
    "predicted_values = {}\n",
    "df_feature_importances = pd.DataFrame(index=df_candidates.drop(target, axis=1).columns.to_list())\n",
    "models_scores = {}\n",
    "\n",
    "## iterate over piplines. Each pipline contains a scaler and regressor (and optionally a bagging method) \n",
    "pipelines = [\"pipe_ref_model\"]  \n",
    "\n",
    "## Load set of hyperparamters\n",
    "hyperparams_set = pp.load_config(\"../utils/hyperparameter_sets.json\")\n",
    "\n",
    "\n",
    "for pipe_name in pipelines:\n",
    "\n",
    "    TIME0 = datetime.now()\n",
    "\n",
    "    ## load model pipelines\n",
    "    pipe = joblib.load(f'./pipelines/{pipe_name}.pkl')\n",
    " \n",
    "    try:\n",
    "        model_name = re.findall(\"[a-zA-Z]+\", str(pipe.steps[1][1].__class__).split(\".\")[-1])[0] # get model name for python models  \n",
    "    except AttributeError:\n",
    "        model_name = pipe # get R model name\n",
    "    \n",
    "    ## load respective hyperparameter space\n",
    "    param_space = hyperparams_set[f\"{model_name}_hyperparameters\"]\n",
    "\n",
    "    ## if bagging fro model training is used , rename hyperparmeters\n",
    "    if \"bag\" in pipe_name.split(\"_\"):\n",
    "        logger.info(f\"Testing {model_name} with bagging\")\n",
    "        param_space = { k.replace('model', 'bagging__estimator') : v for (k, v) in param_space.items()}\n",
    "\n",
    "\n",
    "\n",
    "    logger.info( f\"\\n############ Applying {model_name} on {target} ############\\n \")\n",
    "\n",
    "    # save original df for later\n",
    "    df_Xy = df_candidates\n",
    "\n",
    "    # rm geometry column which only needed for visualization\n",
    "    df_Xy = df_Xy.drop(\"geometry\", axis=1)\n",
    "\n",
    "    # get predictor names\n",
    "    X_names = df_Xy.drop(target, axis=1).columns.to_list()\n",
    "\n",
    "    # ## remove zero-loss records only for rcloss\n",
    "    # if target == \"Target_relative_contentloss_euro\":\n",
    "    #     logger.info(f\"Removing {df_Xy.loc[df_Xy[target]==0.0,:].shape[0]} zero loss records\")\n",
    "    #     df_Xy = df_Xy.loc[df_Xy[target]!=0.0,:]\n",
    "\n",
    "\n",
    "    ## drop samples where target is nan\n",
    "    logger.info(f\"Removing {df_Xy[target].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "    df_Xy = df_Xy[ ~df_Xy[target].isna()]\n",
    "\n",
    "    ## Elastic Net and Random Forest: drop samples where any value is nan\n",
    "    if (model_name == \"RandomForestRegressor\") | (model_name == \"ElasticNet\") | (model_name == \"cforest\"):\n",
    "        print(\"Dropping records with missing values\")\n",
    "        df_Xy.dropna(inplace=True)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Finally use {df_Xy.shape[0]} records for feature extraction, from those are {(df_Xy[target][df_Xy[target] == 0.0]).count()} cases with zero-loss or zero-reduction\",\n",
    "    )\n",
    "\n",
    "    X = df_Xy[X_names]\n",
    "    y = df_Xy[target]\n",
    "\n",
    " \n",
    "    ## run sklearn model\n",
    "    print(\"value ranges of features:\", df_Xy.describe())\n",
    "\n",
    "    ## fit model for unbiased model evaluation and for final model used for Feature importance, Partial Dependence etc.\n",
    "    mf = t.ModelFitting(\n",
    "        model=pipe, \n",
    "        Xy=df_Xy,\n",
    "        target_name=target,\n",
    "        param_space=param_space,\n",
    "        tuning_score=score_metrics[\"MAE\"], # tune by getting reducing MAE\n",
    "        cv=inner_cv,\n",
    "        kfolds_and_repeats=kfolds_and_repeats,\n",
    "        seed=seed,\n",
    "    )\n",
    "    models_trained_ncv = mf.model_fit_ncv()\n",
    "\n",
    "    # save models from nested cv and final model on entire ds\n",
    "    joblib.dump(models_trained_ncv, f\"../reference_model_results/{aoi}/models_trained/degree_of_loss/nested_cv_models/{model_name}_{target}.joblib\")\n",
    "        \n",
    "    ## evaluate model    \n",
    "    me = e.ModelEvaluation(\n",
    "        models_trained_ncv=models_trained_ncv, \n",
    "        Xy=df_Xy,\n",
    "        target_name=target,\n",
    "        score_metrics=score_metrics,\n",
    "        cv=outer_cv,\n",
    "        kfolds=kfolds_and_repeats[0],\n",
    "        seed=seed,\n",
    "    )\n",
    "    model_evaluation_results = me.model_evaluate_ncv()\n",
    "\n",
    "    \n",
    "    ## visual check if hyperparameter ranges are good or need to be adapted\n",
    "    logger.info(f\"Performance of best estimators on outer test-sets:\") \n",
    "    for i in range(len(model_evaluation_results[\"estimator\"])):\n",
    "        print(f\"{model_name}: \", model_evaluation_results[\"estimator\"][i].best_params_)\n",
    "\n",
    "\n",
    "    ## store models evaluation \n",
    "    models_scores[model_name] =  {\n",
    "        k: model_evaluation_results[k] for k in tuple(\"test_\" + s for s in list(score_metrics.keys()))\n",
    "    } # get evaluation scores, metric names start with \"test_<metricname>\"\n",
    "    \n",
    "    ## reverse sklearn.cross_validate() outputted regression scores (e.g. MAE, RMSE, SMAPE, R2)\n",
    "    models_scores[model_name] = me.negate_scores_from_sklearn_cross_valdiate(models_scores[model_name])\n",
    "\n",
    "\n",
    "    ## Final model\n",
    "\n",
    "    ## get final model based on best MAE score during outer cv\n",
    "    best_idx = list(models_scores[model_name][\"test_MAE\"]).index(min(models_scores[model_name][\"test_MAE\"]))\n",
    "    final_model = model_evaluation_results[\"estimator\"][best_idx]\n",
    "    logger.info(f\"Params of best model: {final_model.best_params_}\") \n",
    "    final_model = final_model.best_estimator_\n",
    "\n",
    "    logger.info(f\"Performance of best model:\") \n",
    "    for metric in models_scores[model_name].keys():\n",
    "        print(metric, models_scores[model_name][metric][best_idx])\n",
    "\n",
    "\n",
    "    ## predict on entire dataset and save final model\n",
    "    y_pred = final_model.predict(X) ## need to derive regression coefficients \n",
    "    final_models_trained[model_name] = final_model \n",
    "    joblib.dump(final_model, f\"../reference_model_results/{aoi}/models_trained/degree_of_loss/final_models/{model_name}_{target}.joblib\")\n",
    "\n",
    "    ## Feature importance of best model\n",
    "    importances = me.permutation_feature_importance(final_model, repeats=5)\n",
    "\n",
    "\n",
    "\n",
    "    # ## Summarize all models and their evaluation\n",
    "\n",
    "    ## store fitted models and their evaluation results for later \n",
    "    eval_sets[model_name] = df_Xy\n",
    "    models_trained[f\"{model_name}\"] = models_trained_ncv\n",
    "    predicted_values[model_name] = me.residuals\n",
    "\n",
    "    ## store Feature Importances of each model\n",
    "    logger.info(\"\\nSelect features based on permutation feature importance\")\n",
    "    df_importance = pd.DataFrame(\n",
    "        {\n",
    "            f\"{model_name}_importances\" : importances[0],   # averaged importnace scores across repeats\n",
    "            f\"{model_name}_importances_std\" : importances[1]\n",
    "        },\n",
    "        index=X_names,\n",
    "    )\n",
    "    df_feature_importances = df_feature_importances.merge(\n",
    "        df_importance[f\"{model_name}_importances\"],   # only use mean FI, drop std of FI\n",
    "        left_index=True, right_index=True, how=\"outer\")\n",
    "    df_feature_importances = df_feature_importances.sort_values(f\"{model_name}_importances\", ascending=False)  # get most important features to the top\n",
    "    logger.info(f\"5 most important features: {df_feature_importances.iloc[:5].index.to_list()}\")\n",
    "\n",
    "\n",
    "    logger.info(\n",
    "    f\"\\nTraining and evaluation of {model_name} took {(datetime.now() - TIME0).total_seconds() / 60} minutes\\n\"\n",
    "    )\n",
    "            \n",
    "\n",
    "\n",
    "## Plot performance ranges of all evaluated estimators from outer cross-validation \n",
    "logger.info(\"Creating boxplots for range of performane scores from outer folds of nested cross-validation\")\n",
    "f.boxplot_outer_scores_ncv(\n",
    "    models_scores,\n",
    "    outfile=f\"../reference_model_results/{aoi}/models_evaluation/degree_of_loss/boxplot_scores4ncv_{target}.png\")\n",
    "\n",
    "# store avergaed scores and std for later usage\n",
    "model_evaluation = pd.DataFrame(models_scores[\"RandomForestRegressor\"]).mean(axis=0)  # get mean of outer cv metrics (negative MAE and neg RMSE, pos. R2, pos MBE, posSMAPE)\n",
    "model_evaluation_std = pd.DataFrame(models_scores[\"RandomForestRegressor\"]).std(axis=0)   # get respective standard deviations\n",
    "\n",
    "model_evaluation = pd.concat([ model_evaluation, model_evaluation_std], axis=1)\n",
    "model_evaluation.columns = [\"RandomForestRegressor_score\", \"RandomForestRegressor_score_std\"]\n",
    "\n",
    "## rename metrics\n",
    "model_evaluation.index = model_evaluation.index.str.replace(\"test_\", \"\")\n",
    "\n",
    "outfile = f\"../reference_model_results/{aoi}/models_evaluation/degree_of_loss/performance_{target}.xlsx\"\n",
    "model_evaluation.round(3).to_excel(outfile, index=True)\n",
    "# logger.info(f\"Outer evaluation scores of nested cross-validation (mean) :\\n {model_evaluation.round(3)} \\n.. saved to {outfile}\")\n",
    "logger.info(f\"Outer evaluation scores of nested cross-validation (median) :\\n {model_evaluation.round(3)} \\n.. saved to {outfile}\")\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "## Feature Importances \n",
    "### drop features which dont reduce the loss\n",
    "df_feature_importances_plot = df_feature_importances\n",
    "print(df_feature_importances.head(3))\n",
    "df_feature_importances_plot = df_feature_importances_plot.loc[df_feature_importances_plot[f\"{model_name}_importances\"] > 0.0, : ] \n",
    "df_feature_importances_plot = df_feature_importances_plot.sort_values(f\"{model_name}_importances\", ascending=True)\n",
    "\n",
    "\n",
    "## TODO update with plt_fi() func as soons a its more flexible in number of models passed to func()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.figure(figsize=(30, 22), facecolor=\"w\")\n",
    "fig = df_feature_importances_plot.plot.barh(\n",
    "    color=\"darkblue\",\n",
    "    width=0.5,\n",
    "    alpha=.7,\n",
    "    )\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"\")\n",
    "# plt.title(f\"Feature Importances for {target.replace('_',' ')}\")\n",
    "\n",
    "top_bar = mpatches.Patch(\n",
    "    color=\"darkblue\", \n",
    "    label=f\"{model_name.replace('cR','c R')}\", alpha=.7,\n",
    ")\n",
    "plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "plt.tick_params(axis='y', which='major', labelsize=12)\n",
    "plt.legend(handles=[top_bar], loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    " \n",
    "fig.get_figure().savefig(\n",
    "    f\"../reference_model_results/{aoi}/models_evaluation/degree_of_loss/feature_importances_{target}.png\", \n",
    "    bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "## Save final feature space \n",
    "### The final selection of features is used later for the non-parametric Bayesian Network\n",
    "\n",
    "## drop records with missing target values\n",
    "logger.info(f\"Dropping {df_candidates[f'{target}'].isna().sum()} records from entire dataset due that these values are nan in target variable\")\n",
    "df_candidates = df_candidates[ ~df_candidates[target].isna()]\n",
    "logger.info(f\"Keeping {df_candidates.shape[0]} records and {df_candidates.shape[1]} features\")\n",
    "\n",
    "\n",
    "## sort features by their overall importance (weighted sum across across all features) \n",
    "final_feature_names = df_feature_importances.sort_values(ascending=False).index##[:10]\n",
    "\n",
    "## save importnat features, first column contains target variable\n",
    "fs.save_selected_features(\n",
    "    df_candidates.drop(target, axis=1), # TODO adpat function that target is only once added\n",
    "    pd.DataFrame(df_candidates, columns=[target]), \n",
    "    final_feature_names,\n",
    "    filename=f\"../reference_model_results/{aoi}/selected_features/degree_of_loss/final_predictors_{target}.xlsx\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Empirical ~ predicted\n",
    "## use y_pred cross-valdiated from outer folds, mulitplied by 100 for more readable output\n",
    "for k,v in predicted_values.items():\n",
    "    print(f\"\\n{k} predicted target from cross-valdiated outer folds:\")\n",
    "    print(em.empirical_vs_predicted(predicted_values[k][\"y_true\"], predicted_values[k][\"y_pred\"]))\n",
    "\n",
    "\n",
    "# ### Plot prediction error \n",
    "f.plot_residuals(\n",
    "    residuals=predicted_values, \n",
    "    model_names_abbreviation=[\"RandomForestRegressor\"],  \n",
    "    model_names_plot=[\"Random Forest\"],\n",
    "    outfile=f\"../reference_model_results/{aoi}/models_evaluation/degree_of_loss/residuals_{target}.png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RandomForestRegressor'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_c3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
